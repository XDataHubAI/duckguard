{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DuckGuard","text":""},{"location":"#data-quality-that-just-works","title":"Data Quality That Just Works","text":"<p>3 lines of code \u00b7 Any data source \u00b7 10x faster</p> <p>One API for CSV, Parquet, Snowflake, Databricks, BigQuery, and 15+ sources. No boilerplate.</p> <pre><code>pip install duckguard\n</code></pre> <pre><code>from duckguard import connect\n\norders = connect(\"s3://warehouse/orders.parquet\")       # or Snowflake, Databricks, CSV...\nassert orders.customer_id.is_not_null()                 # Just like pytest!\nassert orders.total_amount.between(0, 10000)\nassert orders.status.isin([\"pending\", \"shipped\", \"delivered\"])\n\nquality = orders.score()\nprint(f\"Grade: {quality.grade}\")  # A, B, C, D, or F\n</code></pre> <p>Same 3 lines whether your data lives in S3, Snowflake, Databricks, or a local CSV.</p>"},{"location":"#why-duckguard","title":"Why DuckGuard?","text":"<p>Every data quality tool asks you to write 50+ lines of boilerplate before you validate a single column. DuckGuard gives you a pytest-like API powered by DuckDB's speed.</p> Feature DuckGuard Great Expectations Soda Core Pandera Lines of code to start 3 50+ 10+ 5+ Time for 1GB CSV ~4 sec ~45 sec ~20 sec ~15 sec Memory for 1GB CSV ~200 MB ~4 GB ~1.5 GB ~1.5 GB Learning curve Minutes Days Hours Minutes Pytest-like API \u2713 \u2014 \u2014 \u2014 DuckDB-powered \u2713 \u2014 Partial \u2014 PII detection Built-in \u2014 \u2014 \u2014 Anomaly detection 7 methods \u2014 Partial \u2014 Data contracts \u2713 \u2014 \u2713 \u2713"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>:material-rocket-launch: Getting Started</p> <p>Install and validate your first dataset in 30 seconds</p> </li> <li> <p>:material-check-circle: Validation Guide</p> <p>Column checks, cross-dataset validation, conditional rules</p> </li> <li> <p>:material-database: Connectors</p> <p>CSV, Parquet, S3, PostgreSQL, Snowflake, BigQuery, and more</p> </li> <li> <p>:material-snowflake: Snowflake \u00b7 :material-fire: Databricks \u00b7 :material-microsoft: Fabric \u00b7 :material-notebook: Kaggle</p> <p>Platform-specific guides for your data stack</p> </li> <li> <p>:material-puzzle: Integrations</p> <p>pytest, dbt, Airflow, GitHub Actions, Slack, Teams</p> </li> <li> <p>:material-console: CLI Reference</p> <p>Command-line tools for validation, profiling, and reports</p> </li> <li> <p>:material-api: API Reference</p> <p>Complete Python API documentation</p> </li> </ul>"},{"location":"#whats-new-in-30","title":"What's New in 3.0","text":"<p>DuckGuard 3.0 adds 23 new check types while maintaining 100% backward compatibility:</p> <ul> <li>Conditional checks \u2014 validate only when conditions are met</li> <li>Multi-column checks \u2014 composite keys, column pair relationships</li> <li>Query-based checks \u2014 custom SQL with built-in security</li> <li>Distributional checks \u2014 KS test, chi-square, normality testing</li> <li>7 anomaly detection methods \u2014 Z-score, IQR, ML baselines, seasonal</li> <li>Enterprise security \u2014 multi-layer SQL injection prevention</li> </ul> <p>Read the full changelog \u2192</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to DuckGuard will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#320-2026-02-03","title":"[3.2.0] \u2014 2026-02-03","text":"<p>AI-powered data quality, improved semantic detection, and Apache 2.0 license.</p>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>AI Module (<code>duckguard.ai</code>) \u2014 Native LLM integration for data quality:</li> <li><code>explainer.explain()</code> \u2014 Natural language quality summaries</li> <li><code>rules_generator.suggest_rules()</code> \u2014 AI-powered validation rule generation</li> <li><code>fixer.suggest_fixes()</code> \u2014 AI-suggested data quality fixes</li> <li><code>natural_language.natural_rules()</code> \u2014 Plain English validation rules</li> <li>Multi-provider support: OpenAI, Anthropic, Ollama (local)</li> <li>AI CLI Commands \u2014 <code>duckguard explain</code>, <code>duckguard suggest</code>, <code>duckguard fix</code></li> <li>Documentation Site \u2014 34-page MkDocs Material site at xdatahubai.github.io/duckguard</li> <li>NOTICE file \u2014 Apache 2.0 attribution</li> <li>GitHub Templates \u2014 Bug report, feature request, and PR templates</li> <li>SECURITY.md \u2014 Vulnerability reporting policy</li> <li>Dependabot \u2014 Automated dependency updates</li> <li>PEP 561 \u2014 <code>py.typed</code> marker for static type checkers</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Semantic type detection \u2014 Name patterns now take priority over ambiguous value patterns; latitude/longitude require 4+ decimal places; added currency patterns for tax, shipping, unit_price</li> <li>SQL injection prevention \u2014 Added <code>_escape_sql_string()</code> to <code>column.py</code> (matches, isin, _get_failed_rows) and <code>conditional.py</code> (isin_when, matches_when)</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>License \u2014 Switched from Elastic License 2.0 to Apache License 2.0 (OSI-approved)</li> <li>Classifier \u2014 Updated PyPI classifier to <code>License :: OSI Approved :: Apache Software License</code></li> </ul>"},{"location":"changelog/#310-2026-01-30","title":"[3.1.0] \u2014 2026-01-30","text":"<p>Enhanced profiler: wired 4 existing helper modules into AutoProfiler, added <code>duckguard profile</code> CLI command, and made profiling thresholds configurable.</p>"},{"location":"changelog/#added_1","title":"Added","text":"<p>Integrated Profiling Pipeline \u2014 AutoProfiler now leverages all 4 helper modules:</p> <ul> <li>PatternMatcher \u2014 25+ built-in patterns (email, SSN, UUID, credit card, etc.) replace the previous 7 hardcoded patterns</li> <li>QualityScorer \u2014 Every column gets a quality score (0\u2013100) and letter grade (A\u2013F)</li> <li>DistributionAnalyzer (deep mode) \u2014 distribution type, skewness, kurtosis, normality test</li> <li>OutlierDetector (deep mode) \u2014 IQR-based outlier count and percentage</li> </ul> <p>Percentile Statistics \u2014 <code>median_value</code>, <code>p25_value</code>, <code>p75_value</code> now included in column profiles.</p> <p>Configurable Thresholds \u2014 <code>null_threshold</code>, <code>unique_threshold</code>, <code>enum_max_values</code>, <code>pattern_sample_size</code>, <code>pattern_min_confidence</code></p> <p><code>duckguard profile</code> CLI Command:</p> <pre><code>duckguard profile data.csv\nduckguard profile data.csv --deep\nduckguard profile data.csv --format json -o profile.json\n</code></pre>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li><code>ColumnProfile</code>: 10 new optional fields (backward-compatible <code>None</code> defaults)</li> <li><code>ProfileResult</code>: 2 new fields (<code>overall_quality_score</code>, <code>overall_quality_grade</code>)</li> <li>AutoProfiler delegates pattern detection to <code>PatternMatcher</code></li> </ul>"},{"location":"changelog/#300-2026-01-27","title":"[3.0.0] \u2014 2026-01-27","text":"<p>Major feature release \u2014 23 new check types, enterprise-grade security, 100% backward compatible with 2.x.</p>"},{"location":"changelog/#added_2","title":"Added","text":"<p>Conditional Expectations (5 check types) \u2014 validate only when a SQL condition is met:</p> <ul> <li><code>not_null_when</code>, <code>unique_when</code>, <code>between_when</code>, <code>isin_when</code>, <code>matches_when</code></li> </ul> <p>Multi-Column Expectations (8 check types) \u2014 cross-column validation:</p> <ul> <li><code>expect_column_pair_satisfy</code>, <code>expect_columns_unique</code>, <code>expect_multicolumn_sum_to_equal</code></li> <li>Column comparison operators: <code>column_a_gt_b</code>, <code>column_a_gte_b</code>, <code>column_a_lt_b</code>, <code>column_a_lte_b</code>, <code>column_a_eq_b</code></li> </ul> <p>Query-Based Expectations (6 check types) \u2014 custom SQL validation:</p> <ul> <li><code>expect_query_to_return_no_rows</code>, <code>expect_query_to_return_rows</code></li> <li><code>expect_query_result_to_equal</code>, <code>expect_query_result_to_be_between</code></li> <li><code>query_result_gt</code>, <code>query_result_lt</code></li> </ul> <p>Distributional Checks (4 check types) \u2014 statistical tests:</p> <ul> <li><code>expect_distribution_normal</code>, <code>expect_distribution_uniform</code></li> <li><code>expect_ks_test</code>, <code>expect_chi_square_test</code></li> </ul> <p>Enhanced Profiling:</p> <ul> <li>DistributionAnalyzer \u2014 best-fit distribution, skewness, kurtosis, normality testing</li> <li>OutlierDetector \u2014 Z-score, IQR, Isolation Forest, LOF, consensus detection</li> <li>PatternMatcher \u2014 25+ built-in patterns with confidence scoring</li> <li>QualityScorer \u2014 multi-dimensional quality grading (A\u2013F)</li> </ul> <p>Security:</p> <ul> <li>Multi-layer SQL injection prevention</li> <li>QueryValidator + QuerySecurityValidator + ExpressionParser</li> <li>80+ security tests, OWASP Top 10 compliance</li> <li>READ-ONLY enforcement, 30s timeout, 10K row limit</li> </ul>"},{"location":"changelog/#performance","title":"Performance","text":"<p>Benchmarks on 1M rows: conditional 2.1s, multi-column 3.8s, query 2.5\u20137.2s, distributional 8.3s.</p>"},{"location":"changelog/#dependencies","title":"Dependencies","text":"<ul> <li><code>scipy&gt;=1.11.0</code> (optional) \u2014 <code>pip install 'duckguard[statistics]'</code></li> <li><code>scikit-learn&gt;=1.3.0</code> (optional) \u2014 <code>pip install 'duckguard[profiling]'</code></li> </ul>"},{"location":"changelog/#230-2025-01-25","title":"[2.3.0] \u2014 2025-01-25","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Cross-dataset validation: <code>exists_in()</code>, <code>references()</code>, <code>find_orphans()</code>, <code>matches_values()</code></li> <li>Dataset reconciliation: <code>reconcile()</code> with key matching, value comparison, tolerance</li> <li>Distribution drift: <code>detect_drift()</code> using Kolmogorov-Smirnov test</li> <li>Group-by checks: <code>group_by().row_count_greater_than()</code>, <code>stats()</code>, <code>validate()</code></li> <li>Row count comparison: <code>row_count_matches()</code> between datasets</li> </ul>"},{"location":"changelog/#221-2025-01-24","title":"[2.2.1] \u2014 2025-01-24","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Resolved lint errors and minor bug fixes</li> </ul>"},{"location":"changelog/#220-2025-01-23","title":"[2.2.0] \u2014 2025-01-23","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Initial public release with core features:</li> <li>Data profiling and quality scoring</li> <li>YAML-based validation rules</li> <li>Semantic type detection and PII detection</li> <li>Data contracts (generate, validate, diff)</li> <li>Anomaly detection (z-score, IQR, baseline, KS test)</li> <li>CLI interface with Rich output</li> <li>pytest integration</li> <li>Connectors: CSV, Parquet, JSON, Excel, S3, GCS, Azure, PostgreSQL, MySQL, SQLite, Snowflake, BigQuery, Redshift, SQL Server, Databricks, Oracle, MongoDB, Kafka</li> <li>HTML/PDF report generation</li> <li>Slack, Teams, Email notifications</li> <li>Historical tracking and trend analysis</li> </ul>"},{"location":"cli/","title":"CLI Reference","text":"<p>DuckGuard provides a full-featured command-line interface powered by Typer and Rich.</p>"},{"location":"cli/#installation","title":"Installation","text":"<pre><code>pip install duckguard\n</code></pre>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#duckguard-check","title":"<code>duckguard check</code>","text":"<p>Run data quality checks on a data source.</p> <pre><code>duckguard check &lt;source&gt; [options]\n</code></pre> Option Description <code>--config</code>, <code>-c</code> Path to duckguard.yaml rules file <code>--table</code>, <code>-t</code> Table name (for databases) <code>--not-null</code>, <code>-n</code> Columns that must not be null (repeatable) <code>--unique</code>, <code>-u</code> Columns that must be unique (repeatable) <code>--output</code>, <code>-o</code> Output file (JSON) <code>--verbose</code>, <code>-V</code> Verbose output <pre><code># Quick checks\nduckguard check data.csv\nduckguard check data.csv --not-null id --unique email\n\n# YAML rules\nduckguard check data.csv --config duckguard.yaml\n\n# Database\nduckguard check postgres://localhost/db --table orders\n\n# Save results\nduckguard check data.csv --output results.json\n</code></pre> <p>Exit code: <code>0</code> if all checks pass, <code>1</code> if any fail.</p>"},{"location":"cli/#duckguard-discover","title":"<code>duckguard discover</code>","text":"<p>Analyze data and auto-generate validation rules.</p> <pre><code>duckguard discover &lt;source&gt; [options]\n</code></pre> Option Description <code>--table</code>, <code>-t</code> Table name <code>--output</code>, <code>-o</code> Output file for rules (YAML) <code>--format</code>, <code>-f</code> Output format: yaml, python <pre><code>duckguard discover data.csv\nduckguard discover data.csv --output duckguard.yaml\n</code></pre>"},{"location":"cli/#duckguard-profile","title":"<code>duckguard profile</code>","text":"<p>Profile a dataset \u2014 statistics, patterns, quality scores.</p> <pre><code>duckguard profile &lt;source&gt; [options]\n</code></pre> Option Description <code>--table</code>, <code>-t</code> Table name <code>--deep</code>, <code>-d</code> Enable deep profiling (distributions, outliers) <code>--format</code>, <code>-f</code> Output format: table (default), json <code>--output</code>, <code>-o</code> Output file (JSON) <pre><code>duckguard profile data.csv\nduckguard profile data.csv --deep\nduckguard profile data.csv --format json -o profile.json\n</code></pre>"},{"location":"cli/#duckguard-contract","title":"<code>duckguard contract</code>","text":"<p>Manage data contracts.</p> <pre><code>duckguard contract &lt;action&gt; &lt;source&gt; [options]\n</code></pre> <p>Actions: <code>generate</code>, <code>validate</code>, <code>diff</code></p> Option Description <code>--contract</code>, <code>-c</code> Contract file path <code>--output</code>, <code>-o</code> Output file <code>--strict</code> Strict validation mode <pre><code># Generate from data\nduckguard contract generate data.csv --output orders.contract.yaml\n\n# Validate data against contract\nduckguard contract validate data.csv --contract orders.contract.yaml\n\n# Compare two versions\nduckguard contract diff old.contract.yaml new.contract.yaml\n</code></pre>"},{"location":"cli/#duckguard-anomaly","title":"<code>duckguard anomaly</code>","text":"<p>Detect anomalies in data.</p> <pre><code>duckguard anomaly &lt;source&gt; [options]\n</code></pre> Option Description <code>--method</code>, <code>-m</code> Method: zscore (default), iqr, percent_change, baseline, ks_test <code>--threshold</code> Detection threshold <code>--column</code>, <code>-c</code> Specific columns (repeatable) <code>--learn-baseline</code>, <code>-L</code> Learn and store baseline <pre><code>duckguard anomaly data.csv\nduckguard anomaly data.csv --method iqr --threshold 2.0\nduckguard anomaly data.csv --column amount --column quantity\nduckguard anomaly data.csv --learn-baseline\nduckguard anomaly data.csv --method baseline\n</code></pre>"},{"location":"cli/#duckguard-report","title":"<code>duckguard report</code>","text":"<p>Generate HTML or PDF quality reports.</p> <pre><code>duckguard report &lt;source&gt; [options]\n</code></pre> Option Description <code>--config</code>, <code>-c</code> YAML rules file <code>--format</code>, <code>-f</code> html (default), pdf <code>--output</code>, <code>-o</code> Output file <code>--title</code> Report title <code>--store</code>, <code>-s</code> Store results in history <code>--trends</code> Include trend charts <code>--dark-mode</code> auto, light, dark <code>--logo</code> Logo URL for header <pre><code>duckguard report data.csv --output report.html\nduckguard report data.csv --format pdf --output report.pdf\nduckguard report data.csv --store --trends --title \"Daily Report\"\n</code></pre>"},{"location":"cli/#duckguard-freshness","title":"<code>duckguard freshness</code>","text":"<p>Check data freshness.</p> <pre><code>duckguard freshness &lt;source&gt; [options]\n</code></pre> Option Description <code>--column</code>, <code>-c</code> Timestamp column <code>--max-age</code>, <code>-m</code> Max age: 1h, 6h, 24h, 7d (default: 24h) <code>--format</code>, <code>-f</code> table (default), json <pre><code>duckguard freshness data.csv\nduckguard freshness data.csv --max-age 6h\nduckguard freshness data.csv --column updated_at\n</code></pre>"},{"location":"cli/#duckguard-schema","title":"<code>duckguard schema</code>","text":"<p>Track schema evolution.</p> <pre><code>duckguard schema &lt;source&gt; [options]\n</code></pre> Option Description <code>--action</code>, <code>-a</code> show, capture, history, changes <code>--format</code>, <code>-f</code> table, json <code>--limit</code>, <code>-l</code> Number of results <pre><code>duckguard schema data.csv\nduckguard schema data.csv --action capture\nduckguard schema data.csv --action history\nduckguard schema data.csv --action changes\n</code></pre>"},{"location":"cli/#duckguard-history","title":"<code>duckguard history</code>","text":"<p>Query historical validation results.</p> <pre><code>duckguard history [source] [options]\n</code></pre> Option Description <code>--last</code>, <code>-l</code> Time period: 7d, 30d, 90d <code>--format</code>, <code>-f</code> table, json <code>--trend</code>, <code>-t</code> Show trend analysis <pre><code>duckguard history\nduckguard history data.csv --last 7d\nduckguard history data.csv --trend\n</code></pre>"},{"location":"cli/#duckguard-info","title":"<code>duckguard info</code>","text":"<p>Display information about a data source.</p> <pre><code>duckguard info &lt;source&gt; [--table TABLE]\n</code></pre> <pre><code>duckguard info data.csv\nduckguard info postgres://localhost/db --table users\n</code></pre>"},{"location":"cli/#global-options","title":"Global Options","text":"Option Description <code>--version</code>, <code>-v</code> Show version <code>--help</code> Show help"},{"location":"api/","title":"API Reference","text":"<p>Core classes and functions exported by <code>duckguard</code>.</p>"},{"location":"api/#entry-point","title":"Entry Point","text":"<pre><code>from duckguard import connect\n\ndata = connect(\"data.csv\")\n</code></pre>"},{"location":"api/#connectsource-tablenone-schemanone-databasenone-options-dataset","title":"<code>connect(source, *, table=None, schema=None, database=None, **options) \u2192 Dataset</code>","text":"<p>Connect to any data source. Auto-detects format from file extension or connection string prefix.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#dataset","title":"Dataset","text":"<pre><code>from duckguard import Dataset\n</code></pre> Property / Method Returns Description <code>row_count</code> <code>int</code> Number of rows <code>columns</code> <code>list[str]</code> Column names <code>column_count</code> <code>int</code> Number of columns <code>source</code> <code>str</code> Source path <code>name</code> <code>str</code> Dataset name <code>data.column_name</code> <code>Column</code> Access column by attribute <code>data[\"column_name\"]</code> <code>Column</code> Access column by bracket <code>column(name)</code> <code>Column</code> Access column by method <code>has_column(name)</code> <code>bool</code> Check column exists <code>head(n=5)</code> <code>list[dict]</code> First n rows <code>sample(n=10)</code> <code>list[dict]</code> Sample n rows <code>execute_sql(sql)</code> <code>list[tuple]</code> Run custom SQL <code>score(weights=None)</code> <code>QualityScore</code> Quality score <code>freshness</code> <code>FreshnessResult</code> Freshness info <code>is_fresh(max_age)</code> <code>bool</code> Freshness check <code>group_by(columns)</code> <code>GroupedDataset</code> Group for validation <code>reconcile(target, ...)</code> <code>ReconciliationResult</code> Compare datasets <code>row_count_matches(other, tolerance)</code> <code>ValidationResult</code> Compare row counts"},{"location":"api/#multi-column-methods","title":"Multi-Column Methods","text":"Method Description <code>expect_column_pair_satisfy(column_a, column_b, expression, threshold)</code> Column pair relationship <code>expect_columns_unique(columns, threshold)</code> Composite key uniqueness <code>expect_multicolumn_sum_to_equal(columns, expected_sum, threshold)</code> Sum constraint"},{"location":"api/#query-methods","title":"Query Methods","text":"Method Description <code>expect_query_to_return_no_rows(query, message)</code> Query returns 0 rows <code>expect_query_to_return_rows(query, message)</code> Query returns \u22651 row <code>expect_query_result_to_equal(query, expected, tolerance)</code> Scalar result match <code>expect_query_result_to_be_between(query, min_value, max_value)</code> Scalar in range"},{"location":"api/#column","title":"Column","text":"<p>Accessed via <code>dataset.column_name</code> or <code>dataset[\"column_name\"]</code>.</p> Property / Method Description <code>null_count</code>, <code>null_percent</code> Null statistics <code>unique_count</code>, <code>unique_percent</code> Uniqueness statistics <code>total_count</code> Non-null count <code>min</code>, <code>max</code>, <code>mean</code>, <code>median</code> Numeric statistics <code>between(min, max)</code> Range validation <code>isin(values)</code> Allowed values check <code>matches(pattern)</code> Regex pattern check <code>has_no_duplicates()</code> Uniqueness check <code>greater_than(value)</code> Minimum check <code>less_than(value)</code> Maximum check <code>not_null_when(condition)</code> Conditional null check <code>unique_when(condition)</code> Conditional uniqueness <code>between_when(min, max, condition)</code> Conditional range <code>isin_when(values, condition)</code> Conditional allowed values <code>matches_when(pattern, condition)</code> Conditional pattern <code>expect_distribution_normal()</code> Normal distribution test <code>expect_distribution_uniform()</code> Uniform distribution test <code>expect_ks_test(distribution)</code> KS test <code>expect_chi_square_test()</code> Chi-square test"},{"location":"api/#validationresult","title":"ValidationResult","text":"<pre><code>from duckguard import ValidationResult\n</code></pre> Property / Method Description <code>passed</code> <code>bool</code> \u2014 check passed <code>actual_value</code> Actual value found <code>expected_value</code> Expected value <code>message</code> Summary string <code>details</code> Metadata dict <code>failed_rows</code> <code>list[FailedRow]</code> \u2014 sample failures <code>total_failures</code> Total failure count <code>summary()</code> Human-readable summary <code>to_dataframe()</code> Export to pandas DataFrame <code>get_failed_values()</code> List of bad values <code>get_failed_row_indices()</code> List of row indices"},{"location":"api/#qualityscore","title":"QualityScore","text":"<pre><code>score = data.score()\nscore.overall       # 0-100\nscore.grade         # A, B, C, D, F\nscore.completeness  # Dimension score\nscore.uniqueness    # Dimension score\nscore.validity      # Dimension score\nscore.consistency   # Dimension score\n</code></pre>"},{"location":"api/#top-level-functions","title":"Top-Level Functions","text":"Function Description <code>connect(source)</code> Connect to data source <code>profile(dataset)</code> Profile dataset <code>load_rules(path)</code> Load YAML rules <code>execute_rules(ruleset)</code> Execute rules <code>generate_rules(dataset)</code> Auto-generate rules <code>load_contract(path)</code> Load data contract <code>validate_contract(contract, source)</code> Validate against contract <code>generate_contract(source)</code> Auto-generate contract <code>diff_contracts(old, new)</code> Compare contracts <code>detect_type(dataset, column)</code> Detect semantic type <code>detect_types_for_dataset(dataset)</code> Detect all types <code>detect_anomalies(dataset)</code> Anomaly detection <code>score(dataset)</code> Quality score"},{"location":"blog/why-we-built-duckguard/","title":"Why We Built DuckGuard","text":"<p>January 2026</p>"},{"location":"blog/why-we-built-duckguard/#the-frustration","title":"The Frustration","text":"<p>I've been a data engineer for years. Every project, the same ritual: set up data quality checks. Every time, the same pain.</p> <p>Great Expectations wants you to understand contexts, datasources, assets, batch requests, expectation suites, and validators before you can check if a column has nulls. That's not quality engineering \u2014 that's ceremony.</p> <p>Soda Core is better but pushes you into YAML-first configuration. Want to do something beyond the built-in checks? Good luck.</p> <p>Pandera is clean but pandas-only. The moment your data doesn't fit in memory, you're stuck.</p> <p>Every tool in this space makes the same mistake: they're designed for the tool's architecture, not for the engineer's workflow.</p>"},{"location":"blog/why-we-built-duckguard/#the-insight","title":"The Insight","text":"<p>I write tests every day. <code>pytest</code> doesn't ask me to configure a test context, register a test source, build a test batch, and initialize a test validator. I just write:</p> <pre><code>def test_something():\n    assert result == expected\n</code></pre> <p>Why can't data quality work the same way?</p> <p>And separately: every data tool loads everything into pandas DataFrames. For a 1 GB CSV, that means 4 GB of RAM and 45 seconds of waiting. But DuckDB can scan that same file in 4 seconds using 200 MB. The technology exists \u2014 nobody's wiring it up for data quality.</p>"},{"location":"blog/why-we-built-duckguard/#the-result","title":"The Result","text":"<pre><code>from duckguard import connect\n\norders = connect(\"orders.csv\")\nassert orders.customer_id.is_not_null()\nassert orders.total_amount.between(0, 10000)\n</code></pre> <p>That's DuckGuard. Three lines. Reads like English. Runs on DuckDB.</p> <p>No boilerplate. No configuration ceremony. No memory explosion. Just connect and validate.</p>"},{"location":"blog/why-we-built-duckguard/#what-we-learned-building-it","title":"What We Learned Building It","text":""},{"location":"blog/why-we-built-duckguard/#1-the-api-is-the-product","title":"1. The API is the product","text":"<p>We spent more time on the API surface than on the engine. Every method name was debated. <code>is_not_null()</code> not <code>expect_column_values_to_not_be_null()</code>. <code>between(0, 100)</code> not <code>expect_column_values_to_be_between(column, 0, 100)</code>.</p> <p>Data quality checks should read like assertions, not Java method signatures from 2005.</p>"},{"location":"blog/why-we-built-duckguard/#2-speed-changes-behavior","title":"2. Speed changes behavior","text":"<p>When validation takes 45 seconds, engineers run it in CI and forget about it. When it takes 4 seconds, they run it interactively. They explore. They add more checks. Speed isn't just a feature \u2014 it changes how people work with data quality.</p>"},{"location":"blog/why-we-built-duckguard/#3-batteries-matter","title":"3. Batteries matter","text":"<p>PII detection, anomaly detection, data contracts, drift detection, reconciliation \u2014 these aren't nice-to-haves. They're the things engineers build themselves (badly) because their data quality tool doesn't include them. We built them in from the start.</p>"},{"location":"blog/why-we-built-duckguard/#4-row-level-errors-change-everything","title":"4. Row-level errors change everything","text":"<p>\"3% of values failed\" is useless. \"Row 5: quantity=500, Row 23: quantity=-2\" is actionable. Every DuckGuard validation returns the specific rows that failed. This alone saves hours of debugging.</p>"},{"location":"blog/why-we-built-duckguard/#the-numbers","title":"The Numbers","text":"Metric Great Expectations DuckGuard Lines to start 50+ 3 1 GB CSV 45 sec / 4 GB 4 sec / 200 MB 10 GB Parquet 8 min / 32 GB 45 sec / 2 GB Dependencies 20+ 7 Learning curve Days Minutes"},{"location":"blog/why-we-built-duckguard/#whats-next","title":"What's Next","text":"<p>DuckGuard 3.0 shipped with conditional checks, multi-column validation, query-based expectations, and 7 anomaly detection methods. We're working on:</p> <ul> <li>AI-powered rule suggestion \u2014 point DuckGuard at your data, get validation rules automatically</li> <li>Streaming validation \u2014 check data quality in real-time pipelines</li> <li>Deeper integrations \u2014 native dbt tests, Dagster assets, Prefect flows</li> </ul> <p>We're building DuckGuard in the open. Star us on GitHub, try it on your data, and tell us what sucks.</p> <pre><code>pip install duckguard\n</code></pre> <p>DuckGuard is open-source under the Apache License 2.0. Built by the XDataHubAI team.</p>"},{"location":"connectors/cloud-storage/","title":"Cloud Storage","text":"<p>Connect to files on S3, GCS, and Azure Blob Storage.</p>"},{"location":"connectors/cloud-storage/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\n# AWS S3\ndata = connect(\"s3://my-bucket/orders.parquet\")\n\n# Google Cloud Storage\ndata = connect(\"gs://my-bucket/data.csv\")\n\n# Azure Blob Storage\ndata = connect(\"az://my-container/data.parquet\")\n</code></pre>"},{"location":"connectors/cloud-storage/#aws-s3","title":"AWS S3","text":"<p>DuckGuard uses DuckDB's <code>httpfs</code> extension for S3 access.</p> <pre><code>data = connect(\"s3://my-bucket/orders.parquet\")\ndata = connect(\"s3://my-bucket/data/orders.csv\")\n</code></pre>"},{"location":"connectors/cloud-storage/#authentication","title":"Authentication","text":"<p>DuckDB picks up AWS credentials automatically from:</p> <ol> <li>Environment variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>AWS_DEFAULT_REGION</code></li> <li>AWS credentials file: <code>~/.aws/credentials</code></li> <li>IAM roles (EC2, ECS, Lambda)</li> </ol> <pre><code># Environment variables\nexport AWS_ACCESS_KEY_ID=AKIA...\nexport AWS_SECRET_ACCESS_KEY=secret...\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"connectors/cloud-storage/#s3-compatible-storage","title":"S3-Compatible Storage","text":"<p>Works with MinIO, LocalStack, and other S3-compatible services:</p> <pre><code># MinIO / LocalStack\ndata = connect(\"s3://bucket/data.parquet\")\n# Configure endpoint via DuckDB settings or env vars\n</code></pre>"},{"location":"connectors/cloud-storage/#google-cloud-storage","title":"Google Cloud Storage","text":"<pre><code>data = connect(\"gs://my-bucket/orders.parquet\")\ndata = connect(\"gs://my-bucket/data/events.csv\")\n</code></pre>"},{"location":"connectors/cloud-storage/#authentication_1","title":"Authentication","text":"<p>Uses Google Cloud credentials from:</p> <ol> <li>Environment variable: <code>GOOGLE_APPLICATION_CREDENTIALS</code> (path to service account JSON)</li> <li>Default credentials: <code>gcloud auth application-default login</code></li> <li>Service account on GCP compute</li> </ol> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n</code></pre>"},{"location":"connectors/cloud-storage/#azure-blob-storage","title":"Azure Blob Storage","text":"<pre><code>data = connect(\"az://my-container/orders.parquet\")\ndata = connect(\"abfss://container@account.dfs.core.windows.net/data.parquet\")\n</code></pre>"},{"location":"connectors/cloud-storage/#authentication_2","title":"Authentication","text":"<p>Uses Azure credentials from:</p> <ol> <li>Environment variables: <code>AZURE_STORAGE_CONNECTION_STRING</code> or <code>AZURE_STORAGE_ACCOUNT</code> + <code>AZURE_STORAGE_KEY</code></li> <li>Azure CLI: <code>az login</code></li> <li>Managed identity on Azure compute</li> </ol> <pre><code>export AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;...\"\n</code></pre>"},{"location":"connectors/cloud-storage/#supported-file-formats","title":"Supported File Formats","text":"<p>All cloud connectors support the same formats as local files:</p> Format S3 GCS Azure CSV \u2705 \u2705 \u2705 Parquet \u2705 \u2705 \u2705 JSON/JSONL \u2705 \u2705 \u2705"},{"location":"connectors/cloud-storage/#usage-patterns","title":"Usage Patterns","text":"<pre><code>data = connect(\"s3://analytics/orders/2024/orders.parquet\")\n\n# Same Dataset API as local files\nassert data.row_count &gt; 0\nassert data.order_id.null_percent == 0\n\n# Quality scoring\nscore = data.score()\nprint(f\"Quality: {score.grade}\")\n\n# Profiling\nfrom duckguard import profile\nresult = profile(data)\n</code></pre>"},{"location":"connectors/cloud-storage/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use Parquet on cloud storage \u2014 columnar format means DuckDB only reads the columns it needs</li> <li>Partition data by date/region for faster scans</li> <li>Use regional buckets close to your compute for lower latency</li> <li>DuckDB handles parallel reads automatically</li> </ul>"},{"location":"connectors/databases/","title":"Database Connectors","text":"<p>Connect to PostgreSQL, MySQL, SQLite, Snowflake, BigQuery, and more.</p>"},{"location":"connectors/databases/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\n# PostgreSQL\ndata = connect(\"postgres://user:pass@host:5432/mydb\", table=\"orders\")\n\n# MySQL\ndata = connect(\"mysql://user:pass@host:3306/mydb\", table=\"users\")\n\n# SQLite\ndata = connect(\"sqlite:///local.db\", table=\"events\")\ndata = connect(\"local.db\", table=\"events\")  # .db extension auto-detected\n</code></pre>"},{"location":"connectors/databases/#postgresql","title":"PostgreSQL","text":"<pre><code>pip install 'duckguard[postgres]'\n</code></pre> <pre><code>data = connect(\n    \"postgres://user:password@localhost:5432/mydb\",\n    table=\"orders\",\n    schema=\"public\"\n)\n\nassert data.row_count &gt; 0\nassert data.order_id.null_percent == 0\n</code></pre>"},{"location":"connectors/databases/#mysql","title":"MySQL","text":"<pre><code>pip install 'duckguard[mysql]'\n</code></pre> <pre><code>data = connect(\n    \"mysql://user:password@localhost:3306/mydb\",\n    table=\"users\"\n)\n</code></pre>"},{"location":"connectors/databases/#sqlite","title":"SQLite","text":"<p>No extra dependencies needed \u2014 SQLite is built-in:</p> <pre><code>data = connect(\"sqlite:///path/to/database.db\", table=\"events\")\n\n# Auto-detected by file extension\ndata = connect(\"database.db\", table=\"events\")\ndata = connect(\"database.sqlite\", table=\"events\")\ndata = connect(\"database.sqlite3\", table=\"events\")\n</code></pre>"},{"location":"connectors/databases/#snowflake","title":"Snowflake","text":"<pre><code>pip install 'duckguard[snowflake]'\n</code></pre> <pre><code>data = connect(\n    \"snowflake://account/database\",\n    table=\"orders\",\n    schema=\"public\"\n)\n</code></pre>"},{"location":"connectors/databases/#bigquery","title":"BigQuery","text":"<pre><code>pip install 'duckguard[bigquery]'\n</code></pre> <pre><code>data = connect(\n    \"bigquery://project-id\",\n    table=\"orders\",\n    schema=\"dataset_name\"\n)\n</code></pre>"},{"location":"connectors/databases/#redshift","title":"Redshift","text":"<pre><code>pip install 'duckguard[postgres]'\n</code></pre> <pre><code>data = connect(\n    \"redshift://cluster.redshift.amazonaws.com:5439/mydb\",\n    table=\"orders\"\n)\n</code></pre>"},{"location":"connectors/databases/#sql-server","title":"SQL Server","text":"<pre><code>pip install 'duckguard[sqlserver]'\n</code></pre> <pre><code>data = connect(\n    \"mssql://user:pass@host:1433/mydb\",\n    table=\"orders\"\n)\n</code></pre>"},{"location":"connectors/databases/#databricks","title":"Databricks","text":"<pre><code>pip install 'duckguard[databricks]'\n</code></pre> <pre><code>data = connect(\n    \"databricks://workspace.databricks.com\",\n    table=\"orders\"\n)\n</code></pre>"},{"location":"connectors/databases/#microsoft-fabric","title":"Microsoft Fabric","text":"<pre><code>pip install 'duckguard[fabric]'\n</code></pre> <pre><code># OneLake (Parquet/Delta in Lakehouse)\ndata = connect(\n    \"fabric://workspace/lakehouse/Tables/orders\",\n    token=\"eyJ...\"\n)\n\n# SQL endpoint\ndata = connect(\n    \"fabric+sql://workspace-guid.datawarehouse.fabric.microsoft.com\",\n    table=\"orders\",\n    database=\"my_lakehouse\",\n    token=\"eyJ...\"\n)\n</code></pre>"},{"location":"connectors/databases/#oracle","title":"Oracle","text":"<pre><code>pip install 'duckguard[oracle]'\n</code></pre> <pre><code>data = connect(\"oracle://user:pass@host:1521/mydb\", table=\"orders\")\n</code></pre>"},{"location":"connectors/databases/#mongodb","title":"MongoDB","text":"<pre><code>pip install 'duckguard[mongodb]'\n</code></pre> <pre><code>data = connect(\"mongodb://localhost:27017/mydb\", table=\"orders\")\ndata = connect(\"mongodb+srv://user:pass@cluster.mongodb.net/mydb\", table=\"orders\")\n</code></pre>"},{"location":"connectors/databases/#kafka","title":"Kafka","text":"<pre><code>pip install 'duckguard[kafka]'\n</code></pre> <pre><code>data = connect(\"kafka://localhost:9092\", table=\"orders-topic\")\n</code></pre>"},{"location":"connectors/databases/#common-parameters","title":"Common Parameters","text":"<p>All database connectors support:</p> Parameter Description <code>table</code> Table name (required) <code>schema</code> Schema/namespace <code>database</code> Database name <pre><code>data = connect(\n    \"postgres://localhost/mydb\",\n    table=\"orders\",\n    schema=\"analytics\",\n    database=\"warehouse\"\n)\n</code></pre>"},{"location":"connectors/databases/#install-all","title":"Install All","text":"<pre><code>pip install 'duckguard[all]'\n</code></pre> <p>This installs drivers for all supported databases.</p>"},{"location":"connectors/files/","title":"File Connectors","text":"<p>Connect to CSV, Parquet, JSON, and Excel files.</p>"},{"location":"connectors/files/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\n# CSV\norders = connect(\"data/orders.csv\")\n\n# Parquet\norders = connect(\"data/orders.parquet\")\n\n# JSON / JSONL\nevents = connect(\"data/events.json\")\nlogs = connect(\"data/logs.jsonl\")\n\n# Excel\nreport = connect(\"data/report.xlsx\")\n</code></pre>"},{"location":"connectors/files/#csv","title":"CSV","text":"<p>DuckGuard uses DuckDB's CSV reader, which auto-detects delimiters, headers, and types.</p> <pre><code># Standard CSV\ndata = connect(\"data.csv\")\n\n# Tab-separated\ndata = connect(\"data.tsv\")\n\n# Custom options via DuckDB\ndata = connect(\"data.csv\")\nassert data.row_count &gt; 0\n</code></pre> <p>Auto-detected: delimiter, header row, column types, quoting, encoding.</p>"},{"location":"connectors/files/#parquet","title":"Parquet","text":"<p>Parquet is the recommended format \u2014 it's fastest and preserves types.</p> <pre><code>data = connect(\"data.parquet\")\n\n# Partitioned parquet (directory of files)\ndata = connect(\"data/year=2024/month=01/\")\n</code></pre> <p>Parquet preserves exact types (integers, decimals, timestamps) without any parsing overhead.</p>"},{"location":"connectors/files/#json","title":"JSON","text":"<p>Supports standard JSON arrays and newline-delimited JSON (JSONL/NDJSON):</p> <pre><code># JSON array: [{\"id\": 1, ...}, {\"id\": 2, ...}]\ndata = connect(\"data.json\")\n\n# Newline-delimited JSON (one object per line)\ndata = connect(\"logs.jsonl\")\ndata = connect(\"events.ndjson\")\n</code></pre>"},{"location":"connectors/files/#excel","title":"Excel","text":"<pre><code>data = connect(\"report.xlsx\")\ndata = connect(\"legacy.xls\")\n</code></pre> <p>Note</p> <p>Excel support uses DuckDB's <code>spatial</code> extension. Large Excel files may be slower than CSV/Parquet.</p>"},{"location":"connectors/files/#file-paths","title":"File Paths","text":"<p>DuckGuard accepts relative and absolute paths:</p> <pre><code># Relative to current directory\ndata = connect(\"data/orders.csv\")\n\n# Absolute path\ndata = connect(\"/home/user/data/orders.csv\")\n\n# Glob patterns (via DuckDB)\ndata = connect(\"data/*.parquet\")\n</code></pre>"},{"location":"connectors/files/#working-with-the-dataset","title":"Working with the Dataset","text":"<p>All file connectors return the same <code>Dataset</code> object:</p> <pre><code>data = connect(\"orders.csv\")\n\n# Basic info\nprint(data.row_count)       # 10000\nprint(data.columns)         # ['id', 'amount', 'status', ...]\nprint(data.column_count)    # 5\n\n# Access columns\nprint(data.amount.null_percent)\nprint(data.amount.min)\nprint(data.amount.max)\n\n# Sample data\nrows = data.head(5)\nprint(rows)\n\n# Run validations\nassert data.order_id.null_percent == 0\nassert data.amount.between(0, 10000)\n</code></pre>"},{"location":"connectors/files/#performance-tips","title":"Performance Tips","text":"Format Read Speed File Size Type Safety Parquet \u26a1 Fastest Small Full CSV Fast Large Inferred JSON Moderate Large Inferred Excel Slowest Medium Inferred <p>Recommendation: Convert CSV/JSON to Parquet for repeated validation runs.</p>"},{"location":"connectors/modern-formats/","title":"Modern Formats","text":"<p>Connect to Delta Lake and Apache Iceberg tables.</p>"},{"location":"connectors/modern-formats/#delta-lake","title":"Delta Lake","text":"<p>DuckGuard reads Delta Lake tables through DuckDB's Delta extension.</p>"},{"location":"connectors/modern-formats/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\n# Local Delta table\ndata = connect(\"path/to/delta_table/\")\n\n# Delta on S3\ndata = connect(\"s3://bucket/delta_table/\")\n</code></pre>"},{"location":"connectors/modern-formats/#time-travel","title":"Time Travel","text":"<p>DuckDB's Delta support enables reading specific versions:</p> <pre><code># Read Delta table at a specific version via DuckDB SQL\ndata = connect(\"delta_table/\")\nrows = data.execute_sql(\n    \"SELECT * FROM delta_scan('{source}', version=5)\"\n)\n</code></pre>"},{"location":"connectors/modern-formats/#requirements","title":"Requirements","text":"<p>Delta Lake support requires the DuckDB <code>delta</code> extension, which loads automatically on first use.</p>"},{"location":"connectors/modern-formats/#validation","title":"Validation","text":"<pre><code>data = connect(\"s3://datalake/orders_delta/\")\n\n# Same API as any other source\nassert data.row_count &gt; 0\nassert data.order_id.null_percent == 0\n\nscore = data.score()\nprint(f\"Quality: {score.grade}\")\n</code></pre>"},{"location":"connectors/modern-formats/#apache-iceberg","title":"Apache Iceberg","text":"<p>DuckGuard reads Iceberg tables through DuckDB's Iceberg extension.</p>"},{"location":"connectors/modern-formats/#quick-start_1","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\n# Local Iceberg table\ndata = connect(\"path/to/iceberg_table/\")\n\n# Iceberg on S3\ndata = connect(\"s3://bucket/iceberg_table/\")\n</code></pre>"},{"location":"connectors/modern-formats/#requirements_1","title":"Requirements","text":"<p>Iceberg support requires the DuckDB <code>iceberg</code> extension.</p>"},{"location":"connectors/modern-formats/#validation_1","title":"Validation","text":"<pre><code>data = connect(\"s3://warehouse/orders_iceberg/\")\n\nassert data.row_count &gt; 0\nresult = data.amount.between(0, 100000)\nassert result.passed\n</code></pre>"},{"location":"connectors/modern-formats/#comparison","title":"Comparison","text":"Feature Delta Lake Iceberg ACID transactions \u2705 \u2705 Time travel \u2705 \u2705 Schema evolution \u2705 \u2705 Partition pruning \u2705 \u2705 DuckDB extension <code>delta</code> <code>iceberg</code> Cloud support S3, GCS, Azure S3, GCS, Azure"},{"location":"connectors/modern-formats/#practical-patterns","title":"Practical Patterns","text":""},{"location":"connectors/modern-formats/#quality-gate-for-lakehouse","title":"Quality gate for lakehouse","text":"<pre><code>from duckguard import connect, load_rules, execute_rules\n\ndata = connect(\"s3://datalake/orders/\")\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, dataset=data)\n\nif not result.passed:\n    raise RuntimeError(f\"Quality gate failed: {result.failed_count} checks\")\n</code></pre>"},{"location":"connectors/modern-formats/#compare-delta-versions","title":"Compare Delta versions","text":"<pre><code>current = connect(\"s3://lake/orders/\")\nbackup = connect(\"s3://lake/orders_backup/\")\n\nresult = current.row_count_matches(backup, tolerance=100)\nassert result.passed\n</code></pre>"},{"location":"connectors/modern-formats/#profile-a-delta-table","title":"Profile a Delta table","text":"<pre><code>from duckguard import connect, profile\n\ndata = connect(\"s3://lake/events/\")\nresult = profile(data, deep=True)\n\nfor col in result.columns:\n    print(f\"{col.name}: {col.quality_grade} ({col.null_percent:.1f}% nulls)\")\n</code></pre>"},{"location":"connectors/overview/","title":"Connectors Overview","text":"<p>DuckGuard connects to anything through a single <code>connect()</code> function. It auto-detects the source type.</p>"},{"location":"connectors/overview/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\n# Local files\ndata = connect(\"orders.csv\")\ndata = connect(\"data/orders.parquet\")\ndata = connect(\"reports/data.json\")\ndata = connect(\"sheet.xlsx\")\n\n# Cloud storage\ndata = connect(\"s3://bucket/orders.parquet\")\ndata = connect(\"gs://bucket/data.csv\")\ndata = connect(\"az://container/data.parquet\")\n\n# Databases\ndata = connect(\"postgres://localhost/mydb\", table=\"orders\")\ndata = connect(\"mysql://localhost/mydb\", table=\"users\")\ndata = connect(\"sqlite:///local.db\", table=\"events\")\ndata = connect(\"snowflake://account/db\", table=\"orders\")\n\n# DataFrames (pandas, polars, pyarrow)\ndata = connect(df)\n</code></pre>"},{"location":"connectors/overview/#supported-sources","title":"Supported Sources","text":""},{"location":"connectors/overview/#files","title":"Files","text":"Format Extensions Guide CSV <code>.csv</code>, <code>.tsv</code> Files Parquet <code>.parquet</code> Files JSON <code>.json</code>, <code>.jsonl</code>, <code>.ndjson</code> Files Excel <code>.xlsx</code>, <code>.xls</code> Files"},{"location":"connectors/overview/#cloud-storage","title":"Cloud Storage","text":"Provider Prefix Guide AWS S3 <code>s3://</code> Cloud Storage Google Cloud Storage <code>gs://</code> Cloud Storage Azure Blob Storage <code>az://</code>, <code>abfss://</code> Cloud Storage"},{"location":"connectors/overview/#databases","title":"Databases","text":"Database Prefix Extra PostgreSQL <code>postgres://</code> <code>pip install duckguard[postgres]</code> MySQL <code>mysql://</code> <code>pip install duckguard[mysql]</code> SQLite <code>sqlite://</code> or <code>.db</code> Built-in Snowflake <code>snowflake://</code> <code>pip install duckguard[snowflake]</code> BigQuery <code>bigquery://</code> <code>pip install duckguard[bigquery]</code> Redshift <code>redshift://</code> <code>pip install duckguard[postgres]</code> SQL Server <code>mssql://</code> <code>pip install duckguard[sqlserver]</code> Databricks <code>databricks://</code> <code>pip install duckguard[databricks]</code> Oracle <code>oracle://</code> <code>pip install duckguard[oracle]</code> MongoDB <code>mongodb://</code> <code>pip install duckguard[mongodb]</code> Kafka <code>kafka://</code> <code>pip install duckguard[kafka]</code>"},{"location":"connectors/overview/#modern-formats","title":"Modern Formats","text":"Format Guide Delta Lake Modern Formats Apache Iceberg Modern Formats"},{"location":"connectors/overview/#dataframes","title":"DataFrames","text":"Library Detection pandas Has <code>.shape</code> and <code>.columns</code> polars Has <code>__dataframe__</code> pyarrow Has <code>to_pandas</code>"},{"location":"connectors/overview/#common-options","title":"Common Options","text":"<pre><code># Database connections\ndata = connect(\"postgres://host/db\", table=\"orders\", schema=\"public\")\n\n# All connectors return a Dataset\nprint(data.row_count)\nprint(data.columns)\nprint(data.column_count)\n</code></pre>"},{"location":"connectors/overview/#custom-connectors","title":"Custom Connectors","text":"<p>Register your own connector:</p> <pre><code>from duckguard.connectors import register_connector, Connector\n\nclass MyConnector(Connector):\n    @classmethod\n    def can_handle(cls, source) -&gt; bool:\n        return str(source).startswith(\"mydb://\")\n\n    def connect(self, config):\n        # Return a Dataset\n        ...\n\nregister_connector(MyConnector)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>No system dependencies required</li> </ul>"},{"location":"getting-started/installation/#basic-install","title":"Basic Install","text":"<pre><code>pip install duckguard\n</code></pre> <p>This installs DuckGuard with its core dependencies: DuckDB, Typer, Rich, PyArrow, Pydantic, and PyYAML.</p>"},{"location":"getting-started/installation/#optional-features","title":"Optional Features","text":"<p>Install additional features as needed:</p> Statistical TestsReportsDatabase ConnectorsAirflowEverything <p><pre><code>pip install 'duckguard[statistics]'\n</code></pre> Adds scipy for distributional checks (normality tests, KS test, chi-square).</p> <p><pre><code>pip install 'duckguard[reports]'\n</code></pre> Adds Jinja2 and WeasyPrint for HTML/PDF report generation.</p> <pre><code># Individual databases\npip install 'duckguard[postgres]'\npip install 'duckguard[snowflake]'\npip install 'duckguard[bigquery]'\npip install 'duckguard[databricks]'\npip install 'duckguard[mysql]'\npip install 'duckguard[redshift]'\npip install 'duckguard[oracle]'\npip install 'duckguard[mongodb]'\npip install 'duckguard[kafka]'\n\n# All databases at once\npip install 'duckguard[databases]'\n</code></pre> <pre><code>pip install 'duckguard[airflow]'\n</code></pre> <pre><code>pip install 'duckguard[all]'\n</code></pre>"},{"location":"getting-started/installation/#development-install","title":"Development Install","text":"<pre><code>git clone https://github.com/XDataHubAI/duckguard.git\ncd duckguard\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>duckguard --version\n</code></pre> <pre><code>import duckguard\nprint(duckguard.__version__)\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart \u2192 \u2014 Validate your first dataset</li> <li>Why DuckGuard \u2192 \u2014 See how it compares</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get from zero to validated in 30 seconds.</p>"},{"location":"getting-started/quickstart/#connect-to-your-data","title":"Connect to Your Data","text":"<pre><code>from duckguard import connect\n\n# Files\norders = connect(\"orders.csv\")           # CSV\norders = connect(\"orders.parquet\")       # Parquet\norders = connect(\"orders.json\")          # JSON\n\n# Cloud\norders = connect(\"s3://bucket/orders.parquet\")\n\n# Databases\norders = connect(\"postgres://localhost/db\", table=\"orders\")\n\n# pandas DataFrame\nimport pandas as pd\norders = connect(pd.read_csv(\"orders.csv\"))\n</code></pre> <p>DuckGuard connects to anything \u2014 files, cloud storage, databases, DataFrames. See all connectors \u2192</p>"},{"location":"getting-started/quickstart/#validate-columns","title":"Validate Columns","text":"<p>Validations work like pytest assertions \u2014 readable, composable, and they tell you exactly what failed.</p> <pre><code># Null &amp; uniqueness\nassert orders.order_id.is_not_null()\nassert orders.order_id.is_unique()\n\n# Range checks\nassert orders.total_amount.between(0, 10000)\nassert orders.quantity.greater_than(0)\n\n# Patterns &amp; enums\nassert orders.email.matches(r'^[\\w.+-]+@[\\w-]+\\.[\\w.]+$')\nassert orders.status.isin([\"pending\", \"shipped\", \"delivered\"])\n</code></pre>"},{"location":"getting-started/quickstart/#debug-failures","title":"Debug Failures","text":"<p>When a check fails, you get row-level details:</p> <pre><code>result = orders.quantity.between(1, 100)\n\nif not result.passed:\n    print(result.summary())\n    # Column 'quantity' has 3 values outside [1, 100]\n    #\n    # Sample of 3 failing rows (total: 3):\n    #   Row 5: quantity=500 - Value outside range [1, 100]\n    #   Row 23: quantity=-2 - Value outside range [1, 100]\n    #   Row 29: quantity=0 - Value outside range [1, 100]\n</code></pre>"},{"location":"getting-started/quickstart/#score-your-data","title":"Score Your Data","text":"<p>Get an instant quality grade:</p> <pre><code>score = orders.score()\n\nprint(score.grade)          # A, B, C, D, or F\nprint(score.completeness)   # % non-null\nprint(score.uniqueness)     # % unique keys\nprint(score.validity)       # % passing checks\nprint(score.consistency)    # % consistent format\n</code></pre>"},{"location":"getting-started/quickstart/#use-yaml-rules","title":"Use YAML Rules","text":"<p>Define checks declaratively:</p> <pre><code># duckguard.yaml\nname: orders_validation\n\nchecks:\n  order_id:\n    - not_null\n    - unique\n  quantity:\n    - between: [1, 1000]\n  status:\n    - allowed_values: [pending, shipped, delivered]\n</code></pre> <pre><code>from duckguard import load_rules, execute_rules\n\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, \"orders.csv\")\nprint(f\"Passed: {result.passed_count}/{result.total_checks}\")\n</code></pre> <p>Or auto-discover rules from your data:</p> <pre><code>duckguard discover orders.csv &gt; duckguard.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#run-from-cli","title":"Run from CLI","text":"<pre><code># Validate\nduckguard check orders.csv --config duckguard.yaml\n\n# Profile\nduckguard profile orders.csv\n\n# Generate report\nduckguard report orders.csv --output report.html\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Column Validation \u2014 All validation methods</li> <li>Cross-Dataset Checks \u2014 FK validation, reconciliation</li> <li>Anomaly Detection \u2014 7 detection methods</li> <li>Data Contracts \u2014 Schema enforcement</li> <li>Integrations \u2014 pytest, dbt, Airflow, CI/CD</li> </ul>"},{"location":"getting-started/why-duckguard/","title":"Why DuckGuard?","text":""},{"location":"getting-started/why-duckguard/#the-problem","title":"The Problem","text":"<p>Data quality tools are stuck in 2018. Here's what it looks like to validate a single column with the market leader:</p> <pre><code># Great Expectations \u2014 50+ lines before you validate anything\nfrom great_expectations import get_context\n\ncontext = get_context()\ndatasource = context.sources.add_pandas(\"my_ds\")\nasset = datasource.add_dataframe_asset(name=\"orders\", dataframe=df)\nbatch_request = asset.build_batch_request()\nexpectation_suite = context.add_expectation_suite(\"orders_suite\")\nvalidator = context.get_validator(\n    batch_request=batch_request,\n    expectation_suite_name=\"orders_suite\"\n)\nvalidator.expect_column_values_to_not_be_null(\"customer_id\")\n# ... and you're just getting started\n</code></pre> <p>This is insane. You need to understand contexts, datasources, assets, batch requests, expectation suites, and validators \u2014 just to check if a column has nulls.</p>"},{"location":"getting-started/why-duckguard/#the-duckguard-way","title":"The DuckGuard Way","text":"<pre><code>from duckguard import connect\n\norders = connect(\"orders.csv\")\nassert orders.customer_id.is_not_null()\n</code></pre> <p>Done. No ceremony. No boilerplate. If you can write pytest, you can write DuckGuard.</p>"},{"location":"getting-started/why-duckguard/#speed","title":"Speed","text":"<p>DuckGuard is built on DuckDB, the fastest embedded analytics engine. This means:</p> Dataset Size Great Expectations DuckGuard 1 GB CSV 45 sec / 4 GB RAM 4 sec / 200 MB RAM 10 GB Parquet 8 min / 32 GB RAM 45 sec / 2 GB RAM <p>Why? DuckDB uses columnar storage, vectorized execution, and SIMD optimizations. DuckGuard reads files directly \u2014 no loading into pandas, no DataFrame conversion, no memory explosion.</p>"},{"location":"getting-started/why-duckguard/#feature-comparison","title":"Feature Comparison","text":"What you need DuckGuard Great Expectations Soda Core Validate a column 1 line 50+ lines 10+ lines (YAML) PII detection Built-in \u2717 \u2717 Anomaly detection 7 methods \u2717 Partial Row-level errors Built-in Yes \u2717 Data contracts Built-in \u2717 Yes Conditional checks Built-in \u2717 \u2717 Query-based checks Built-in \u2717 Yes Drift detection Built-in \u2717 \u2717 Reconciliation Built-in \u2717 \u2717 Quality scoring (A-F) Built-in \u2717 \u2717 Learning curve Minutes Days Hours"},{"location":"getting-started/why-duckguard/#who-is-duckguard-for","title":"Who Is DuckGuard For?","text":"<p>Data engineers who want validation that doesn't slow down their pipelines.</p> <p>Analytics engineers who want data quality checks as readable as their SQL.</p> <p>ML engineers who need to detect data drift before it breaks their models.</p> <p>Anyone who's tired of writing 50 lines of YAML to check if a column is not null.</p>"},{"location":"getting-started/why-duckguard/#design-principles","title":"Design Principles","text":"<ol> <li>Zero boilerplate \u2014 If it takes more than 3 lines to start, it's too many</li> <li>Speed by default \u2014 DuckDB under the hood, not pandas</li> <li>Batteries included \u2014 PII, anomalies, contracts, drift \u2014 all built in</li> <li>Pytest-native \u2014 Use <code>assert</code>, not <code>.expect_column_values_to_be_blah()</code></li> <li>Progressive complexity \u2014 Simple things simple, complex things possible</li> </ol>"},{"location":"guide/ai-features/","title":"AI-Powered Data Quality (Coming in v3.2)","text":"<p>Preview</p> <p>This feature is under development. API may change.</p>"},{"location":"guide/ai-features/#overview","title":"Overview","text":"<p>DuckGuard 3.2 will add AI-powered data quality capabilities \u2014 the first data quality tool with native LLM integration.</p>"},{"location":"guide/ai-features/#planned-features","title":"Planned Features","text":""},{"location":"guide/ai-features/#duckguard-explain","title":"<code>duckguard explain</code>","text":"<p>Point DuckGuard at your data and get a plain-English quality summary:</p> <pre><code>duckguard explain orders.csv\n</code></pre> <pre><code>\ud83d\udcca Orders Data Quality Summary\n\nYour dataset has 29 rows across 14 columns. Overall quality: B+ (85/100)\n\nIssues found:\n\u2022 2 rows have missing ship_date values \u2014 these appear to be pending orders,\n  which is expected based on the status column\n\u2022 The phone column has inconsistent formatting \u2014 US numbers use +1 prefix\n  but UK numbers use +44. Consider standardizing to E.164 format\n\u2022 quantity values range from 1-5, but total_amount suggests some bulk orders\n  may be missing\n\nRecommendations:\n1. Add a not_null_when check: ship_date should be present when status != 'pending'\n2. Add a phone format validation with matches() using E.164 pattern\n3. Verify quantity * unit_price = subtotal for all rows (found 0 mismatches \u2713)\n</code></pre>"},{"location":"guide/ai-features/#duckguard-suggest","title":"<code>duckguard suggest</code>","text":"<p>Auto-generate validation rules from your data patterns:</p> <pre><code>duckguard suggest orders.csv --output duckguard.yaml\n</code></pre> <pre><code>from duckguard import connect, suggest_rules\n\norders = connect(\"orders.csv\")\nrules = suggest_rules(orders)\n\n# Returns YAML rules tailored to your actual data patterns\nprint(rules)\n</code></pre> <p>The AI analyzes column distributions, semantic types, relationships, and patterns to generate rules that match your data's actual structure \u2014 not generic boilerplate.</p>"},{"location":"guide/ai-features/#duckguard-fix","title":"<code>duckguard fix</code>","text":"<p>Get AI-suggested data cleaning steps:</p> <pre><code>duckguard fix orders.csv\n</code></pre> <pre><code>\ud83d\udd27 Suggested Fixes for orders.csv\n\n1. [MISSING DATA] ship_date is null for 2 rows\n   \u2192 These are pending orders (status='pending'). No action needed,\n     but add a conditional check: ship_date.not_null_when(\"status != 'pending'\")\n\n2. [FORMAT] phone has mixed formats (+12125551001 vs +442071234567)\n   \u2192 Standardize to E.164: all numbers should start with + followed by country code\n   \u2192 SQL: UPDATE orders SET phone = regexp_replace(phone, ...)\n\n3. [POTENTIAL ISSUE] email column contains PII\n   \u2192 29/29 rows contain email addresses\n   \u2192 Consider: masking in non-prod environments, adding to PII inventory\n</code></pre>"},{"location":"guide/ai-features/#natural-language-rules","title":"Natural Language Rules","text":"<p>Define validation rules in plain English:</p> <pre><code>from duckguard import connect\nfrom duckguard.ai import natural_rules\n\norders = connect(\"orders.csv\")\n\n# Plain English \u2192 validation rules\nresults = natural_rules(orders, [\n    \"order IDs should never be null or duplicated\",\n    \"quantities should be positive integers under 1000\",\n    \"every shipped order must have a tracking number\",\n    \"email addresses should be valid format\",\n])\n\nfor r in results:\n    print(f\"{'\u2713' if r.passed else '\u2717'} {r.message}\")\n</code></pre>"},{"location":"guide/ai-features/#architecture","title":"Architecture","text":"<pre><code>User Data \u2192 AutoProfiler \u2192 AI Analysis \u2192 Actionable Output\n                \u2193                \u2193\n         Column stats      LLM interprets\n         Patterns          patterns and\n         Semantic types    generates rules\n         Anomalies         in context\n</code></pre> <p>The AI layer is thin \u2014 it takes the rich profiling data DuckGuard already produces and uses an LLM to: 1. Interpret patterns in human terms 2. Generate contextual validation rules 3. Explain anomalies with domain awareness 4. Suggest fixes with actual SQL/code</p>"},{"location":"guide/ai-features/#supported-llms","title":"Supported LLMs","text":"<pre><code>from duckguard.ai import configure\n\n# OpenAI\nconfigure(provider=\"openai\", api_key=\"sk-...\")\n\n# Anthropic\nconfigure(provider=\"anthropic\", api_key=\"sk-ant-...\")\n\n# Local (Ollama)\nconfigure(provider=\"ollama\", model=\"llama3\")\n</code></pre>"},{"location":"guide/anomaly-detection/","title":"Anomaly Detection","text":"<p>DuckGuard includes 7 built-in anomaly detection methods, from simple statistical tests to ML-powered baselines.</p>"},{"location":"guide/anomaly-detection/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import detect_anomalies\n\nreport = detect_anomalies(orders, method=\"zscore\", columns=[\"quantity\", \"amount\"])\n\nprint(report.has_anomalies)    # True/False\nprint(report.anomaly_count)    # Number of anomalies\n\nfor a in report.anomalies:\n    print(f\"{a.column}: score={a.score:.2f}, anomaly={a.is_anomaly}\")\n</code></pre>"},{"location":"guide/anomaly-detection/#methods","title":"Methods","text":"Method Best for How it works <code>zscore</code> Normal distributions Flags values &gt;3\u03c3 from mean <code>iqr</code> Skewed data Uses interquartile range (Q1-1.5\u00b7IQR, Q3+1.5\u00b7IQR) <code>modified_zscore</code> Robust to outliers Uses median absolute deviation <code>percent_change</code> Time series Flags sudden changes between periods <code>baseline</code> Historical comparison Fit on historical data, score new values <code>ks_test</code> Distribution drift Kolmogorov-Smirnov test between distributions <code>seasonal</code> Periodic patterns Time-aware anomaly detection"},{"location":"guide/anomaly-detection/#using-anomalydetector","title":"Using AnomalyDetector","text":"<pre><code>from duckguard.anomaly import AnomalyDetector\n\n# IQR method with custom threshold\ndetector = AnomalyDetector(method=\"iqr\", threshold=1.5)\nreport = detector.detect(orders, columns=[\"quantity\"])\n</code></pre>"},{"location":"guide/anomaly-detection/#ml-baseline","title":"ML Baseline","text":"<p>Fit a baseline on historical data and score new values against it:</p> <pre><code>from duckguard.anomaly import BaselineMethod\n\nbaseline = BaselineMethod(sensitivity=2.0)\nbaseline.fit([100, 102, 98, 105, 97, 103])\n\n# Score a single value\nscore = baseline.score(250)\nprint(score.is_anomaly)  # True\nprint(score.score)       # deviation score\n\n# Score an entire column\nscores = baseline.score(orders.total_amount)\n</code></pre>"},{"location":"guide/anomaly-detection/#ks-test-distribution-drift","title":"KS Test (Distribution Drift)","text":"<pre><code>from duckguard.anomaly import KSTestMethod\n\nks = KSTestMethod(p_value_threshold=0.05)\nks.fit([1, 2, 3, 4, 5])\n\ncomparison = ks.compare_distributions([10, 11, 12, 13, 14])\nprint(comparison.is_drift)   # True\nprint(comparison.p_value)    # 0.008\nprint(comparison.message)    # \"Significant distribution drift detected\"\n</code></pre>"},{"location":"guide/anomaly-detection/#seasonal-detection","title":"Seasonal Detection","text":"<pre><code>from duckguard.anomaly import SeasonalMethod\n\nseasonal = SeasonalMethod(period=\"daily\", sensitivity=2.0)\nseasonal.fit([10, 12, 11, 13, 9, 14])\n</code></pre>"},{"location":"guide/anomaly-detection/#cli","title":"CLI","text":"<pre><code># Detect anomalies with Z-score\nduckguard anomaly orders.csv --method zscore\n\n# IQR method on specific columns\nduckguard anomaly orders.csv --method iqr --columns quantity,amount\n</code></pre>"},{"location":"guide/column-validation/","title":"Column Validation","text":"<p>DuckGuard's column validation API is designed to read like English. Every method returns a <code>ValidationResult</code> with <code>.passed</code>, <code>.message</code>, <code>.summary()</code>, and <code>.failed_rows</code>.</p>"},{"location":"guide/column-validation/#null-uniqueness","title":"Null &amp; Uniqueness","text":"<pre><code>from duckguard import connect\n\norders = connect(\"orders.csv\")\n\n# No nulls allowed\nassert orders.order_id.is_not_null()\n\n# All values must be distinct\nassert orders.order_id.is_unique()\n\n# Alias for is_unique\nassert orders.order_id.has_no_duplicates()\n</code></pre>"},{"location":"guide/column-validation/#column-properties","title":"Column Properties","text":"<p>Access statistics directly on any column:</p> <pre><code>col = orders.total_amount\n\ncol.null_count       # Number of null values\ncol.null_percent     # Percentage of nulls\ncol.unique_count     # Number of distinct values\ncol.min              # Minimum value\ncol.max              # Maximum value\ncol.mean             # Mean (numeric columns)\ncol.median           # Median (numeric columns)\ncol.stddev           # Standard deviation\n</code></pre>"},{"location":"guide/column-validation/#range-checks","title":"Range Checks","text":"<pre><code># Inclusive range\nassert orders.total_amount.between(0, 10000)\n\n# Exclusive bounds\nassert orders.total_amount.greater_than(0)\nassert orders.total_amount.less_than(100000)\n</code></pre>"},{"location":"guide/column-validation/#pattern-matching","title":"Pattern Matching","text":"<pre><code># Regex validation\nassert orders.email.matches(r'^[\\w.+-]+@[\\w-]+\\.[\\w.]+$')\n\n# String length\nassert orders.order_id.value_lengths_between(5, 10)\n</code></pre>"},{"location":"guide/column-validation/#enum-checks","title":"Enum Checks","text":"<pre><code>assert orders.status.isin([\"pending\", \"shipped\", \"delivered\"])\n</code></pre>"},{"location":"guide/column-validation/#working-with-results","title":"Working with Results","text":"<p>Every validation returns a <code>ValidationResult</code>:</p> <pre><code>result = orders.quantity.between(1, 100)\n\n# Check pass/fail\nresult.passed          # True or False\n\n# Human-readable message\nresult.message         # \"Column 'quantity' has 3 values outside [1, 100]\"\n\n# Detailed summary with row-level errors\nprint(result.summary())\n\n# Access failed rows directly\nfor row in result.failed_rows:\n    print(f\"Row {row.row_number}: {row.value} ({row.reason})\")\n\n# Get just the values or indices\nresult.get_failed_values()        # [500, -2, 0]\nresult.get_failed_row_indices()   # [5, 23, 29]\n</code></pre>"},{"location":"guide/column-validation/#using-with-pytest","title":"Using with pytest","text":"<pre><code># test_data_quality.py\nfrom duckguard import connect\n\ndef test_orders_quality():\n    orders = connect(\"data/orders.csv\")\n    assert orders.row_count &gt; 0\n    assert orders.order_id.is_not_null()\n    assert orders.order_id.is_unique()\n    assert orders.quantity.between(0, 10000)\n    assert orders.status.isin([\"pending\", \"shipped\", \"delivered\"])\n</code></pre> <p>Run with <code>pytest</code>:</p> <pre><code>pytest test_data_quality.py -v\n</code></pre>"},{"location":"guide/column-validation/#next","title":"Next","text":"<ul> <li>Quality Scoring \u2192</li> <li>Cross-Dataset Checks \u2192</li> <li>Conditional Checks \u2192</li> </ul>"},{"location":"guide/conditional-checks/","title":"Conditional Checks","text":"<p>Apply validation rules only when a SQL condition is met. Perfect for business rules that depend on context.</p>"},{"location":"guide/conditional-checks/#basic-usage","title":"Basic Usage","text":"<pre><code>from duckguard import connect\n\norders = connect(\"orders.csv\")\n\n# Email required only for shipped orders\norders.email.not_null_when(\"status = 'shipped'\")\n\n# Quantity must be 1-100 for US orders\norders.quantity.between_when(1, 100, \"country = 'US'\")\n\n# Status must be shipped or delivered for UK\norders.status.isin_when([\"shipped\", \"delivered\"], \"country = 'UK'\")\n</code></pre>"},{"location":"guide/conditional-checks/#available-methods","title":"Available Methods","text":"Method Description <code>col.not_null_when(condition)</code> Not null when condition is true <code>col.unique_when(condition)</code> Unique when condition is true <code>col.between_when(min, max, condition)</code> Range check when condition is true <code>col.isin_when(values, condition)</code> Enum check when condition is true <code>col.matches_when(pattern, condition)</code> Pattern match when condition is true"},{"location":"guide/conditional-checks/#conditions","title":"Conditions","text":"<p>Conditions are SQL WHERE clauses. Use any valid SQL expression:</p> <pre><code># Simple equality\norders.email.not_null_when(\"status = 'active'\")\n\n# Multiple conditions\norders.phone.not_null_when(\"country = 'US' AND type = 'business'\")\n\n# Numeric comparisons\norders.discount.between_when(0, 50, \"amount &gt; 1000\")\n\n# Date conditions\norders.tracking_number.not_null_when(\"ship_date IS NOT NULL\")\n</code></pre>"},{"location":"guide/conditional-checks/#thresholds","title":"Thresholds","text":"<p>By default, all rows matching the condition must pass. Use <code>threshold</code> for partial pass rates:</p> <pre><code># At least 95% of shipped orders must have tracking numbers\nresult = orders.tracking.not_null_when(\"status = 'shipped'\", threshold=0.95)\n</code></pre>"},{"location":"guide/conditional-checks/#security","title":"Security","text":"<p>Conditions go through multi-layer SQL injection prevention:</p> <ul> <li>Forbidden keyword detection (INSERT, UPDATE, DELETE, DROP, etc.)</li> <li>Injection pattern blocking (OR 1=1, UNION SELECT, comment injection)</li> <li>Complexity scoring and validation</li> <li>READ-ONLY enforcement</li> </ul> <p>You cannot modify data through conditions \u2014 only filter rows for validation.</p>"},{"location":"guide/conditional-checks/#real-world-examples","title":"Real-World Examples","text":"<pre><code># E-commerce: validate based on order type\norders.gift_message.not_null_when(\"is_gift = true\")\norders.tax_amount.between_when(0, 1000, \"country = 'US'\")\n\n# Finance: conditional compliance\ntxns.kyc_verified.not_null_when(\"amount &gt; 10000\")\ntxns.currency.isin_when(['USD', 'EUR'], \"region = 'EMEA'\")\n\n# Healthcare: conditional requirements\nrecords.insurance_id.not_null_when(\"visit_type = 'inpatient'\")\nrecords.diagnosis_code.matches_when(r'^[A-Z]\\d{2}', \"status = 'discharged'\")\n</code></pre>"},{"location":"guide/cross-dataset/","title":"Cross-Dataset Validation","text":"<p>Validate foreign key relationships, reconcile datasets, and detect distribution drift between data sources.</p>"},{"location":"guide/cross-dataset/#foreign-key-checks","title":"Foreign Key Checks","text":"<pre><code>from duckguard import connect\n\norders = connect(\"orders.csv\")\ncustomers = connect(\"customers.csv\")\n\n# All customer_ids must exist in customers table\nresult = orders.customer_id.exists_in(customers.customer_id)\n\n# FK with null handling\nresult = orders.customer_id.references(customers.customer_id, allow_nulls=True)\n</code></pre>"},{"location":"guide/cross-dataset/#finding-orphans","title":"Finding Orphans","text":"<pre><code>orphans = orders.customer_id.find_orphans(customers.customer_id)\nprint(f\"Invalid IDs: {orphans}\")\n# ['CUST-999', 'CUST-deleted']\n</code></pre>"},{"location":"guide/cross-dataset/#value-set-comparison","title":"Value Set Comparison","text":"<pre><code># Check if two columns share the same distinct values\nresult = orders.status.matches_values(lookup.code)\n</code></pre>"},{"location":"guide/cross-dataset/#row-count-comparison","title":"Row Count Comparison","text":"<pre><code># With tolerance (allows \u00b110 rows)\nresult = orders.row_count_matches(backup, tolerance=10)\n</code></pre>"},{"location":"guide/cross-dataset/#reconciliation","title":"Reconciliation","text":"<p>Full row-by-row comparison between two datasets:</p> <pre><code>source = connect(\"orders_source.parquet\")\ntarget = connect(\"orders_migrated.parquet\")\n\nrecon = source.reconcile(\n    target,\n    key_columns=[\"order_id\"],\n    compare_columns=[\"amount\", \"status\", \"customer_id\"],\n)\n\nprint(recon.match_percentage)    # 95.5\nprint(recon.missing_in_target)   # 3\nprint(recon.extra_in_target)     # 1\nprint(recon.value_mismatches)    # {'amount': 5, 'status': 2}\nprint(recon.summary())\n</code></pre> <p>Use cases:</p> <ul> <li>Validating data migrations</li> <li>Comparing source vs. warehouse</li> <li>Verifying ETL transformations</li> </ul>"},{"location":"guide/cross-dataset/#distribution-drift-detection","title":"Distribution Drift Detection","text":"<p>Detect statistical drift between a baseline and current data using the Kolmogorov-Smirnov test:</p> <pre><code>baseline = connect(\"orders_jan.parquet\")\ncurrent = connect(\"orders_feb.parquet\")\n\ndrift = current.amount.detect_drift(baseline.amount)\n\nprint(drift.is_drifted)    # True/False\nprint(drift.p_value)       # 0.0023\nprint(drift.statistic)     # KS statistic\nprint(drift.message)       # \"Significant drift detected (p=0.0023)\"\n</code></pre> <p>Use cases:</p> <ul> <li>Detecting data source changes</li> <li>ML feature drift monitoring</li> <li>Data pipeline regression testing</li> </ul>"},{"location":"guide/data-contracts/","title":"Data Contracts","text":"<p>Define schema, quality SLAs, and ownership for your data in YAML \u2014 then validate automatically.</p>"},{"location":"guide/data-contracts/#quick-start","title":"Quick Start","text":"<pre><code># orders.contract.yaml\ncontract:\n  name: orders\n  version: \"1.2.0\"\n\n  schema:\n    - name: order_id\n      type: string\n      required: true\n      unique: true\n    - name: amount\n      type: decimal\n      required: true\n      constraints:\n        - type: range\n          value: [0, 100000]\n    - name: email\n      type: string\n      semantic_type: email\n      pii: true\n\n  quality:\n    completeness: 99.5\n    freshness: \"24h\"\n    row_count_min: 1000\n\n  metadata:\n    owner: platform-team\n    description: Order transactions from checkout\n    consumers: [analytics, finance]\n</code></pre> <pre><code>from duckguard import load_contract, validate_contract\n\ncontract = load_contract(\"orders.contract.yaml\")\nresult = validate_contract(contract, \"orders.parquet\")\n\nassert result.passed\nprint(result.summary())\n</code></pre>"},{"location":"guide/data-contracts/#schema-validation","title":"Schema Validation","text":"<p>Contracts validate that your data matches the expected schema:</p> <ul> <li>Missing fields \u2014 required fields that don't exist \u2192 error; optional \u2192 warning</li> <li>Extra fields \u2014 fields in data not in contract \u2192 info (or error in strict mode)</li> <li>Required (not null) \u2014 fields marked <code>required: true</code> must have zero nulls</li> <li>Unique \u2014 fields marked <code>unique: true</code> must have 100% unique values</li> </ul>"},{"location":"guide/data-contracts/#field-types","title":"Field Types","text":"<p><code>string</code>, <code>integer</code>, <code>float</code>, <code>decimal</code>, <code>boolean</code>, <code>date</code>, <code>datetime</code>, <code>timestamp</code>, <code>time</code>, <code>uuid</code>, <code>json</code>, <code>array</code>, <code>object</code>, <code>binary</code>, <code>any</code></p>"},{"location":"guide/data-contracts/#field-constraints","title":"Field Constraints","text":"<pre><code>constraints:\n  - type: range\n    value: [0, 100000]\n  - type: min\n    value: 0\n  - type: max\n    value: 999\n  - type: pattern\n    value: \"^[A-Z]{3}-\\\\d{4}$\"\n  - type: allowed_values\n    value: [pending, shipped, delivered]\n</code></pre>"},{"location":"guide/data-contracts/#quality-slas","title":"Quality SLAs","text":"<pre><code>quality:\n  completeness: 99.5        # Min % of non-null cells\n  freshness: \"24h\"           # Max data age\n  row_count_min: 1000        # Minimum rows\n  row_count_max: 10000000    # Maximum rows\n  uniqueness:\n    order_id: 100.0          # 100% unique\n    email: 95.0              # 95%+ unique\n</code></pre>"},{"location":"guide/data-contracts/#auto-generate-contracts","title":"Auto-Generate Contracts","text":"<p>Generate a contract from your existing data:</p> <pre><code>from duckguard import generate_contract\n\ncontract = generate_contract(\"orders.parquet\", name=\"orders\", owner=\"data-team\")\n\n# Save to YAML\ngenerate_contract(\"orders.parquet\", output=\"orders.contract.yaml\")\n</code></pre> <p>The generator infers types, constraints, PII flags, and quality SLAs from the data.</p>"},{"location":"guide/data-contracts/#compare-contract-versions","title":"Compare Contract Versions","text":"<p>Detect breaking changes between contract versions:</p> <pre><code>from duckguard import load_contract, diff_contracts\n\nold = load_contract(\"orders.v1.yaml\")\nnew = load_contract(\"orders.v2.yaml\")\n\ndiff = diff_contracts(old, new)\n\nprint(diff.has_breaking_changes)  # True/False\nprint(diff.suggest_version_bump())  # 'major', 'minor', 'patch'\nprint(diff.summary())\n\nfor change in diff.breaking_changes:\n    print(f\"\u274c {change.message}\")\n</code></pre> <p>Breaking changes: field removed, type changed, new required field, constraint added.</p>"},{"location":"guide/data-contracts/#strict-mode","title":"Strict Mode","text":"<p>In strict mode, extra fields in data (not defined in contract) are errors:</p> <pre><code>result = validate_contract(contract, \"orders.parquet\", strict_mode=True)\n</code></pre>"},{"location":"guide/data-contracts/#cli","title":"CLI","text":"<pre><code># Generate contract\nduckguard contract generate data.csv --output orders.contract.yaml\n\n# Validate data against contract\nduckguard contract validate data.csv --contract orders.contract.yaml\n\n# Diff two versions\nduckguard contract diff old.contract.yaml new.contract.yaml\n</code></pre>"},{"location":"guide/data-contracts/#validation-result","title":"Validation Result","text":"<pre><code>result = validate_contract(contract, source)\n\nresult.passed          # True if no errors\nresult.schema_valid    # Schema checks only\nresult.quality_valid   # Quality SLA checks only\nresult.error_count     # Number of errors\nresult.warning_count   # Number of warnings\nresult.errors          # List of error messages\nresult.warnings        # List of warning messages\nresult.violations      # Full list of ContractViolation objects\n</code></pre>"},{"location":"guide/distribution-drift/","title":"Distribution &amp; Drift Detection","text":"<p>Test whether columns follow expected statistical distributions using KS tests and chi-square tests.</p>"},{"location":"guide/distribution-drift/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\ndata = connect(\"measurements.csv\")\n\n# Test for normal distribution\nresult = data.temperature.expect_distribution_normal()\nassert result.passed\n\n# Test for uniform distribution\nresult = data.random_value.expect_distribution_uniform()\nassert result.passed\n</code></pre> <p>Note</p> <p>Distributional checks require <code>scipy&gt;=1.11.0</code>. Install with: <code>pip install 'duckguard[statistics]'</code></p>"},{"location":"guide/distribution-drift/#normal-distribution-test","title":"Normal Distribution Test","text":"<p>Uses the Kolmogorov-Smirnov test against a fitted normal distribution:</p> <pre><code>result = data.temperature.expect_distribution_normal(\n    significance_level=0.05  # default\n)\n\nprint(result.details[\"pvalue\"])     # p-value from KS test\nprint(result.details[\"statistic\"])  # KS statistic\nprint(result.details[\"mean\"])       # Sample mean\nprint(result.details[\"std\"])        # Sample std deviation\n</code></pre> <p>The test passes when <code>p-value &gt; significance_level</code> (null hypothesis: data is normal).</p>"},{"location":"guide/distribution-drift/#uniform-distribution-test","title":"Uniform Distribution Test","text":"<pre><code>result = data.random_value.expect_distribution_uniform(\n    significance_level=0.05\n)\n</code></pre> <p>Values are scaled to <code>[0, 1]</code> before testing against the standard uniform distribution.</p>"},{"location":"guide/distribution-drift/#ks-test-any-distribution","title":"KS Test (Any Distribution)","text":"<p>Test against any scipy distribution:</p> <pre><code># Exponential distribution\nresult = data.wait_times.expect_ks_test(distribution=\"expon\")\n\n# Normal (same as expect_distribution_normal)\nresult = data.values.expect_ks_test(distribution=\"norm\")\n</code></pre>"},{"location":"guide/distribution-drift/#chi-square-test-categorical","title":"Chi-Square Test (Categorical)","text":"<p>Test if observed frequencies match expected frequencies:</p> <pre><code># Test if dice is fair\nresult = data.roll.expect_chi_square_test(\n    expected_frequencies={1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}\n)\n\n# Test against uniform (no expected_frequencies = assumes uniform)\nresult = data.category.expect_chi_square_test()\n</code></pre> <p>Result details include observed vs expected frequency breakdown:</p> <pre><code>print(result.details[\"observed\"])   # {'A': 50, 'B': 48, 'C': 52}\nprint(result.details[\"expected\"])   # {'A': 50.0, 'B': 50.0, 'C': 50.0}\nprint(result.details[\"degrees_of_freedom\"])\n</code></pre>"},{"location":"guide/distribution-drift/#minimum-sample-size","title":"Minimum Sample Size","text":"<p>All distributional tests require at least 30 samples. The check fails with a clear message if fewer are available.</p>"},{"location":"guide/distribution-drift/#yaml-rules","title":"YAML Rules","text":"<pre><code>checks:\n  temperature:\n    - distribution_normal:\n        significance_level: 0.05\n\n  random_value:\n    - distribution_uniform\n\n  category:\n    - distribution_chi_square:\n        expected_frequencies:\n          A: 0.33\n          B: 0.33\n          C: 0.34\n</code></pre>"},{"location":"guide/distribution-drift/#interpreting-results","title":"Interpreting Results","text":"p-value Interpretation &gt; 0.05 No evidence against the hypothesized distribution (pass) 0.01\u20130.05 Weak evidence against \u2014 borderline &lt; 0.01 Strong evidence data does NOT follow the distribution (fail) <p>Lower <code>significance_level</code> = stricter test (fewer false positives, more false negatives).</p>"},{"location":"guide/freshness-schema/","title":"Freshness &amp; Schema Tracking","text":"<p>Monitor data staleness and track schema evolution over time.</p>"},{"location":"guide/freshness-schema/#freshness-quick-start","title":"Freshness Quick Start","text":"<pre><code>from duckguard import connect\nfrom datetime import timedelta\n\ndata = connect(\"orders.csv\")\n\n# Simple freshness check\nprint(data.freshness.age_human)  # \"2 hours ago\"\nprint(data.freshness.is_fresh)   # True\n\n# Custom threshold\nif not data.is_fresh(timedelta(hours=6)):\n    print(\"Data is stale!\")\n</code></pre>"},{"location":"guide/freshness-schema/#freshness-methods","title":"Freshness Methods","text":"<p>DuckGuard checks freshness via file modification time or timestamp columns:</p>"},{"location":"guide/freshness-schema/#file-modification-time","title":"File Modification Time","text":"<pre><code>from duckguard.freshness import FreshnessMonitor\n\nmonitor = FreshnessMonitor(threshold=timedelta(hours=6))\nresult = monitor.check_file_mtime(\"data.csv\")\n\nprint(result.last_modified)      # datetime\nprint(result.age_human)          # \"3 hours ago\"\nprint(result.is_fresh)           # True/False\nprint(result.method)             # FreshnessMethod.FILE_MTIME\n</code></pre>"},{"location":"guide/freshness-schema/#column-timestamp","title":"Column Timestamp","text":"<pre><code>result = monitor.check_column_timestamp(data, \"updated_at\")\n# Uses MAX(updated_at) to determine freshness\n\n# Use MIN instead (oldest record)\nresult = monitor.check_column_timestamp(data, \"created_at\", use_max=False)\n</code></pre>"},{"location":"guide/freshness-schema/#auto-detection","title":"Auto-Detection","text":"<p><code>monitor.check()</code> picks the best method automatically:</p> <ol> <li>Local file \u2192 uses file mtime</li> <li>Dataset with timestamp column \u2192 auto-detects columns like <code>updated_at</code>, <code>created_at</code>, <code>timestamp</code></li> </ol>"},{"location":"guide/freshness-schema/#freshnessresult","title":"FreshnessResult","text":"<pre><code>result = monitor.check(data)\n\nresult.source             # Data source path\nresult.last_modified      # datetime or None\nresult.age_seconds        # Float or None\nresult.age_human          # \"2 hours ago\"\nresult.is_fresh           # True/False\nresult.threshold_seconds  # Configured threshold\nresult.method             # FILE_MTIME, COLUMN_MAX, etc.\nresult.to_dict()          # JSON-serializable dict\n</code></pre>"},{"location":"guide/freshness-schema/#cli","title":"CLI","text":"<pre><code>duckguard freshness data.csv\nduckguard freshness data.csv --max-age 6h\nduckguard freshness data.csv --column updated_at\nduckguard freshness data.csv --format json\n</code></pre>"},{"location":"guide/freshness-schema/#schema-tracking","title":"Schema Tracking","text":"<p>Capture schema snapshots over time and detect changes.</p>"},{"location":"guide/freshness-schema/#capture-a-snapshot","title":"Capture a Snapshot","text":"<pre><code>from duckguard import connect\nfrom duckguard.schema_history import SchemaTracker\n\ntracker = SchemaTracker()\ndata = connect(\"data.csv\")\n\nsnapshot = tracker.capture(data)\nprint(f\"Captured {snapshot.column_count} columns, {snapshot.row_count} rows\")\n</code></pre>"},{"location":"guide/freshness-schema/#view-history","title":"View History","text":"<pre><code>history = tracker.get_history(data.source, limit=10)\nfor snap in history:\n    print(f\"{snap.captured_at}: {snap.column_count} columns\")\n</code></pre>"},{"location":"guide/freshness-schema/#get-latest-snapshot","title":"Get Latest Snapshot","text":"<pre><code>latest = tracker.get_latest(data.source)\nif latest:\n    for col in latest.columns:\n        print(f\"{col.name}: {col.dtype} (nullable: {col.nullable})\")\n</code></pre>"},{"location":"guide/freshness-schema/#detect-changes","title":"Detect Changes","text":"<pre><code>from duckguard.schema_history import SchemaChangeAnalyzer\n\nanalyzer = SchemaChangeAnalyzer()\n# Compare current schema against the last snapshot\n# (uses tracker internally)\n</code></pre>"},{"location":"guide/freshness-schema/#cli_1","title":"CLI","text":"<pre><code>duckguard schema data.csv                    # Show current schema\nduckguard schema data.csv --action capture   # Capture snapshot\nduckguard schema data.csv --action history   # View history\nduckguard schema data.csv --action changes   # Detect changes\n</code></pre>"},{"location":"guide/freshness-schema/#schema-snapshot-structure","title":"Schema Snapshot Structure","text":"<p>Each snapshot captures:</p> <ul> <li>Column name and data type</li> <li>Nullable flag</li> <li>Column position (ordering)</li> <li>Row count at capture time</li> <li>Timestamp of capture</li> </ul> <p>Snapshots are stored in a local SQLite database and compared for drift detection.</p>"},{"location":"guide/group-by/","title":"Group-By Validation","text":"<p>Run validation checks on each group separately \u2014 per-region, per-date, per-status.</p>"},{"location":"guide/group-by/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\ndata = connect(\"orders.csv\")\n\n# Every region must have &gt;100 rows\nresult = data.group_by(\"region\").row_count_greater_than(100)\nassert result.passed\n\n# Check null percent per group\nresult = data.group_by(\"region\").validate(\n    lambda col: col.null_percent &lt; 5,\n    column=\"customer_id\"\n)\nassert result.passed\n</code></pre>"},{"location":"guide/group-by/#creating-groups","title":"Creating Groups","text":"<p>Use <code>group_by()</code> with a single column or a list:</p> <pre><code># Single column\ngrouped = data.group_by(\"status\")\n\n# Multiple columns\ngrouped = data.group_by([\"date\", \"region\"])\n</code></pre>"},{"location":"guide/group-by/#group-properties","title":"Group Properties","text":"<pre><code>grouped = data.group_by(\"status\")\n\n# List all group keys\nprint(grouped.groups)\n# [{'status': 'active'}, {'status': 'pending'}, ...]\n\n# Count of distinct groups\nprint(grouped.group_count)  # 3\n</code></pre>"},{"location":"guide/group-by/#group-statistics","title":"Group Statistics","text":"<p>Get row counts per group:</p> <pre><code>stats = data.group_by(\"status\").stats()\nfor g in stats:\n    print(f\"{g['status']}: {g['row_count']} rows\")\n# active: 5000 rows\n# pending: 1200 rows\n# cancelled: 300 rows\n</code></pre>"},{"location":"guide/group-by/#row-count-validation","title":"Row Count Validation","text":"<p>Ensure every group meets a minimum row count:</p> <pre><code>result = data.group_by(\"region\").row_count_greater_than(100)\n\nif not result.passed:\n    for g in result.get_failed_groups():\n        print(f\"Region {g.group_key} has only {g.row_count} rows\")\n</code></pre>"},{"location":"guide/group-by/#custom-validation","title":"Custom Validation","text":"<p>Use <code>validate()</code> with a lambda for column-level checks per group:</p> <pre><code># Null percentage check per group\nresult = data.group_by(\"region\").validate(\n    lambda col: col.null_percent &lt; 5,\n    column=\"customer_id\"\n)\n\n# Range check per group\nresult = data.group_by(\"date\").validate(\n    lambda col: col.between(0, 10000),\n    column=\"amount\"\n)\n</code></pre>"},{"location":"guide/group-by/#groupbyresult","title":"GroupByResult","text":"<p>The result object gives you full visibility:</p> <pre><code>result = data.group_by(\"region\").row_count_greater_than(50)\n\nprint(result.passed)          # True/False \u2014 all groups pass?\nprint(result.total_groups)    # Total number of groups\nprint(result.passed_groups)   # Groups that passed\nprint(result.failed_groups)   # Groups that failed\nprint(result.pass_rate)       # Percentage passed\n\n# Inspect failures\nfor g in result.get_failed_groups():\n    print(f\"[{g.key_string}]: {g.row_count} rows\")\n    for cr in g.check_results:\n        if not cr.passed:\n            print(f\"  - {cr.message}\")\n\n# Full summary\nprint(result.summary())\n</code></pre>"},{"location":"guide/group-by/#practical-patterns","title":"Practical Patterns","text":""},{"location":"guide/group-by/#per-partition-quality-gate","title":"Per-partition quality gate","text":"<pre><code># Every date partition must have data\nresult = data.group_by(\"date\").row_count_greater_than(0)\nassert result, f\"Empty partitions found: {result.summary()}\"\n</code></pre>"},{"location":"guide/group-by/#segment-level-completeness","title":"Segment-level completeness","text":"<pre><code># Amount must be non-null in each region\nresult = data.group_by(\"region\").validate(\n    lambda col: col.null_percent == 0,\n    column=\"amount\"\n)\n</code></pre>"},{"location":"guide/group-by/#multi-level-grouping","title":"Multi-level grouping","text":"<pre><code>result = data.group_by([\"country\", \"city\"]).row_count_greater_than(10)\nprint(f\"{result.pass_rate:.0f}% of country/city combos have &gt;10 rows\")\n</code></pre>"},{"location":"guide/multi-column-checks/","title":"Multi-Column Checks","text":"<p>Validate relationships between columns \u2014 date ranges, composite keys, cross-column arithmetic.</p>"},{"location":"guide/multi-column-checks/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\ndata = connect(\"orders.csv\")\n\n# Date range: end must be after start\nresult = data.expect_column_pair_satisfy(\n    column_a=\"end_date\",\n    column_b=\"start_date\",\n    expression=\"end_date &gt;= start_date\"\n)\nassert result.passed\n\n# Composite primary key\nresult = data.expect_columns_unique(columns=[\"user_id\", \"session_id\"])\nassert result.passed\n\n# Components must sum to 100%\nresult = data.expect_multicolumn_sum_to_equal(\n    columns=[\"q1_pct\", \"q2_pct\", \"q3_pct\", \"q4_pct\"],\n    expected_sum=100.0\n)\nassert result.passed\n</code></pre>"},{"location":"guide/multi-column-checks/#column-pair-expressions","title":"Column Pair Expressions","text":"<p><code>expect_column_pair_satisfy()</code> evaluates a SQL expression across two columns.</p> <pre><code># Arithmetic constraint\nresult = data.expect_column_pair_satisfy(\n    column_a=\"total\",\n    column_b=\"subtotal\",\n    expression=\"total = subtotal * 1.1\"  # 10% markup\n)\n\n# With threshold \u2014 allow up to 5% violations\nresult = data.expect_column_pair_satisfy(\n    column_a=\"end_date\",\n    column_b=\"start_date\",\n    expression=\"end_date &gt;= start_date\",\n    threshold=0.95\n)\n</code></pre> <p>Supported operators: <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>=</code>, <code>!=</code>, <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>AND</code>, <code>OR</code></p> <p>The expression parser validates syntax, checks for SQL injection, and scores complexity (max 50 by default).</p>"},{"location":"guide/multi-column-checks/#composite-uniqueness","title":"Composite Uniqueness","text":"<p>Validate that a combination of columns forms a unique key:</p> <pre><code># Two-column composite key\nresult = data.expect_columns_unique(\n    columns=[\"user_id\", \"session_id\"]\n)\n\n# Three-column composite key with threshold\nresult = data.expect_columns_unique(\n    columns=[\"year\", \"month\", \"day\"],\n    threshold=0.99  # Allow 1% duplicates\n)\n\n# Inspect results\nprint(result.details[\"duplicate_combinations\"])\nprint(result.details[\"uniqueness_rate\"])\n</code></pre> <p>Note</p> <p>At least 2 columns are required. For single-column uniqueness, use <code>column.has_no_duplicates()</code>.</p>"},{"location":"guide/multi-column-checks/#multi-column-sum","title":"Multi-Column Sum","text":"<p>Check that columns sum to an expected value per row:</p> <pre><code># Budget allocation check\nresult = data.expect_multicolumn_sum_to_equal(\n    columns=[\"marketing\", \"sales\", \"r_and_d\"],\n    expected_sum=1000000,\n    threshold=0.01  # Allow $0.01 rounding error\n)\n</code></pre>"},{"location":"guide/multi-column-checks/#yaml-rules","title":"YAML Rules","text":"<p>All multi-column checks work in YAML rule files:</p> <pre><code>checks:\n  _multicolumn:\n    - column_pair_satisfy:\n        column_a: end_date\n        column_b: start_date\n        expression: \"end_date &gt;= start_date\"\n\n    - multicolumn_unique:\n        columns: [user_id, session_id]\n\n    - multicolumn_sum:\n        columns: [q1, q2, q3, q4]\n        expected_sum: 100.0\n</code></pre>"},{"location":"guide/multi-column-checks/#result-details","title":"Result Details","text":"<p>Every result includes detailed metadata:</p> <pre><code>result = data.expect_column_pair_satisfy(\n    column_a=\"end_date\",\n    column_b=\"start_date\",\n    expression=\"end_date &gt;= start_date\"\n)\n\nprint(result.details[\"violations\"])       # Count of failing rows\nprint(result.details[\"total_rows\"])       # Total rows checked\nprint(result.details[\"violation_rate\"])   # Fraction that failed\n</code></pre>"},{"location":"guide/pii-detection/","title":"PII Detection","text":"<p>Automatically detect personally identifiable information (PII) in your data using column names and value patterns.</p>"},{"location":"guide/pii-detection/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\nfrom duckguard.semantic import SemanticAnalyzer\n\ndata = connect(\"customers.csv\")\nanalyzer = SemanticAnalyzer()\n\n# Find all PII columns\npii = analyzer.find_pii_columns(data)\nfor col_name, sem_type, warning in pii:\n    print(f\"\u26a0\ufe0f {col_name}: {sem_type.value} \u2014 {warning}\")\n</code></pre>"},{"location":"guide/pii-detection/#semantic-type-detection","title":"Semantic Type Detection","text":"<p>DuckGuard detects 40+ semantic types from column names and value patterns:</p> <pre><code>from duckguard import detect_type, detect_types_for_dataset\n\n# Single column\nsem_type = detect_type(data, \"email\")\nprint(sem_type)  # SemanticType.EMAIL\n\n# All columns at once\ntypes = detect_types_for_dataset(data)\nfor col, stype in types.items():\n    print(f\"{col}: {stype.value}\")\n</code></pre>"},{"location":"guide/pii-detection/#full-dataset-analysis","title":"Full Dataset Analysis","text":"<pre><code>analysis = analyzer.analyze(data)\n\nprint(f\"PII columns: {analysis.pii_columns}\")\nprint(f\"Has PII: {analysis.has_pii}\")\n\nfor col in analysis.columns:\n    print(f\"{col.name}: {col.semantic_type.value} \"\n          f\"(confidence: {col.confidence:.0%})\")\n    if col.is_pii:\n        print(f\"  \u26a0\ufe0f {col.pii_warning}\")\n    if col.suggested_validations:\n        print(f\"  Suggested: {col.suggested_validations}\")\n</code></pre>"},{"location":"guide/pii-detection/#pii-types-detected","title":"PII Types Detected","text":"Type Detection Example Email Name + regex pattern <code>user@example.com</code> Phone Name + digit pattern <code>+1-555-0123</code> SSN Name + <code>\\d{3}-\\d{2}-\\d{4}</code> <code>123-45-6789</code> Credit Card Name + 16-digit pattern <code>4111-1111-1111-1111</code> Person Name Column name matching <code>first_name</code>, <code>surname</code> Address Column name matching <code>street_address</code>"},{"location":"guide/pii-detection/#detection-methods","title":"Detection Methods","text":"<p>Detection uses a two-pass approach:</p> <ol> <li>Column name patterns \u2014 matches against 40+ name patterns (e.g., <code>email</code>, <code>phone_number</code>, <code>ssn</code>)</li> <li>Value patterns \u2014 regex matching on sampled values (e.g., email format, UUID format)</li> </ol> <p>Confidence scores combine both signals (0.0\u20131.0).</p>"},{"location":"guide/pii-detection/#generate-validation-yaml","title":"Generate Validation YAML","text":"<pre><code>analysis = analyzer.analyze(data)\nyaml_rules = analysis.get_validations_yaml()\nprint(yaml_rules)\n</code></pre> <p>Output:</p> <pre><code>checks:\n  email:\n    - pattern: email\n    - unique\n  phone:\n    - pattern: phone\n  order_id:\n    - not_null\n    - unique\n</code></pre>"},{"location":"guide/pii-detection/#quick-scan","title":"Quick Scan","text":"<p>For a fast type-only scan (no statistics):</p> <pre><code>types = analyzer.quick_scan(data)\n# {'order_id': SemanticType.PRIMARY_KEY, 'email': SemanticType.EMAIL, ...}\n</code></pre>"},{"location":"guide/pii-detection/#common-semantic-types","title":"Common Semantic Types","text":"<p>Identity: <code>primary_key</code>, <code>foreign_key</code>, <code>uuid</code>, <code>id</code> Contact: <code>email</code>, <code>phone</code>, <code>url</code>, <code>ip_address</code> PII: <code>ssn</code>, <code>credit_card</code>, <code>person_name</code>, <code>address</code> Location: <code>country</code>, <code>state</code>, <code>city</code>, <code>zipcode</code>, <code>latitude</code>, <code>longitude</code> Time: <code>date</code>, <code>datetime</code>, <code>timestamp</code>, <code>year</code> Numeric: <code>currency</code>, <code>percentage</code>, <code>quantity</code>, <code>age</code> Categorical: <code>boolean</code>, <code>enum</code>, <code>status</code>, <code>category</code></p>"},{"location":"guide/pii-detection/#cli-integration","title":"CLI Integration","text":"<p>The <code>discover</code> and <code>info</code> commands include semantic analysis:</p> <pre><code>duckguard discover data.csv    # Shows semantic types + PII warnings\nduckguard info data.csv        # Shows column types and PII flags\n</code></pre>"},{"location":"guide/profiling/","title":"Profiling","text":"<p>Auto-profile datasets to discover statistics, patterns, and quality scores \u2014 then generate validation rules automatically.</p>"},{"location":"guide/profiling/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect, profile\n\ndata = connect(\"orders.csv\")\nresult = profile(data)\n\nprint(f\"Rows: {result.row_count}, Columns: {result.column_count}\")\nprint(f\"Quality: {result.overall_quality_score:.0f}/100 ({result.overall_quality_grade})\")\n\n# Print suggested rules\nfor rule in result.suggested_rules:\n    print(rule)\n</code></pre> <pre><code># CLI\nduckguard profile data.csv\nduckguard profile data.csv --deep\nduckguard profile data.csv --format json -o profile.json\n</code></pre>"},{"location":"guide/profiling/#column-profiles","title":"Column Profiles","text":"<p>Each column gets a full statistical profile:</p> <pre><code>for col in result.columns:\n    print(f\"{col.name}: {col.dtype}\")\n    print(f\"  Nulls: {col.null_percent:.1f}%\")\n    print(f\"  Unique: {col.unique_percent:.1f}%\")\n    print(f\"  Min: {col.min_value}, Max: {col.max_value}\")\n    print(f\"  Mean: {col.mean_value}, Median: {col.median_value}\")\n    print(f\"  Stddev: {col.stddev_value}\")\n    print(f\"  P25: {col.p25_value}, P75: {col.p75_value}\")\n    print(f\"  Quality: {col.quality_score:.0f}/100 ({col.quality_grade})\")\n    print(f\"  Rules: {len(col.suggested_rules)}\")\n</code></pre>"},{"location":"guide/profiling/#deep-profiling","title":"Deep Profiling","text":"<p>Enable distribution analysis and outlier detection with <code>deep=True</code>:</p> <pre><code>result = profile(data, deep=True)\n\nfor col in result.columns:\n    if col.distribution_type:\n        print(f\"{col.name}: {col.distribution_type}\")\n        print(f\"  Skewness: {col.skewness:.2f}\")\n        print(f\"  Kurtosis: {col.kurtosis:.2f}\")\n        print(f\"  Normal: {col.is_normal}\")\n        print(f\"  Outliers: {col.outlier_count} ({col.outlier_percentage:.1f}%)\")\n</code></pre> <p>Deep profiling uses:</p> <ul> <li>DistributionAnalyzer \u2014 fits distributions, tests normality (requires scipy)</li> <li>OutlierDetector \u2014 IQR-based outlier detection (works without scipy)</li> </ul>"},{"location":"guide/profiling/#rule-suggestions","title":"Rule Suggestions","text":"<p>The profiler generates Python assertions automatically:</p> <pre><code># Suggested rules look like:\n# assert data.order_id.null_percent == 0\n# assert data.order_id.has_no_duplicates()\n# assert data.amount.between(0, 10000)\n# assert data.status.isin(['pending', 'shipped', 'delivered'])\n</code></pre> <p>Rules are generated based on:</p> Pattern Rule 0% nulls <code>null_percent == 0</code> &lt;1% nulls <code>null_percent &lt; N</code> (with buffer) 100% unique <code>has_no_duplicates()</code> Numeric range <code>between(min, max)</code> (with 10% buffer) Non-negative <code>min &gt;= 0</code> Low cardinality (\u226420 values) <code>isin([...])</code> Pattern match (email, UUID, etc.) <code>matches(r\"...\")</code>"},{"location":"guide/profiling/#generate-test-files","title":"Generate Test Files","text":"<pre><code>from duckguard.profiler import AutoProfiler\n\nprofiler = AutoProfiler()\ntest_code = profiler.generate_test_file(data, output_var=\"orders\")\nprint(test_code)\n</code></pre> <p>Output:</p> <pre><code>\"\"\"Auto-generated data quality tests by DuckGuard.\"\"\"\n\nfrom duckguard import connect\n\ndef test_orders():\n    orders = connect(\"orders.csv\")\n\n    # Basic dataset checks\n    assert orders.row_count &gt; 0\n\n    # order_id validations\n    assert orders.order_id.null_percent == 0\n    assert orders.order_id.has_no_duplicates()\n\n    # amount validations\n    assert orders.total_amount.between(0, 11000)\n    assert orders.total_amount.min &gt;= 0\n</code></pre>"},{"location":"guide/profiling/#configurable-thresholds","title":"Configurable Thresholds","text":"<pre><code>from duckguard.profiler import AutoProfiler\n\nprofiler = AutoProfiler(\n    null_threshold=1.0,           # Suggest not_null below 1% nulls\n    unique_threshold=99.0,        # Suggest unique above 99%\n    enum_max_values=20,           # Max distinct values for enum suggestion\n    pattern_sample_size=1000,     # Samples for pattern detection\n    pattern_min_confidence=90.0,  # Min confidence for pattern match\n)\nresult = profiler.profile(data)\n</code></pre>"},{"location":"guide/quality-scoring/","title":"Quality Scoring","text":"<p>DuckGuard scores data quality across four dimensions and assigns an A-F letter grade.</p>"},{"location":"guide/quality-scoring/#quick-score","title":"Quick Score","text":"<pre><code>from duckguard import connect\n\norders = connect(\"orders.csv\")\nscore = orders.score()\n\nprint(score.grade)          # A, B, C, D, or F\nprint(score.overall)        # 0-100 composite score\n</code></pre>"},{"location":"guide/quality-scoring/#quality-dimensions","title":"Quality Dimensions","text":"Dimension What it measures Completeness % of non-null values across columns Uniqueness % of unique values in key columns Validity % of values passing type and range checks Consistency % of values with consistent formatting <pre><code>score = orders.score()\n\nprint(score.completeness)   # 98.5\nprint(score.uniqueness)     # 100.0\nprint(score.validity)       # 95.2\nprint(score.consistency)    # 97.8\n</code></pre>"},{"location":"guide/quality-scoring/#grade-scale","title":"Grade Scale","text":"Grade Score Range Meaning A 90-100 Excellent \u2014 production ready B 80-89 Good \u2014 minor issues C 70-79 Fair \u2014 needs attention D 60-69 Poor \u2014 significant issues F &lt;60 Failing \u2014 not usable"},{"location":"guide/quality-scoring/#profiling-for-deeper-insight","title":"Profiling for Deeper Insight","text":"<pre><code>from duckguard import AutoProfiler\n\nprofiler = AutoProfiler()\nprofile = profiler.profile(orders)\n\nprint(f\"Columns: {profile.column_count}\")\nprint(f\"Rows: {profile.row_count}\")\nprint(f\"Quality: {profile.overall_quality_grade} ({profile.overall_quality_score:.1f}/100)\")\n\n# Per-column breakdown\nfor col in profile.columns:\n    print(f\"  {col.name}: grade={col.quality_grade}, nulls={col.null_percent:.1f}%\")\n</code></pre>"},{"location":"guide/quality-scoring/#deep-profiling","title":"Deep Profiling","text":"<p>Enable distribution analysis and outlier detection for numeric columns:</p> <pre><code>deep_profiler = AutoProfiler(deep=True)\nprofile = deep_profiler.profile(orders)\n\nfor col in profile.columns:\n    if col.distribution_type:\n        print(f\"  {col.name}: {col.distribution_type}, skew={col.skewness:.2f}\")\n    if col.outlier_count is not None:\n        print(f\"    outliers: {col.outlier_count} ({col.outlier_percentage:.1f}%)\")\n</code></pre>"},{"location":"guide/quality-scoring/#custom-thresholds","title":"Custom Thresholds","text":"<pre><code>strict = AutoProfiler(\n    null_threshold=0.0,            # Any nulls trigger not_null suggestion\n    unique_threshold=100.0,        # Must be 100% unique\n    pattern_min_confidence=95.0,   # Higher confidence for patterns\n)\nprofile = strict.profile(orders)\n</code></pre>"},{"location":"guide/quality-scoring/#auto-generated-rules","title":"Auto-Generated Rules","text":"<p>DuckGuard can suggest validation rules based on your data:</p> <pre><code>from duckguard import generate_rules\n\nyaml_rules = generate_rules(orders, dataset_name=\"orders\")\nprint(yaml_rules)  # Ready-to-use YAML\n</code></pre>"},{"location":"guide/quality-scoring/#cli","title":"CLI","text":"<pre><code># Quick profile\nduckguard profile orders.csv\n\n# Deep profile with JSON output\nduckguard profile orders.csv --deep --format json\n\n# Save to file\nduckguard profile orders.csv -o profile.json\n</code></pre>"},{"location":"guide/query-checks/","title":"Query-Based Checks","text":"<p>Write custom SQL to validate complex business logic that can't be expressed with standard checks.</p>"},{"location":"guide/query-checks/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\ndata = connect(\"orders.csv\")\n\n# Find violations \u2014 query should return no rows\nresult = data.expect_query_to_return_no_rows(\n    query=\"SELECT * FROM table WHERE total &lt; subtotal\"\n)\nassert result.passed\n\n# Ensure expected data exists\nresult = data.expect_query_to_return_rows(\n    query=\"SELECT * FROM table WHERE status = 'active'\"\n)\nassert result.passed\n</code></pre>"},{"location":"guide/query-checks/#available-methods","title":"Available Methods","text":""},{"location":"guide/query-checks/#no-rows-find-violations","title":"No Rows (Find Violations)","text":"<p>Write a query that finds bad rows. Passes if zero rows returned:</p> <pre><code># No future dates\nresult = data.expect_query_to_return_no_rows(\n    query=\"SELECT * FROM table WHERE order_date &gt; CURRENT_DATE\"\n)\n\n# No orphaned records\nresult = data.expect_query_to_return_no_rows(\n    query=\"SELECT * FROM table WHERE status = 'shipped' AND tracking_number IS NULL\"\n)\n</code></pre>"},{"location":"guide/query-checks/#returns-rows-data-exists","title":"Returns Rows (Data Exists)","text":"<p>Ensure a query returns at least one row:</p> <pre><code>result = data.expect_query_to_return_rows(\n    query=\"SELECT * FROM table WHERE created_at &gt;= CURRENT_DATE - 7\"\n)\n</code></pre>"},{"location":"guide/query-checks/#result-equals","title":"Result Equals","text":"<p>Check a scalar query result against an expected value:</p> <pre><code># Exact match\nresult = data.expect_query_result_to_equal(\n    query=\"SELECT COUNT(*) FROM table WHERE status = 'pending'\",\n    expected=0\n)\n\n# With numeric tolerance\nresult = data.expect_query_result_to_equal(\n    query=\"SELECT AVG(price) FROM table\",\n    expected=100.0,\n    tolerance=5.0\n)\n</code></pre>"},{"location":"guide/query-checks/#result-between","title":"Result Between","text":"<p>Validate a query result falls within a range:</p> <pre><code>result = data.expect_query_result_to_be_between(\n    query=\"SELECT AVG(price) FROM table\",\n    min_value=10.0,\n    max_value=1000.0\n)\n\n# Null rate validation\nresult = data.expect_query_result_to_be_between(\n    query=\"\"\"\n        SELECT (COUNT(*) FILTER (WHERE price IS NULL)) * 100.0 / COUNT(*)\n        FROM table\n    \"\"\",\n    min_value=0.0,\n    max_value=5.0  # Max 5% nulls\n)\n</code></pre>"},{"location":"guide/query-checks/#table-reference","title":"Table Reference","text":"<p>Use <code>table</code> in your queries to reference the dataset \u2014 DuckGuard replaces it with the actual source:</p> <pre><code># \u2705 Correct\nquery = \"SELECT * FROM table WHERE amount &lt; 0\"\n\n# \u274c Don't hardcode file paths\nquery = \"SELECT * FROM 'data/orders.csv' WHERE amount &lt; 0\"\n</code></pre>"},{"location":"guide/query-checks/#security","title":"Security","text":"<p>Query-based checks enforce multiple security layers:</p> Control Detail Read-only Only <code>SELECT</code> statements allowed Forbidden keywords <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>DROP</code>, <code>CREATE</code>, etc. blocked Injection patterns <code>OR 1=1</code>, <code>UNION SELECT</code>, stacked queries blocked Complexity limit Score capped at 50 (JOINs, subqueries, aggregates add points) Timeout 30-second execution limit Row limit Results capped at 10,000 rows"},{"location":"guide/query-checks/#yaml-rules","title":"YAML Rules","text":"<pre><code>checks:\n  _query:\n    - query_no_rows:\n        query: \"SELECT * FROM table WHERE total &lt; subtotal\"\n    - query_result_equals:\n        query: \"SELECT COUNT(*) FROM table WHERE status = 'error'\"\n        expected: 0\n    - query_result_between:\n        query: \"SELECT AVG(amount) FROM table\"\n        min_value: 10\n        max_value: 500\n</code></pre>"},{"location":"guide/row-level-errors/","title":"Row-Level Errors","text":"<p>Pinpoint exactly which rows failed validation \u2014 with values, reasons, and context.</p>"},{"location":"guide/row-level-errors/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\n\ndata = connect(\"orders.csv\")\nresult = data.amount.between(1, 1000)\n\nif not result:\n    print(result.summary())\n    # Sample of 10 failing rows (total: 47):\n    #   Row 23: amount=0 - expected between 1 and 1000\n    #   Row 156: amount=-5 - expected between 1 and 1000\n    #   ... and 42 more failures\n</code></pre>"},{"location":"guide/row-level-errors/#failedrow-structure","title":"FailedRow Structure","text":"<p>Each failed row captures:</p> <pre><code>for row in result.failed_rows:\n    print(row.row_index)   # 1-based row number\n    print(row.column)      # Column name\n    print(row.value)       # Actual value that failed\n    print(row.expected)    # What was expected\n    print(row.reason)      # Human-readable explanation\n    print(row.context)     # Additional row data (dict)\n</code></pre>"},{"location":"guide/row-level-errors/#accessing-failed-data","title":"Accessing Failed Data","text":"<pre><code>result = data.email.matches(r\"^[\\w.]+@[\\w.]+$\")\n\n# Get just the bad values\nbad_values = result.get_failed_values()\n# ['not-an-email', '', 'missing@', ...]\n\n# Get row indices\nbad_rows = result.get_failed_row_indices()\n# [12, 45, 78, ...]\n\n# Total failure count (may exceed sample size)\nprint(result.total_failures)  # 47\n</code></pre>"},{"location":"guide/row-level-errors/#convert-to-dataframe","title":"Convert to DataFrame","text":"<p>Export failed rows to pandas for analysis:</p> <pre><code>df = result.to_dataframe()\nprint(df)\n#    row_index  column       value              expected  reason\n# 0         12   email  not-an-email   matches pattern    ...\n# 1         45   email               matches pattern    ...\n</code></pre> <p>Note</p> <p>Requires pandas: <code>pip install pandas</code></p>"},{"location":"guide/row-level-errors/#summary-output","title":"Summary Output","text":"<p>The <code>summary()</code> method gives a concise overview:</p> <pre><code>print(result.summary())\n</code></pre> <p>Output:</p> <pre><code>Column 'amount' has 47 values outside range [1, 1000]\n\nSample of 5 failing rows (total: 47):\n  Row 23: amount=0 - expected between 1 and 1000\n  Row 156: amount=-5 - expected between 1 and 1000\n  Row 301: amount=1500 - expected between 1 and 1000\n  Row 445: amount=-12.5 - expected between 1 and 1000\n  Row 502: amount=0 - expected between 1 and 1000\n  ... and 42 more failures\n</code></pre>"},{"location":"guide/row-level-errors/#validationresult-in-detail","title":"ValidationResult in Detail","text":"<pre><code>result = data.amount.between(0, 100)\n\nresult.passed          # True/False\nresult.actual_value    # Count of failures\nresult.expected_value  # What was expected\nresult.message         # Human-readable summary\nresult.details         # Additional metadata dict\nresult.failed_rows     # List[FailedRow] \u2014 sample of failures\nresult.total_failures  # Total count (may exceed sample)\n</code></pre>"},{"location":"guide/row-level-errors/#boolean-context","title":"Boolean Context","text":"<p><code>ValidationResult</code> works directly in assertions and <code>if</code> statements:</p> <pre><code># Use in assert\nassert data.customer_id.null_percent == 0\n\n# Use in if\nresult = data.amount.between(0, 10000)\nif not result:\n    print(f\"Failed: {result.message}\")\n</code></pre>"},{"location":"guide/row-level-errors/#practical-patterns","title":"Practical Patterns","text":""},{"location":"guide/row-level-errors/#debugging-pipeline-failures","title":"Debugging pipeline failures","text":"<pre><code>result = data.price.between(0, 999)\nif not result:\n    df = result.to_dataframe()\n    df.to_csv(\"failed_rows.csv\")\n    raise AssertionError(result.summary())\n</code></pre>"},{"location":"guide/row-level-errors/#conditional-alerting","title":"Conditional alerting","text":"<pre><code>result = data.email.matches(r\"^[\\w.]+@[\\w.]+\\.\\w{2,}$\")\nif result.total_failures &gt; 100:\n    send_alert(f\"Email validation: {result.total_failures} failures\")\nelif result.total_failures &gt; 0:\n    log_warning(result.summary())\n</code></pre>"},{"location":"guide/row-level-errors/#combining-with-quality-score","title":"Combining with quality score","text":"<pre><code>score = data.score()\nresults = [\n    data.order_id.null_percent == 0,\n    data.amount.between(0, 10000),\n    data.email.matches(r\"^[\\w.]+@[\\w.]+\\.\\w{2,}$\"),\n]\n\nfor r in results:\n    if not r:\n        print(r.summary())\n\nprint(f\"Overall quality: {score.grade}\")\n</code></pre>"},{"location":"guide/yaml-rules/","title":"YAML Rules","text":"<p>Define validation rules in YAML \u2014 no Python needed. DuckGuard supports both structured and natural-language formats.</p>"},{"location":"guide/yaml-rules/#quick-start","title":"Quick Start","text":"<pre><code># duckguard.yaml\nsource: data/orders.csv\n\nchecks:\n  customer_id:\n    - not_null\n    - unique\n\n  amount:\n    - positive\n    - range: [0, 10000]\n\n  email:\n    - pattern: email\n    - null_percent: \"&lt; 5\"\n\n  status:\n    - allowed_values: [pending, shipped, delivered]\n\ntable:\n  - row_count: \"&gt; 0\"\n</code></pre> <pre><code>duckguard check data/orders.csv --config duckguard.yaml\n</code></pre>"},{"location":"guide/yaml-rules/#natural-language-format","title":"Natural Language Format","text":"<p>Use the <code>rules:</code> key for plain-English expressions:</p> <pre><code>source: data/orders.csv\n\nrules:\n  - order_id is not null\n  - order_id is unique\n  - amount &gt;= 0\n  - amount between 0 and 10000\n  - status in ['pending', 'shipped', 'delivered']\n  - row_count &gt; 0\n</code></pre>"},{"location":"guide/yaml-rules/#check-types","title":"Check Types","text":""},{"location":"guide/yaml-rules/#null-checks","title":"Null Checks","text":"<pre><code>checks:\n  name:\n    - not_null               # Zero nulls\n    - null_percent: \"&lt; 5\"   # Less than 5% nulls\n</code></pre>"},{"location":"guide/yaml-rules/#uniqueness","title":"Uniqueness","text":"<pre><code>checks:\n  email:\n    - unique                 # 100% unique\n    - unique_percent: \"&gt; 95\" # At least 95% unique\n    - no_duplicates          # Same as unique\n</code></pre>"},{"location":"guide/yaml-rules/#value-ranges","title":"Value Ranges","text":"<pre><code>checks:\n  amount:\n    - positive             # &gt; 0\n    - non_negative         # &gt;= 0\n    - min: 0               # All values &gt;= 0\n    - max: 10000           # All values &lt;= 10000\n    - range: [0, 10000]    # Between 0 and 10000\n    - between: [0, 10000]  # Alias for range\n</code></pre>"},{"location":"guide/yaml-rules/#patterns","title":"Patterns","text":"<pre><code>checks:\n  email:\n    - pattern: email       # Built-in email pattern\n  phone:\n    - pattern: phone       # Built-in phone pattern\n  id:\n    - pattern: uuid        # Built-in UUID pattern\n  custom:\n    - pattern: \"^[A-Z]{3}-\\\\d{4}$\"  # Custom regex\n</code></pre> <p>Built-in patterns: <code>email</code>, <code>phone</code>, <code>uuid</code>, <code>url</code>, <code>ip_address</code>, <code>date_iso</code>, <code>datetime_iso</code>, <code>ssn</code>, <code>zip_us</code>, <code>credit_card</code>, <code>slug</code>, <code>alpha</code>, <code>alphanumeric</code>, <code>numeric</code></p>"},{"location":"guide/yaml-rules/#allowed-values","title":"Allowed Values","text":"<pre><code>checks:\n  status:\n    - allowed_values: [pending, shipped, delivered]\n  # Aliases also work:\n  country:\n    - isin: [US, CA, UK, DE]\n</code></pre>"},{"location":"guide/yaml-rules/#string-length","title":"String Length","text":"<pre><code>checks:\n  name:\n    - min_length: 1\n    - max_length: 100\n    - length: [1, 100]    # Combined min/max\n</code></pre>"},{"location":"guide/yaml-rules/#table-level-checks","title":"Table-Level Checks","text":"<pre><code>table:\n  - row_count: \"&gt; 0\"\n  - row_count: \"&gt; 1000\"\n</code></pre>"},{"location":"guide/yaml-rules/#severity-levels","title":"Severity Levels","text":"<p>Override severity per check \u2014 <code>error</code> (default), <code>warning</code>, or <code>info</code>:</p> <pre><code>checks:\n  description:\n    - not_null:\n        severity: warning\n        message: \"Description is recommended\"\n</code></pre>"},{"location":"guide/yaml-rules/#conditional-checks-duckguard-30","title":"Conditional Checks (DuckGuard 3.0)","text":"<pre><code>checks:\n  state:\n    - not_null_when:\n        condition: \"country = 'USA'\"\n  tracking_number:\n    - not_null_when:\n        condition: \"status = 'shipped'\"\n  price:\n    - between_when:\n        value: [0, 999999]\n        condition: \"status = 'COMPLETED'\"\n</code></pre>"},{"location":"guide/yaml-rules/#multi-column-query-checks","title":"Multi-Column &amp; Query Checks","text":"<pre><code>checks:\n  _multicolumn:\n    - column_pair_satisfy:\n        column_a: end_date\n        column_b: start_date\n        expression: \"end_date &gt;= start_date\"\n    - multicolumn_unique:\n        columns: [user_id, session_id]\n\n  _query:\n    - query_no_rows:\n        query: \"SELECT * FROM table WHERE total &lt; subtotal\"\n</code></pre>"},{"location":"guide/yaml-rules/#auto-generate-rules","title":"Auto-Generate Rules","text":"<pre><code>duckguard discover data.csv --output duckguard.yaml\n</code></pre> <p>Or in Python:</p> <pre><code>from duckguard import generate_rules\n\nrules = generate_rules(\"data.csv\", as_yaml=True)\nprint(rules)\n</code></pre>"},{"location":"guide/yaml-rules/#execute-programmatically","title":"Execute Programmatically","text":"<pre><code>from duckguard import load_rules, execute_rules\n\nruleset = load_rules(\"duckguard.yaml\")\nresult = execute_rules(ruleset, source=\"data.csv\")\n\nprint(result.passed)        # True/False\nprint(result.quality_score) # 0-100\nprint(result.failed_count)  # Number of failures\n\nfor failure in result.get_failures():\n    print(f\"[{failure.column}] {failure.message}\")\n</code></pre>"},{"location":"integrations/airflow/","title":"Airflow Integration","text":"<p>Add data quality gates to Airflow DAGs using DuckGuard.</p>"},{"location":"integrations/airflow/#quick-start","title":"Quick Start","text":"<pre><code>from airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndef validate_orders():\n    from duckguard import connect\n\n    data = connect(\"s3://datalake/orders/orders.parquet\")\n    assert data.row_count &gt; 0\n    assert data.order_id.null_percent == 0\n    assert data.amount.between(0, 100000)\n\n    score = data.score()\n    if score.overall &lt; 80:\n        raise ValueError(f\"Quality score {score.overall:.0f} below threshold\")\n\nwith DAG(\"orders_quality\", start_date=datetime(2024, 1, 1), schedule=\"@daily\"):\n    validate = PythonOperator(\n        task_id=\"validate_orders\",\n        python_callable=validate_orders,\n    )\n</code></pre>"},{"location":"integrations/airflow/#quality-gate-pattern","title":"Quality Gate Pattern","text":"<p>Use DuckGuard as a gate between pipeline stages:</p> <pre><code>def extract():\n    \"\"\"Extract data from source.\"\"\"\n    ...\n\ndef validate():\n    \"\"\"Quality gate \u2014 fails the DAG if data is bad.\"\"\"\n    from duckguard import connect, load_rules, execute_rules\n\n    data = connect(\"s3://staging/orders.parquet\")\n    rules = load_rules(\"/opt/airflow/duckguard.yaml\")\n    result = execute_rules(rules, dataset=data)\n\n    if not result.passed:\n        raise ValueError(\n            f\"Quality check failed: {result.failed_count}/{result.total_checks} \"\n            f\"checks failed (score: {result.quality_score:.0f}%)\"\n        )\n\ndef transform():\n    \"\"\"Transform data \u2014 only runs if validation passes.\"\"\"\n    ...\n\nwith DAG(\"etl_pipeline\", schedule=\"@daily\", start_date=datetime(2024, 1, 1)):\n    extract_task = PythonOperator(task_id=\"extract\", python_callable=extract)\n    validate_task = PythonOperator(task_id=\"validate\", python_callable=validate)\n    transform_task = PythonOperator(task_id=\"transform\", python_callable=transform)\n\n    extract_task &gt;&gt; validate_task &gt;&gt; transform_task\n</code></pre>"},{"location":"integrations/airflow/#contract-validation","title":"Contract Validation","text":"<pre><code>def validate_contract():\n    from duckguard import load_contract, validate_contract\n\n    contract = load_contract(\"/opt/airflow/contracts/orders.contract.yaml\")\n    result = validate_contract(contract, \"s3://datalake/orders.parquet\")\n\n    if not result.passed:\n        raise ValueError(result.summary())\n</code></pre>"},{"location":"integrations/airflow/#freshness-checks","title":"Freshness Checks","text":"<pre><code>def check_freshness():\n    from duckguard import connect\n    from datetime import timedelta\n\n    data = connect(\"s3://datalake/orders.parquet\")\n    if not data.is_fresh(timedelta(hours=6)):\n        raise ValueError(f\"Data is stale: {data.freshness.age_human}\")\n</code></pre>"},{"location":"integrations/airflow/#anomaly-detection","title":"Anomaly Detection","text":"<pre><code>def check_anomalies():\n    from duckguard import connect\n    from duckguard.anomaly import detect_anomalies\n\n    data = connect(\"s3://datalake/metrics.parquet\")\n    report = detect_anomalies(data, method=\"zscore\")\n\n    if report.has_anomalies:\n        raise ValueError(f\"Anomalies detected: {report.anomaly_count}\")\n</code></pre>"},{"location":"integrations/airflow/#generate-reports","title":"Generate Reports","text":"<pre><code>def generate_report():\n    from duckguard import connect\n    from duckguard.rules import load_rules, execute_rules\n    from duckguard.reports import HTMLReporter\n\n    data = connect(\"s3://datalake/orders.parquet\")\n    rules = load_rules(\"/opt/airflow/duckguard.yaml\")\n    result = execute_rules(rules, dataset=data)\n\n    reporter = HTMLReporter()\n    reporter.generate(result, \"/opt/airflow/reports/quality.html\")\n</code></pre>"},{"location":"integrations/airflow/#bashoperator-alternative","title":"BashOperator Alternative","text":"<pre><code>validate = BashOperator(\n    task_id=\"validate\",\n    bash_command=\"duckguard check s3://datalake/orders.parquet --config duckguard.yaml\",\n)\n</code></pre> <p>The CLI exits with code 1 on failure, which fails the Airflow task.</p>"},{"location":"integrations/airflow/#tips","title":"Tips","text":"<ul> <li>Install DuckGuard in your Airflow worker image: <code>pip install duckguard</code></li> <li>Store YAML rules and contracts alongside your DAGs</li> <li>Use <code>PythonOperator</code> for flexibility, <code>BashOperator</code> for simplicity</li> <li>Set <code>retries=0</code> on quality gate tasks \u2014 don't retry bad data</li> </ul>"},{"location":"integrations/dbt/","title":"dbt Integration","text":"<p>Use DuckGuard alongside dbt to validate models after transformation.</p>"},{"location":"integrations/dbt/#quick-start","title":"Quick Start","text":"<p>Add a post-hook or write a Python script that validates dbt output:</p> <pre><code># tests/test_dbt_models.py\nfrom duckguard import connect\n\ndef test_stg_orders():\n    \"\"\"Validate the stg_orders dbt model.\"\"\"\n    data = connect(\"target/stg_orders.parquet\")\n\n    assert data.row_count &gt; 0\n    assert data.order_id.null_percent == 0\n    assert data.order_id.has_no_duplicates()\n    assert data.amount.between(0, 100000)\n</code></pre>"},{"location":"integrations/dbt/#validate-dbt-models","title":"Validate dbt Models","text":""},{"location":"integrations/dbt/#after-dbt-run","title":"After <code>dbt run</code>","text":"<p>Point DuckGuard at the output of your dbt models:</p> <pre><code>from duckguard import connect\n\n# If dbt outputs to a database\ndata = connect(\"postgres://localhost/analytics\", table=\"stg_orders\")\n\n# If dbt outputs to files (dbt-duckdb)\ndata = connect(\"target/stg_orders.parquet\")\n</code></pre>"},{"location":"integrations/dbt/#with-dbt-duckdb","title":"With dbt-duckdb","text":"<p>If you use <code>dbt-duckdb</code>, models are stored as DuckDB tables or Parquet files:</p> <pre><code>data = connect(\"target/dev.duckdb\", table=\"stg_orders\")\n</code></pre>"},{"location":"integrations/dbt/#data-contracts-for-dbt-models","title":"Data Contracts for dbt Models","text":"<p>Define contracts for each model:</p> <pre><code># contracts/stg_orders.contract.yaml\ncontract:\n  name: stg_orders\n  version: \"1.0.0\"\n\n  schema:\n    - name: order_id\n      type: string\n      required: true\n      unique: true\n    - name: amount\n      type: decimal\n      required: true\n      constraints:\n        - type: range\n          value: [0, 100000]\n    - name: status\n      type: string\n      required: true\n      constraints:\n        - type: allowed_values\n          value: [pending, shipped, delivered, cancelled]\n\n  quality:\n    completeness: 99.0\n    row_count_min: 100\n</code></pre> <pre><code>from duckguard import load_contract, validate_contract\n\ncontract = load_contract(\"contracts/stg_orders.contract.yaml\")\nresult = validate_contract(contract, \"postgres://localhost/analytics\",\n                           table=\"stg_orders\")\nassert result.passed, result.summary()\n</code></pre>"},{"location":"integrations/dbt/#dbt-test-replacement","title":"dbt Test Replacement","text":"<p>DuckGuard can replace or supplement dbt tests:</p> dbt Test DuckGuard Equivalent <code>unique</code> <code>col.has_no_duplicates()</code> <code>not_null</code> <code>col.null_percent == 0</code> <code>accepted_values</code> <code>col.isin([...])</code> <code>relationships</code> <code>data.reconcile(other, key_columns=[...])</code> <p>DuckGuard adds capabilities dbt tests don't have: anomaly detection, distribution tests, quality scoring, PII detection, and profiling.</p>"},{"location":"integrations/dbt/#pipeline-pattern","title":"Pipeline Pattern","text":"<pre><code>import subprocess\nfrom duckguard import connect, load_rules, execute_rules\n\n# 1. Run dbt\nsubprocess.run([\"dbt\", \"run\"], check=True)\n\n# 2. Validate output\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, source=\"target/dev.duckdb\")\n\nif not result.passed:\n    print(f\"Quality gate failed: {result.failed_count} checks\")\n    exit(1)\n\nprint(f\"Quality score: {result.quality_score:.0f}%\")\n</code></pre>"},{"location":"integrations/dbt/#quality-reports","title":"Quality Reports","text":"<p>Generate HTML reports for dbt model quality:</p> <pre><code>duckguard report target/stg_orders.parquet \\\n  --config duckguard.yaml \\\n  --title \"stg_orders Quality Report\" \\\n  --output reports/stg_orders.html\n</code></pre>"},{"location":"integrations/github-actions/","title":"GitHub Actions","text":"<p>Run DuckGuard checks in CI/CD to catch data quality issues before merge.</p>"},{"location":"integrations/github-actions/#quick-start","title":"Quick Start","text":"<pre><code># .github/workflows/data-quality.yml\nname: Data Quality\n\non:\n  push:\n    paths: ['data/**', 'duckguard.yaml', 'contracts/**']\n  pull_request:\n    paths: ['data/**', 'duckguard.yaml', 'contracts/**']\n\njobs:\n  quality:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install DuckGuard\n        run: pip install duckguard\n\n      - name: Run quality checks\n        run: duckguard check data/orders.csv --config duckguard.yaml\n</code></pre> <p>The CLI exits with code 1 on failure, which fails the workflow.</p>"},{"location":"integrations/github-actions/#yaml-rule-checks","title":"YAML Rule Checks","text":"<pre><code>      - name: Run YAML rules\n        run: duckguard check data/orders.csv --config duckguard.yaml\n\n      - name: Run quick checks\n        run: duckguard check data/orders.csv --not-null order_id --unique email\n</code></pre>"},{"location":"integrations/github-actions/#contract-validation","title":"Contract Validation","text":"<pre><code>      - name: Validate contracts\n        run: |\n          duckguard contract validate data/orders.csv \\\n            --contract contracts/orders.contract.yaml\n</code></pre>"},{"location":"integrations/github-actions/#pytest-tests","title":"pytest Tests","text":"<pre><code>      - name: Run data quality tests\n        run: pytest tests/test_data_quality.py -v --junitxml=results.xml\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v4\n        if: always()\n        with:\n          name: test-results\n          path: results.xml\n</code></pre>"},{"location":"integrations/github-actions/#quality-reports","title":"Quality Reports","text":"<p>Generate and upload HTML reports as artifacts:</p> <pre><code>      - name: Generate quality report\n        run: |\n          duckguard report data/orders.csv \\\n            --config duckguard.yaml \\\n            --output report.html \\\n            --title \"Orders Quality Report\"\n\n      - name: Upload report\n        uses: actions/upload-artifact@v4\n        if: always()\n        with:\n          name: quality-report\n          path: report.html\n</code></pre>"},{"location":"integrations/github-actions/#quality-gate-with-score","title":"Quality Gate with Score","text":"<p>Fail the build if quality drops below a threshold:</p> <pre><code>      - name: Quality gate\n        run: |\n          python -c \"\n          from duckguard import connect\n          data = connect('data/orders.csv')\n          score = data.score()\n          print(f'Quality: {score.overall:.0f}/100 ({score.grade})')\n          if score.overall &lt; 80:\n              raise SystemExit(f'Quality {score.overall:.0f} below 80 threshold')\n          \"\n</code></pre>"},{"location":"integrations/github-actions/#contract-diff-on-pr","title":"Contract Diff on PR","text":"<p>Check for breaking changes when contracts are modified:</p> <pre><code>      - name: Check contract changes\n        if: github.event_name == 'pull_request'\n        run: |\n          git show HEAD~1:contracts/orders.contract.yaml &gt; /tmp/old.yaml 2&gt;/dev/null || exit 0\n          duckguard contract diff /tmp/old.yaml contracts/orders.contract.yaml\n</code></pre>"},{"location":"integrations/github-actions/#full-pipeline-example","title":"Full Pipeline Example","text":"<pre><code>name: Data Quality Pipeline\n\non: [push, pull_request]\n\njobs:\n  quality:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - run: pip install duckguard\n\n      - name: Profile data\n        run: duckguard profile data/orders.csv\n\n      - name: Run checks\n        run: duckguard check data/orders.csv --config duckguard.yaml\n\n      - name: Validate contract\n        run: |\n          duckguard contract validate data/orders.csv \\\n            --contract contracts/orders.contract.yaml\n\n      - name: Generate report\n        if: always()\n        run: duckguard report data/orders.csv --output report.html\n\n      - name: Upload report\n        uses: actions/upload-artifact@v4\n        if: always()\n        with:\n          name: quality-report\n          path: report.html\n</code></pre>"},{"location":"integrations/github-actions/#scheduled-checks","title":"Scheduled Checks","text":"<p>Run quality checks on a schedule (not just on push):</p> <pre><code>on:\n  schedule:\n    - cron: '0 8 * * *'  # Daily at 8am UTC\n</code></pre>"},{"location":"integrations/notifications/","title":"Notifications","text":"<p>Send alerts to Slack, Teams, or Email when data quality checks fail.</p>"},{"location":"integrations/notifications/#quick-start","title":"Quick Start","text":"<pre><code>from duckguard import connect\nfrom duckguard.notifications import SlackNotifier\n\ndata = connect(\"orders.csv\")\nresult = data.amount.between(0, 10000)\n\nif not result:\n    slack = SlackNotifier(webhook_url=\"https://hooks.slack.com/services/...\")\n    slack.send_failure_alert(result)\n</code></pre>"},{"location":"integrations/notifications/#slack","title":"Slack","text":""},{"location":"integrations/notifications/#setup","title":"Setup","text":"<ol> <li>Create a Slack Incoming Webhook at api.slack.com/messaging/webhooks</li> <li>Copy the webhook URL</li> </ol>"},{"location":"integrations/notifications/#usage","title":"Usage","text":"<pre><code>from duckguard.notifications import SlackNotifier\n\nslack = SlackNotifier(\n    webhook_url=\"https://hooks.slack.com/services/T00/B00/xxx\"\n)\n\n# Send alert on failure\nif not result:\n    slack.send_failure_alert(result)\n\n# Send quality score\nscore = data.score()\nslack.send_score_alert(score)\n</code></pre>"},{"location":"integrations/notifications/#environment-variable","title":"Environment Variable","text":"<pre><code>export DUCKGUARD_SLACK_WEBHOOK=\"https://hooks.slack.com/services/...\"\n</code></pre> <pre><code>import os\nslack = SlackNotifier(webhook_url=os.environ[\"DUCKGUARD_SLACK_WEBHOOK\"])\n</code></pre>"},{"location":"integrations/notifications/#microsoft-teams","title":"Microsoft Teams","text":""},{"location":"integrations/notifications/#setup_1","title":"Setup","text":"<ol> <li>Create an Incoming Webhook connector in your Teams channel</li> <li>Copy the webhook URL</li> </ol>"},{"location":"integrations/notifications/#usage_1","title":"Usage","text":"<pre><code>from duckguard.notifications import TeamsNotifier\n\nteams = TeamsNotifier(\n    webhook_url=\"https://outlook.office.com/webhook/...\"\n)\n\nif not result:\n    teams.send_failure_alert(result)\n</code></pre>"},{"location":"integrations/notifications/#email","title":"Email","text":""},{"location":"integrations/notifications/#usage_2","title":"Usage","text":"<pre><code>from duckguard.notifications import EmailNotifier\n\nemail = EmailNotifier(\n    smtp_host=\"smtp.gmail.com\",\n    smtp_port=587,\n    username=\"alerts@company.com\",\n    password=\"app-password\",\n    from_addr=\"alerts@company.com\",\n    to_addrs=[\"team@company.com\"],\n)\n\nif not result:\n    email.send_failure_alert(result)\n</code></pre>"},{"location":"integrations/notifications/#environment-variables","title":"Environment Variables","text":"<pre><code>export DUCKGUARD_SMTP_HOST=\"smtp.gmail.com\"\nexport DUCKGUARD_SMTP_PORT=\"587\"\nexport DUCKGUARD_SMTP_USER=\"alerts@company.com\"\nexport DUCKGUARD_SMTP_PASS=\"app-password\"\n</code></pre>"},{"location":"integrations/notifications/#pipeline-pattern","title":"Pipeline Pattern","text":"<pre><code>from duckguard import connect, load_rules, execute_rules\nfrom duckguard.notifications import SlackNotifier\n\n# Run checks\ndata = connect(\"s3://datalake/orders.parquet\")\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, dataset=data)\n\n# Notify on failure\nif not result.passed:\n    slack = SlackNotifier(webhook_url=os.environ[\"SLACK_WEBHOOK\"])\n    slack.send_failure_alert(result)\n    raise SystemExit(1)\n</code></pre>"},{"location":"integrations/notifications/#airflow-integration","title":"Airflow Integration","text":"<pre><code>def validate_and_notify():\n    from duckguard import connect\n    from duckguard.notifications import SlackNotifier\n\n    data = connect(\"s3://lake/orders.parquet\")\n    result = data.order_id.null_percent == 0\n\n    if not result:\n        slack = SlackNotifier(webhook_url=os.environ[\"SLACK_WEBHOOK\"])\n        slack.send_failure_alert(result)\n        raise ValueError(\"Quality check failed\")\n</code></pre>"},{"location":"integrations/notifications/#alert-content","title":"Alert Content","text":"<p>Alerts include:</p> <ul> <li>Check name and status (PASS/FAIL)</li> <li>Actual vs expected values</li> <li>Failure count and sample</li> <li>Data source and timestamp</li> <li>Quality score (if available)</li> </ul>"},{"location":"integrations/notifications/#custom-notifications","title":"Custom Notifications","text":"<p>Build your own notifier by posting to any webhook:</p> <pre><code>import requests\n\ndef send_custom_alert(result, webhook_url):\n    payload = {\n        \"text\": f\"DuckGuard: {result.message}\",\n        \"passed\": result.passed,\n        \"failures\": result.total_failures,\n    }\n    requests.post(webhook_url, json=payload)\n</code></pre>"},{"location":"integrations/pytest/","title":"pytest Integration","text":"<p>Write data quality tests as standard pytest tests \u2014 no plugins needed.</p>"},{"location":"integrations/pytest/#quick-start","title":"Quick Start","text":"<pre><code># tests/test_data_quality.py\nfrom duckguard import connect\n\ndef test_orders_quality():\n    data = connect(\"data/orders.csv\")\n\n    assert data.row_count &gt; 0\n    assert data.order_id.null_percent == 0\n    assert data.order_id.has_no_duplicates()\n    assert data.amount.between(0, 10000)\n    assert data.status.isin([\"pending\", \"shipped\", \"delivered\"])\n</code></pre> <pre><code>pytest tests/test_data_quality.py -v\n</code></pre>"},{"location":"integrations/pytest/#why-it-works","title":"Why It Works","text":"<p>DuckGuard's <code>ValidationResult</code> implements <code>__bool__</code>, so it works directly with <code>assert</code>:</p> <pre><code>result = data.amount.between(0, 10000)\nassert result  # Passes if all values in range\n</code></pre> <p>On failure, pytest shows the result details automatically.</p>"},{"location":"integrations/pytest/#better-error-messages","title":"Better Error Messages","text":"<p>Include the summary in assertions for detailed failure output:</p> <pre><code>def test_amount_range():\n    data = connect(\"orders.csv\")\n    result = data.amount.between(0, 10000)\n    assert result, result.summary()\n    # On failure:\n    # AssertionError: Column 'amount' has 47 values outside range [0, 10000]\n    # Sample of 5 failing rows (total: 47):\n    #   Row 23: amount=-5 ...\n</code></pre>"},{"location":"integrations/pytest/#fixtures","title":"Fixtures","text":"<p>Use pytest fixtures for shared dataset connections:</p> <pre><code>import pytest\nfrom duckguard import connect\n\n@pytest.fixture\ndef orders():\n    return connect(\"data/orders.csv\")\n\n@pytest.fixture\ndef customers():\n    return connect(\"data/customers.csv\")\n\ndef test_orders_completeness(orders):\n    assert orders.order_id.null_percent == 0\n    assert orders.customer_id.null_percent == 0\n\ndef test_orders_values(orders):\n    assert orders.total_amount.between(0, 100000)\n    assert orders.status.isin([\"pending\", \"shipped\", \"delivered\"])\n\ndef test_customer_uniqueness(customers):\n    assert customers.email.has_no_duplicates()\n</code></pre>"},{"location":"integrations/pytest/#parameterized-tests","title":"Parameterized Tests","text":"<p>Test multiple columns with <code>@pytest.mark.parametrize</code>:</p> <pre><code>@pytest.mark.parametrize(\"column\", [\"order_id\", \"customer_id\", \"product_id\"])\ndef test_required_columns(orders, column):\n    assert orders[column].null_percent == 0\n\n@pytest.mark.parametrize(\"column,min_val,max_val\", [\n    (\"amount\", 0, 100000),\n    (\"quantity\", 1, 10000),\n    (\"discount\", 0, 100),\n])\ndef test_ranges(orders, column, min_val, max_val):\n    assert orders[column].between(min_val, max_val)\n</code></pre>"},{"location":"integrations/pytest/#auto-generate-tests","title":"Auto-Generate Tests","text":"<p>Let DuckGuard write your tests:</p> <pre><code>from duckguard import connect\nfrom duckguard.profiler import AutoProfiler\n\ndata = connect(\"orders.csv\")\nprofiler = AutoProfiler()\ntest_code = profiler.generate_test_file(data, output_var=\"orders\")\n\nwith open(\"tests/test_orders.py\", \"w\") as f:\n    f.write(test_code)\n</code></pre>"},{"location":"integrations/pytest/#yaml-based-tests","title":"YAML-Based Tests","text":"<p>Run YAML rules inside pytest:</p> <pre><code>from duckguard import load_rules, execute_rules\n\ndef test_yaml_rules():\n    rules = load_rules(\"duckguard.yaml\")\n    result = execute_rules(rules)\n    assert result.passed, f\"{result.failed_count} checks failed\"\n</code></pre>"},{"location":"integrations/pytest/#quality-gate-pattern","title":"Quality Gate Pattern","text":"<pre><code>def test_quality_gate():\n    data = connect(\"data/orders.csv\")\n    score = data.score()\n    assert score.overall &gt;= 80, f\"Quality {score.overall:.0f}/100 below threshold\"\n    assert score.grade in (\"A\", \"B\"), f\"Grade {score.grade} below acceptable\"\n</code></pre>"},{"location":"integrations/pytest/#ci-integration","title":"CI Integration","text":"<pre><code># Run data quality tests in CI\npytest tests/test_data_quality.py -v --tb=short --junitxml=results.xml\n</code></pre> <p>See GitHub Actions for CI/CD setup.</p>"},{"location":"integrations/reports/","title":"Reports","text":"<p>Generate beautiful HTML and PDF data quality reports.</p>"},{"location":"integrations/reports/#quick-start","title":"Quick Start","text":"<pre><code># HTML report (default)\nduckguard report data.csv --output report.html\n\n# PDF report\nduckguard report data.csv --format pdf --output report.pdf\n\n# With custom title and YAML rules\nduckguard report data.csv \\\n  --config duckguard.yaml \\\n  --title \"Orders Quality Report\" \\\n  --output report.html\n</code></pre>"},{"location":"integrations/reports/#cli-options","title":"CLI Options","text":"<pre><code>duckguard report &lt;source&gt; [options]\n\nOptions:\n  --config, -c      Path to duckguard.yaml rules file\n  --table, -t       Table name (for databases)\n  --format, -f      Output format: html (default), pdf\n  --output, -o      Output file path (default: report.html)\n  --title           Report title\n  --include-passed  Include passed checks (default: true)\n  --no-passed       Exclude passed checks\n  --store, -s       Store results in history database\n  --trends          Include quality trend charts from history\n  --trend-days      Number of days for trend charts (default: 30)\n  --dark-mode       Theme: auto, light, dark\n  --logo            Logo URL for report header\n</code></pre>"},{"location":"integrations/reports/#features","title":"Features","text":"Feature HTML PDF Dark/light mode \u2705 \u2705 Interactive tables \u2705 \u2014 Quality score \u2705 \u2705 Check results \u2705 \u2705 Trend charts \u2705 \u2705 Shareable \u2705 \u2705"},{"location":"integrations/reports/#python-api","title":"Python API","text":"<pre><code>from duckguard import connect\nfrom duckguard.rules import load_rules, execute_rules\nfrom duckguard.reports import HTMLReporter, PDFReporter, ReportConfig\n\n# Run checks\ndata = connect(\"orders.csv\")\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, dataset=data)\n\n# Configure report\nconfig = ReportConfig(\n    title=\"Orders Quality Report\",\n    include_passed=True,\n    include_trends=False,\n    dark_mode=\"auto\",\n    logo_url=\"https://example.com/logo.png\",\n)\n\n# Generate HTML\nreporter = HTMLReporter(config=config)\nreporter.generate(\n    result, \"report.html\",\n    row_count=data.row_count,\n    column_count=data.column_count,\n)\n\n# Generate PDF (requires weasyprint)\nreporter = PDFReporter(config=config)\nreporter.generate(result, \"report.pdf\")\n</code></pre>"},{"location":"integrations/reports/#trend-charts","title":"Trend Charts","text":"<p>Include historical quality trends by enabling history storage:</p> <pre><code># First, run checks with --store to build history\nduckguard report data.csv --config rules.yaml --store\n\n# Later, include trends in reports\nduckguard report data.csv --config rules.yaml --trends --trend-days 30\n</code></pre>"},{"location":"integrations/reports/#dark-mode","title":"Dark Mode","text":"<p>Reports support three theme modes:</p> <ul> <li><code>auto</code> \u2014 matches the viewer's system preference</li> <li><code>light</code> \u2014 always light theme</li> <li><code>dark</code> \u2014 always dark theme</li> </ul> <pre><code>duckguard report data.csv --dark-mode dark\n</code></pre>"},{"location":"integrations/reports/#pdf-requirements","title":"PDF Requirements","text":"<p>PDF generation requires <code>weasyprint</code>:</p> <pre><code>pip install 'duckguard[reports]'\n</code></pre>"},{"location":"integrations/reports/#example-workflow","title":"Example Workflow","text":"<pre><code># 1. Profile data\nduckguard profile data.csv\n\n# 2. Generate rules\nduckguard discover data.csv --output duckguard.yaml\n\n# 3. Run checks and generate report\nduckguard report data.csv \\\n  --config duckguard.yaml \\\n  --store \\\n  --trends \\\n  --title \"Daily Quality Report\" \\\n  --output reports/quality-$(date +%F).html\n</code></pre>"},{"location":"platforms/azure/","title":"DuckGuard for Azure","text":""},{"location":"platforms/azure/#one-quality-layer-across-your-entire-azure-stack","title":"One Quality Layer Across Your Entire Azure Stack","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Azure Data \u2502     \u2502  Microsoft  \u2502     \u2502   Azure     \u2502\n\u2502   Factory   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Fabric    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Purview   \u2502\n\u2502  (Orchestr) \u2502     \u2502  (Compute)  \u2502     \u2502 (Governance)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502                   \u2502                   \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n                   \u25bc                           \u25bc\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502  \ud83e\udd86 DuckGuard \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502 Quality      \u2502\n           \u2502  (Validate)   \u2502          \u2502 Metadata     \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u25bc          \u25bc          \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Azure  \u2502 \u2502Power BI\u2502 \u2502 Azure  \u2502\n   \u2502Monitor \u2502 \u2502Dashboard\u2502 \u2502DevOps \u2502\n   \u2502(Alert) \u2502 \u2502(Report)\u2502 \u2502 (CI)  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"platforms/azure/#azure-data-factory-integration","title":"Azure Data Factory Integration","text":"<p>Run DuckGuard as a validation step in your ADF pipelines.</p>"},{"location":"platforms/azure/#option-1-notebook-activity-fabric-synapse","title":"Option 1: Notebook Activity (Fabric / Synapse)","text":"<pre><code># ADF Notebook Activity \u2014 runs in Fabric or Synapse Spark\n%pip install duckguard -q\n\nfrom duckguard import connect, load_rules, execute_rules\n\n# Load the table that was just written by the previous pipeline step\ndf = spark.sql(\"SELECT * FROM staging.orders\").toPandas()\ndata = connect(df)\n\n# Validate\nrules = load_rules(\"/lakehouse/default/Files/duckguard.yaml\")\nresult = execute_rules(rules, data)\n\n# Quality gate \u2014 fail the pipeline if quality is below threshold\nscore = data.score()\nif score.grade in (\"D\", \"F\"):\n    raise Exception(\n        f\"Data quality gate FAILED: grade={score.grade} ({score.overall:.1f}/100)\\n\"\n        f\"Failed checks: {result.failed_count}/{result.total_checks}\\n\"\n        f\"{result.summary()}\"\n    )\n\nprint(f\"\u2705 Quality gate passed: {score.grade} ({score.overall:.1f}/100)\")\n</code></pre>"},{"location":"platforms/azure/#option-2-azure-function-activity","title":"Option 2: Azure Function Activity","text":"<p>Trigger a lightweight Azure Function for validation \u2014 no Spark needed:</p> <pre><code># Azure Function (HTTP trigger)\nimport azure.functions as func\nimport json\nfrom duckguard import connect\n\ndef main(req: func.HttpRequest) -&gt; func.HttpResponse:\n    body = req.get_json()\n    source = body[\"source\"]  # e.g., \"fabric+sql://...\"\n    table = body[\"table\"]\n    token = body[\"token\"]\n\n    data = connect(source, table=table, token=token)\n    score = data.score()\n\n    return func.HttpResponse(\n        json.dumps({\n            \"grade\": score.grade,\n            \"overall\": score.overall,\n            \"completeness\": score.completeness,\n            \"uniqueness\": score.uniqueness,\n            \"validity\": score.validity,\n            \"passed\": score.grade not in (\"D\", \"F\"),\n        }),\n        mimetype=\"application/json\"\n    )\n</code></pre> <p>ADF calls this via a Web Activity \u2014 zero compute overhead, scales to zero.</p>"},{"location":"platforms/azure/#option-3-custom-activity-azure-batch","title":"Option 3: Custom Activity (Azure Batch)","text":"<p>For large-scale validation on dedicated compute:</p> <pre><code># custom_activity.py \u2014 runs on Azure Batch via ADF Custom Activity\nimport sys\nfrom duckguard import connect\n\nsource = sys.argv[1]  # Connection string\ntable = sys.argv[2]   # Table name\n\ndata = connect(source, table=table, token=os.environ[\"FABRIC_TOKEN\"])\nscore = data.score()\n\n# Write results for ADF to pick up\nwith open(\"output.json\", \"w\") as f:\n    json.dump({\"grade\": score.grade, \"overall\": score.overall}, f)\n\nif score.grade in (\"D\", \"F\"):\n    sys.exit(1)  # Non-zero exit = ADF marks activity as failed\n</code></pre>"},{"location":"platforms/azure/#microsoft-purview-integration","title":"Microsoft Purview Integration","text":"<p>Push quality scores into Purview for governance and lineage tracking.</p> <pre><code>import requests\nfrom duckguard import connect\n\n# Validate data\ndata = connect(\"fabric://workspace/lakehouse/Tables/orders\", token=token)\nscore = data.score()\nprofile = AutoProfiler().profile(data)\n\n# Push quality metadata to Purview via REST API\npurview_url = \"https://your-purview.purview.azure.com\"\nheaders = {\n    \"Authorization\": f\"Bearer {purview_token}\",\n    \"Content-Type\": \"application/json\",\n}\n\n# Update asset with quality annotations\nquality_metadata = {\n    \"typeName\": \"DataSet\",\n    \"attributes\": {\n        \"qualifiedName\": \"fabric://workspace/lakehouse/Tables/orders\",\n        \"duckguard_quality_grade\": score.grade,\n        \"duckguard_quality_score\": score.overall,\n        \"duckguard_completeness\": score.completeness,\n        \"duckguard_uniqueness\": score.uniqueness,\n        \"duckguard_validity\": score.validity,\n        \"duckguard_last_checked\": datetime.utcnow().isoformat(),\n        \"duckguard_row_count\": data.row_count,\n        \"duckguard_pii_columns\": str(analysis.pii_columns),\n    }\n}\n\nrequests.put(\n    f\"{purview_url}/catalog/api/atlas/v2/entity\",\n    headers=headers,\n    json={\"entity\": quality_metadata}\n)\n</code></pre> <p>Governance Dashboard</p> <p>Once quality scores are in Purview, you can build a governance dashboard showing quality trends across all your data assets.</p>"},{"location":"platforms/azure/#azure-monitor-alerting","title":"Azure Monitor &amp; Alerting","text":"<p>Push quality metrics to Azure Monitor for dashboards and alerts.</p> <pre><code>from opencensus.ext.azure import metrics_exporter\nfrom duckguard import connect\n\n# Set up Azure Monitor exporter\nexporter = metrics_exporter.new_metrics_exporter(\n    connection_string=os.environ[\"APPLICATIONINSIGHTS_CONNECTION_STRING\"]\n)\n\n# Validate\ndata = connect(\"fabric://workspace/lakehouse/Tables/orders\", token=token)\nscore = data.score()\n\n# Push custom metrics\nexporter.export_metrics([\n    {\"name\": \"duckguard/quality_score\", \"value\": score.overall,\n     \"dimensions\": {\"table\": \"orders\", \"grade\": score.grade}},\n    {\"name\": \"duckguard/completeness\", \"value\": score.completeness,\n     \"dimensions\": {\"table\": \"orders\"}},\n])\n</code></pre>"},{"location":"platforms/azure/#alert-rules","title":"Alert Rules","text":"<p>Set up Azure Monitor alerts: - Quality drop: Alert when <code>duckguard/quality_score</code> drops below 70 - PII exposure: Alert when new PII columns are detected - Freshness: Alert when data is stale (via DuckGuard freshness monitor)</p>"},{"location":"platforms/azure/#power-bi-integration","title":"Power BI Integration","text":""},{"location":"platforms/azure/#python-visual","title":"Python Visual","text":"<p>Add a DuckGuard quality scorecard directly in Power BI:</p> <pre><code># Power BI Python visual script\n# Dataset is automatically available as 'dataset'\nfrom duckguard import connect\n\ndata = connect(dataset)  # Power BI passes the DataFrame\nscore = data.score()\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 4, figsize=(12, 3))\ndims = [\n    (\"Completeness\", score.completeness),\n    (\"Uniqueness\", score.uniqueness),\n    (\"Validity\", score.validity),\n    (\"Consistency\", score.consistency),\n]\ncolors = [\"#2ecc71\" if v &gt;= 80 else \"#f39c12\" if v &gt;= 60 else \"#e74c3c\" for _, v in dims]\n\nfor ax, (name, value), color in zip(axes, dims, colors):\n    ax.barh([name], [value], color=color)\n    ax.set_xlim(0, 100)\n    ax.text(value + 2, 0, f\"{value:.0f}%\", va=\"center\", fontweight=\"bold\")\n    ax.set_title(name, fontsize=10)\n\nplt.suptitle(f\"Quality Grade: {score.grade} ({score.overall:.0f}/100)\", fontsize=14, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"platforms/azure/#dataflow-integration","title":"Dataflow Integration","text":"<p>Run DuckGuard in a Power BI Dataflow (Gen2) Python step to validate before loading into the semantic model.</p>"},{"location":"platforms/azure/#azure-devops-pipeline","title":"Azure DevOps Pipeline","text":"<pre><code># azure-pipelines.yml\ntrigger:\n  - main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nsteps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: '3.11'\n\n  - script: pip install duckguard[fabric]\n    displayName: 'Install DuckGuard'\n\n  - script: |\n      python -c \"\n      import os\n      from duckguard import connect\n\n      data = connect(\n          'fabric+sql://$(FABRIC_SERVER)',\n          table='orders',\n          database='$(FABRIC_DATABASE)',\n          token=os.environ['FABRIC_TOKEN'],\n      )\n      score = data.score()\n      print(f'Quality: {score.grade} ({score.overall:.1f}/100)')\n      assert score.grade not in ('D', 'F'), f'Quality gate failed: {score.grade}'\n      \"\n    displayName: 'Data Quality Gate'\n    env:\n      FABRIC_TOKEN: $(FABRIC_TOKEN)\n</code></pre>"},{"location":"platforms/azure/#adls-gen2-blob-storage","title":"ADLS Gen2 / Blob Storage","text":"<p>DuckGuard reads Parquet and Delta files directly from Azure storage:</p> <pre><code>from duckguard import connect\n\n# ADLS Gen2\ndata = connect(\"abfss://container@account.dfs.core.windows.net/path/orders.parquet\")\n\n# Azure Blob Storage\ndata = connect(\"az://container/orders.parquet\")\n\n# With SAS token\ndata = connect(\n    \"az://container/orders.parquet\",\n    azure_storage_connection_string=\"DefaultEndpointsProtocol=https;...\"\n)\n</code></pre> <p>Authentication</p> <p>DuckDB supports Azure auth via:</p> <ul> <li><code>AZURE_STORAGE_CONNECTION_STRING</code> environment variable</li> <li><code>AZURE_STORAGE_ACCOUNT_NAME</code> + <code>AZURE_STORAGE_ACCOUNT_KEY</code></li> <li>SAS tokens</li> <li>Azure AD (via <code>azure-identity</code>)</li> </ul>"},{"location":"platforms/azure/#synapse-analytics","title":"Synapse Analytics","text":"<pre><code># Synapse Dedicated SQL Pool\ndata = connect(\n    \"mssql://your-synapse.sql.azuresynapse.net\",\n    table=\"orders\",\n    database=\"your_pool\",\n    token=\"&lt;azure-ad-token&gt;\"\n)\n\n# Synapse Serverless SQL Pool\ndata = connect(\n    \"mssql://your-synapse-ondemand.sql.azuresynapse.net\",\n    table=\"orders\",\n    database=\"your_db\",\n    token=\"&lt;azure-ad-token&gt;\"\n)\n</code></pre>"},{"location":"platforms/azure/#full-azure-architecture-example","title":"Full Azure Architecture Example","text":"<p>A production-grade data quality pipeline:</p> <pre><code>1. ADF Pipeline triggers on schedule or event\n2. Data lands in Fabric Lakehouse (Bronze layer)\n3. Notebook Activity runs DuckGuard validation\n4. Quality scores pushed to Purview (governance)\n5. Metrics pushed to Azure Monitor (alerting)\n6. If grade &gt;= B: promote to Silver layer\n7. If grade &lt; B: quarantine + notify via Teams webhook\n8. Power BI dashboard shows quality trends\n</code></pre> <pre><code># Step 3-7 in a single notebook:\nfrom duckguard import connect, load_rules, execute_rules\nfrom duckguard.notifications import TeamsNotifier\n\n# Validate\ndata = connect(df)\nscore = data.score()\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, data)\n\n# Push to Purview (step 4)\npush_to_purview(score, table_name=\"orders\")\n\n# Push to Monitor (step 5)\npush_to_monitor(score, table_name=\"orders\")\n\n# Quality gate (steps 6-7)\nif score.grade in (\"A\", \"B\", \"C\"):\n    spark.sql(\"INSERT INTO silver.orders SELECT * FROM bronze.orders\")\n    print(f\"\u2705 Promoted to Silver: {score.grade}\")\nelse:\n    spark.sql(\"INSERT INTO quarantine.orders SELECT * FROM bronze.orders\")\n    teams = TeamsNotifier(webhook_url=os.environ[\"TEAMS_WEBHOOK\"])\n    teams.send(f\"\u26a0\ufe0f Quality gate failed for orders: {score.grade} ({score.overall:.0f}/100)\")\n</code></pre>"},{"location":"platforms/azure/#install","title":"Install","text":"<pre><code># Fabric SQL endpoint\npip install duckguard[fabric]\n\n# Full Azure stack (Fabric + ADLS + SQL Server)\npip install duckguard[fabric,sqlserver]\n\n# Everything\npip install duckguard[all]\n</code></pre>"},{"location":"platforms/databricks/","title":"DuckGuard for Databricks","text":""},{"location":"platforms/databricks/#data-quality-for-unity-catalog-pytest-like-syntax-zero-spark-overhead","title":"Data quality for Unity Catalog \u2014 pytest-like syntax, zero Spark overhead","text":"<pre><code>from duckguard import connect\n\ndg = connect(\"databricks://workspace.databricks.com\", table=\"catalog.schema.orders\")\ndg.validate()\n</code></pre> <p>No Spark cluster. No notebook dependency. No 10-minute startup.</p>"},{"location":"platforms/databricks/#the-problem","title":"The Problem","text":"<p>You want to validate data in Databricks. Current options:</p> <ul> <li>Great Expectations \u2014 Needs a running Spark cluster to execute checks. Your <code>XS</code> cluster takes 5 minutes to start. Your validation checks take 8 seconds. You're paying for 5 minutes of idle compute.</li> <li>dbt tests \u2014 Only works inside dbt. Can't run from CI without a full dbt setup.</li> <li>Custom PySpark \u2014 You're writing SQL with extra steps.</li> </ul> <p>DuckGuard connects to Databricks via SQL endpoints. No Spark. No cluster. Runs from your laptop, CI runner, or a $5 VM.</p> <p>Cost savings</p> <p>A Databricks SQL warehouse (serverless) costs ~$0.07/query for validation aggregations. A Spark cluster costs ~$0.50/hour minimum just to exist. DuckGuard uses SQL warehouses by default.</p>"},{"location":"platforms/databricks/#quick-start","title":"Quick Start","text":""},{"location":"platforms/databricks/#install","title":"Install","text":"<pre><code>pip install duckguard[databricks]\n</code></pre>"},{"location":"platforms/databricks/#connect","title":"Connect","text":"Connection StringExplicit ParametersEnvironment Variables <pre><code>from duckguard import connect\n\ndg = connect(\n    \"databricks://my-workspace.cloud.databricks.com\",\n    table=\"catalog.schema.orders\",\n)\n</code></pre> <pre><code>from duckguard import connect\n\ndg = connect(\n    \"databricks://\",\n    server_hostname=\"my-workspace.cloud.databricks.com\",\n    http_path=\"/sql/1.0/warehouses/abc123\",\n    access_token=\"dapi1234567890\",\n    catalog=\"main\",\n    schema=\"default\",\n    table=\"orders\",\n)\n</code></pre> <pre><code>import os\nfrom duckguard import connect\n\n# Reads DATABRICKS_HOST, DATABRICKS_HTTP_PATH, DATABRICKS_TOKEN\ndg = connect(\"databricks://\", table=\"main.default.orders\")\n</code></pre>"},{"location":"platforms/databricks/#connection-requirements","title":"Connection Requirements","text":"Parameter Environment Variable Example <code>server_hostname</code> <code>DATABRICKS_HOST</code> <code>my-workspace.cloud.databricks.com</code> <code>http_path</code> <code>DATABRICKS_HTTP_PATH</code> <code>/sql/1.0/warehouses/abc123</code> <code>access_token</code> <code>DATABRICKS_TOKEN</code> <code>dapi...</code> <p>Where to find these</p> <ol> <li>server_hostname \u2014 Your workspace URL (without <code>https://</code>)</li> <li>http_path \u2014 SQL Warehouse \u2192 Connection Details \u2192 HTTP Path</li> <li>access_token \u2014 Settings \u2192 Developer \u2192 Access Tokens \u2192 Generate</li> </ol>"},{"location":"platforms/databricks/#databricks-notebooks","title":"Databricks Notebooks","text":"<p>Use DuckGuard directly in notebook cells:</p>"},{"location":"platforms/databricks/#cell-1-install","title":"Cell 1: Install","text":"<pre><code>%pip install duckguard[databricks]\ndbutils.library.restartPython()\n</code></pre>"},{"location":"platforms/databricks/#cell-2-connect-and-validate","title":"Cell 2: Connect and Validate","text":"<pre><code>from duckguard import connect\n\n# In a Databricks notebook, DuckGuard auto-detects the environment\n# and uses the notebook's authentication context\ndg = connect(\"databricks://auto\", table=\"main.default.orders\")\n\nresult = dg.expect({\n    \"order_id\": {\"not_null\": True, \"unique\": True},\n    \"amount\": {\"min\": 0},\n    \"status\": {\"in\": [\"pending\", \"shipped\", \"delivered\"]},\n})\n\nresult.show()  # Rich display in notebook output\n</code></pre>"},{"location":"platforms/databricks/#cell-3-profile","title":"Cell 3: Profile","text":"<pre><code>profile = dg.profile()\nprofile.show()  # Interactive profiling widget\n</code></pre> <p>Notebook auto-detection</p> <p>When running inside a Databricks notebook, <code>connect(\"databricks://auto\")</code> uses the notebook's built-in authentication. No tokens or hostnames needed.</p>"},{"location":"platforms/databricks/#delta-lake-support","title":"Delta Lake Support","text":"<p>Connect directly to Delta tables on storage:</p> <pre><code>from duckguard import connect\n\n# Read Delta table from cloud storage \u2014 no Databricks cluster needed\ndg = connect(\"delta://s3://my-bucket/delta/orders\")\nresult = dg.validate()\n</code></pre> S3ADLSGCSLocal <pre><code>dg = connect(\"delta://s3://my-bucket/warehouse/orders\")\n</code></pre> <pre><code>dg = connect(\"delta://abfss://container@account.dfs.core.windows.net/orders\")\n</code></pre> <pre><code>dg = connect(\"delta://gs://my-bucket/warehouse/orders\")\n</code></pre> <pre><code>dg = connect(\"delta:///tmp/delta/orders\")\n</code></pre> <p>Delta direct access</p> <p>Reading Delta tables directly from storage bypasses Databricks access controls. Use the <code>databricks://</code> connector for Unity Catalog-governed access.</p>"},{"location":"platforms/databricks/#unity-catalog-integration","title":"Unity Catalog Integration","text":""},{"location":"platforms/databricks/#three-level-namespace","title":"Three-Level Namespace","text":"<p>DuckGuard uses Unity Catalog's <code>catalog.schema.table</code> convention:</p> <pre><code>dg = connect(\"databricks://workspace.databricks.com\", table=\"main.sales.orders\")\n#                                                             ^^^^ ^^^^^ ^^^^^^\n#                                                          catalog schema table\n</code></pre>"},{"location":"platforms/databricks/#validate-across-catalogs","title":"Validate Across Catalogs","text":"<pre><code>from duckguard import connect\n\nconn = connect(\"databricks://workspace.databricks.com\")\n\n# Validate production\nprod = conn.table(\"prod.sales.orders\")\nprod_result = prod.expect({\"order_id\": {\"not_null\": True, \"unique\": True}})\n\n# Validate staging with same rules\nstaging = conn.table(\"staging.sales.orders\")\nstaging_result = staging.expect({\"order_id\": {\"not_null\": True, \"unique\": True}})\n\n# Compare\nprint(f\"Prod: {prod_result.stats['rows_checked']} rows, passed={prod_result.passed}\")\nprint(f\"Staging: {staging_result.stats['rows_checked']} rows, passed={staging_result.passed}\")\n</code></pre>"},{"location":"platforms/databricks/#information-schema-checks","title":"Information Schema Checks","text":"<p>Validate table metadata via Unity Catalog:</p> <pre><code>dg = connect(\"databricks://workspace.databricks.com\", table=\"prod.sales.orders\")\n\n# Check table exists and has expected columns\nmeta = dg.inspect()\nassert \"order_id\" in meta.columns\nassert meta.columns[\"order_id\"].type == \"BIGINT\"\nassert meta.row_count &gt; 0\n</code></pre>"},{"location":"platforms/databricks/#workflows","title":"Workflows","text":""},{"location":"platforms/databricks/#ci-pipeline-no-cluster-required","title":"CI Pipeline \u2014 No Cluster Required","text":"<pre><code># .github/workflows/data-quality.yml\nname: Data Quality\non:\n  schedule:\n    - cron: '0 6 * * *'  # Daily at 6 AM UTC\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip install duckguard[databricks]\n      - run: pytest tests/data_quality/ -v\n        env:\n          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}\n          DATABRICKS_HTTP_PATH: ${{ secrets.DATABRICKS_HTTP_PATH }}\n          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}\n</code></pre> <p>The CI runner is a basic Ubuntu VM. No Spark. No JVM. No cluster startup. Tests run in seconds.</p>"},{"location":"platforms/databricks/#after-dbt-runs-on-databricks","title":"After dbt Runs on Databricks","text":"<pre><code># tests/test_databricks_output.py\nfrom duckguard import connect\n\ndef test_fact_orders():\n    dg = connect(\"databricks://\", table=\"prod.analytics.fct_orders\")\n    result = dg.expect({\n        \"order_id\": {\"not_null\": True, \"unique\": True},\n        \"customer_id\": {\"not_null\": True},\n        \"order_total\": {\"min\": 0},\n        \"order_date\": {\"not_null\": True},\n    })\n    assert result.passed, result.failures()\n\ndef test_dim_customers():\n    dg = connect(\"databricks://\", table=\"prod.analytics.dim_customers\")\n    result = dg.expect({\n        \"customer_id\": {\"not_null\": True, \"unique\": True},\n        \"email\": {\"not_null\": True, \"pattern\": r\".+@.+\\..+\"},\n        \"created_at\": {\"not_null\": True},\n    })\n    assert result.passed, result.failures()\n</code></pre>"},{"location":"platforms/databricks/#databricks-workflow-job","title":"Databricks Workflow Job","text":"<pre><code># validate_job.py \u2014 run as a Databricks Workflow task (Python script)\nfrom duckguard import connect\n\ndg = connect(\"databricks://auto\", table=\"prod.sales.orders\")\n\nresult = dg.expect({\n    \"order_id\": {\"not_null\": True, \"unique\": True},\n    \"amount\": {\"min\": 0, \"max\": 1000000},\n})\n\nif not result.passed:\n    raise Exception(f\"Validation failed:\\n{result.failures()}\")\n\nprint(f\"\u2713 Validated {result.stats['rows_checked']} rows\")\n</code></pre> <p>Add this as a task in your Databricks Workflow, chained after your ETL task. Uses the job cluster's auth context automatically.</p>"},{"location":"platforms/databricks/#migration-from-great-expectations-on-databricks","title":"Migration from Great Expectations on Databricks","text":""},{"location":"platforms/databricks/#before-ge-spark","title":"Before: GE + Spark","text":"<pre><code># Requires: running Spark cluster, GE config, YAML files\nimport great_expectations as gx\nfrom great_expectations.datasource.fluent import SparkDatasource\n\ncontext = gx.get_context()\n\ndatasource = context.sources.add_spark(\"my_spark\")\nasset = datasource.add_dataframe_asset(\"orders\")\n\ndf = spark.table(\"prod.sales.orders\")\nbatch = asset.build_batch_request(dataframe=df)\n\ncontext.add_or_update_expectation_suite(\"orders_suite\")\nvalidator = context.get_validator(\n    batch_request=batch,\n    expectation_suite_name=\"orders_suite\",\n)\nvalidator.expect_column_values_to_not_be_null(\"order_id\")\nvalidator.expect_column_values_to_be_unique(\"order_id\")\nvalidator.expect_column_values_to_be_between(\"amount\", min_value=0)\nvalidator.save_expectation_suite()\n\ncheckpoint = context.add_or_update_checkpoint(\n    name=\"orders_cp\",\n    validations=[{\n        \"batch_request\": batch,\n        \"expectation_suite_name\": \"orders_suite\",\n    }],\n)\nresult = checkpoint.run()\n</code></pre> <p>Requirements: Spark cluster running, <code>great_expectations/</code> directory with YAML configs, checkpoint YAML.</p>"},{"location":"platforms/databricks/#after-duckguard","title":"After: DuckGuard","text":"<pre><code>from duckguard import connect\n\ndg = connect(\"databricks://workspace.databricks.com\", table=\"prod.sales.orders\")\nresult = dg.expect({\n    \"order_id\": {\"not_null\": True, \"unique\": True},\n    \"amount\": {\"min\": 0},\n})\nassert result.passed\n</code></pre> <p>Requirements: <code>pip install duckguard[databricks]</code>. That's it.</p>"},{"location":"platforms/databricks/#migration-steps","title":"Migration Steps","text":"<ol> <li><code>pip install duckguard[databricks]</code></li> <li>Replace GE datasource config \u2192 one <code>connect()</code> call</li> <li>Convert <code>expect_column_*</code> calls \u2192 dict format</li> <li>Replace checkpoints \u2192 <code>pytest</code></li> <li>Delete <code>great_expectations/</code> directory and all YAML</li> <li>Stop paying for a Spark cluster to run validation</li> </ol> <p>What you save</p> <ul> <li>No Spark cluster for validation \u2014 use SQL warehouse or direct Delta access</li> <li>No YAML \u2014 expectations live in Python code</li> <li>No GE context \u2014 no config directory, no checkpoint files</li> <li>Faster CI \u2014 no JVM startup, no cluster provisioning</li> </ul>"},{"location":"platforms/databricks/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Your Code       \u2502\n\u2502  (laptop / CI)   \u2502\n\u2502                  \u2502\n\u2502  duckguard       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 SQL over HTTPS\n         \u2502 (Databricks SQL connector)\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Databricks      \u2502\n\u2502  SQL Warehouse   \u2502\n\u2502  (serverless)    \u2502\n\u2502                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Unity      \u2502  \u2502\n\u2502  \u2502 Catalog    \u2502  \u2502\n\u2502  \u2502            \u2502  \u2502\n\u2502  \u2502 Delta Lake \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>DuckGuard sends aggregation queries to a SQL warehouse. No data leaves Databricks. No Spark driver needed on your side. Authentication goes through Databricks tokens over HTTPS.</p>"},{"location":"platforms/fabric/","title":"DuckGuard for Microsoft Fabric","text":""},{"location":"platforms/fabric/#validate-your-fabric-data-in-3-lines-of-python","title":"Validate your Fabric data in 3 lines of Python","text":"<pre><code>from duckguard import connect\n\ndata = connect(\"fabric://workspace/lakehouse/Tables/orders\", token=\"...\")\nassert data.customer_id.is_not_null()\nassert data.total_amount.between(0, 10000)\n</code></pre> <p>No Spark cluster. No complex setup. Runs from a Fabric notebook, your laptop, or CI.</p>"},{"location":"platforms/fabric/#the-problem","title":"The Problem","text":"<p>Data quality in Microsoft Fabric today means:</p> <ul> <li>PySpark checks \u2014 spin up a cluster, write SQL with extra steps, no reusable framework</li> <li>Great Expectations \u2014 50+ lines of config before your first check, needs a running compute</li> <li>Manual SQL \u2014 write T-SQL queries against the SQL endpoint, no automation, no scoring</li> </ul> <p>DuckGuard connects directly to your Lakehouse or Warehouse. One API, 3 lines, any table.</p>"},{"location":"platforms/fabric/#quick-start","title":"Quick Start","text":"<pre><code>pip install duckguard[fabric]\n</code></pre>"},{"location":"platforms/fabric/#option-1-onelake-direct-file-access","title":"Option 1: OneLake (Direct File Access)","text":"<p>Access Parquet and Delta tables in your Lakehouse without a SQL endpoint:</p> <pre><code>from duckguard import connect\n\n# Lakehouse table\ndata = connect(\n    \"fabric://my-workspace/my-lakehouse/Tables/orders\",\n    token=\"&lt;azure-ad-token&gt;\"\n)\n\n# Full OneLake path\ndata = connect(\n    \"onelake://my-workspace/my-lakehouse.Lakehouse/Files/raw/orders.parquet\",\n    token=\"&lt;azure-ad-token&gt;\"\n)\n\n# Validate\nassert data.customer_id.is_not_null()\nassert data.total_amount.between(0, 10000)\nscore = data.score()\nprint(f\"Grade: {score.grade}\")\n</code></pre>"},{"location":"platforms/fabric/#option-2-sql-endpoint","title":"Option 2: SQL Endpoint","text":"<p>Query via T-SQL \u2014 works with both Lakehouse and Warehouse:</p> <pre><code>data = connect(\n    \"fabric+sql://your-guid.datawarehouse.fabric.microsoft.com\",\n    table=\"orders\",\n    database=\"my_lakehouse\",\n    token=\"&lt;azure-ad-token&gt;\"\n)\n</code></pre>"},{"location":"platforms/fabric/#option-3-inside-a-fabric-notebook","title":"Option 3: Inside a Fabric Notebook","text":"<p>Load via Spark, validate via DuckGuard \u2014 no token needed:</p> <pre><code># Fabric notebook cell\n%pip install duckguard -q\n\nfrom duckguard import connect\n\n# Load from Lakehouse via Spark\ndf = spark.sql(\"SELECT * FROM my_lakehouse.orders\").toPandas()\ndata = connect(df)\n\n# Full quality analysis\nscore = data.score()\nprint(f\"Quality: {score.grade} ({score.overall:.1f}/100)\")\n</code></pre>"},{"location":"platforms/fabric/#authentication","title":"Authentication","text":""},{"location":"platforms/fabric/#in-a-fabric-notebook","title":"In a Fabric Notebook","text":"<pre><code># Automatic \u2014 use mssparkutils\ntoken = mssparkutils.credentials.getToken(\"pbi\")\ndata = connect(\"fabric://workspace/lakehouse/Tables/orders\", token=token)\n</code></pre>"},{"location":"platforms/fabric/#from-external-environments","title":"From External Environments","text":"<pre><code># Azure Identity (recommended)\nfrom azure.identity import DefaultAzureCredential\ncredential = DefaultAzureCredential()\ntoken = credential.get_token(\n    \"https://analysis.windows.net/powerbi/api/.default\"\n).token\n\ndata = connect(\"fabric://workspace/lakehouse/Tables/orders\", token=token)\n</code></pre> <p>Environment Variables</p> <p>Set <code>AZURE_CLIENT_ID</code>, <code>AZURE_TENANT_ID</code>, and <code>AZURE_CLIENT_SECRET</code> for service principal auth, or use managed identity in Azure-hosted environments.</p>"},{"location":"platforms/fabric/#fabric-pipeline-integration","title":"Fabric Pipeline Integration","text":""},{"location":"platforms/fabric/#notebook-activity-in-data-pipeline","title":"Notebook Activity in Data Pipeline","text":"<p>Add a notebook activity that runs quality checks after data loads:</p> <pre><code>from duckguard import connect, load_rules, execute_rules\n\n# Load from Lakehouse\ndf = spark.sql(\"SELECT * FROM my_lakehouse.orders\").toPandas()\ndata = connect(df)\n\n# Validate\nrules = load_rules(\"/lakehouse/default/Files/duckguard.yaml\")\nresult = execute_rules(rules, data)\n\nif not result.passed:\n    # Fail the pipeline\n    raise Exception(f\"Quality check failed: {result.failed_count} failures\")\n\n# Log results\nprint(f\"Quality: {data.score().grade}\")\nprint(f\"Checks: {result.passed_count}/{result.total_checks} passed\")\n</code></pre>"},{"location":"platforms/fabric/#auto-generate-rules","title":"Auto-Generate Rules","text":"<p>Let DuckGuard analyze your table and generate validation rules:</p> <pre><code>from duckguard import connect, generate_rules\n\ndata = connect(df)\nyaml_rules = generate_rules(data, dataset_name=\"orders\")\n\n# Save to Lakehouse Files\nwith open(\"/lakehouse/default/Files/duckguard.yaml\", \"w\") as f:\n    f.write(yaml_rules)\n</code></pre>"},{"location":"platforms/fabric/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"platforms/fabric/#pytest-with-fabric-sql-endpoint","title":"pytest with Fabric SQL Endpoint","text":"<pre><code># tests/test_fabric_quality.py\nimport os\nfrom duckguard import connect\n\ndef test_orders_quality():\n    orders = connect(\n        \"fabric+sql://workspace.datawarehouse.fabric.microsoft.com\",\n        table=\"orders\",\n        database=\"my_lakehouse\",\n        token=os.environ[\"FABRIC_TOKEN\"],\n    )\n    assert orders.row_count &gt; 0\n    assert orders.order_id.is_not_null()\n    assert orders.order_id.is_unique()\n    assert orders.total_amount.between(0, 50000)\n</code></pre>"},{"location":"platforms/fabric/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Fabric Data Quality\non:\n  schedule:\n    - cron: '0 8 * * *'  # Daily at 8 AM\n\njobs:\n  quality-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with: { python-version: \"3.11\" }\n      - run: pip install duckguard[fabric]\n      - run: pytest tests/test_fabric_quality.py\n        env:\n          FABRIC_TOKEN: ${{ secrets.FABRIC_TOKEN }}\n</code></pre>"},{"location":"platforms/fabric/#what-duckguard-gives-you-on-fabric","title":"What DuckGuard Gives You on Fabric","text":"Feature Raw SQL PySpark DuckGuard Lines of code 10+ per check 10+ per check 3 Quality scoring Manual Manual Built-in (A-F) PII detection Manual Manual Automatic Anomaly detection Manual Manual 7 methods Data contracts No No Yes Schema tracking No No Yes Drift detection No No Yes Needs Spark cluster No Yes No pytest integration No No Yes"},{"location":"platforms/fabric/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd3 Fabric Quickstart Notebook</li> <li>\ud83d\udcda Full Documentation</li> <li>\ud83d\udd0c All Connectors</li> <li>\ud83e\udd16 AI Features</li> </ul>"},{"location":"platforms/kaggle/","title":"DuckGuard on Kaggle &amp; Colab","text":""},{"location":"platforms/kaggle/#profile-any-dataset-in-30-seconds","title":"Profile any dataset in 30 seconds","text":"<pre><code>!pip install duckguard -q\nfrom duckguard import profile\nprofile(\"orders.csv\").show()\n</code></pre> <p>Three lines. Full data profile. Before you write a single model.</p>"},{"location":"platforms/kaggle/#why-this-matters","title":"Why This Matters","text":"<p>Data quality is the #1 reason ML models fail in production.</p> <p>Not architecture. Not hyperparameters. Bad data.</p> <ul> <li>Null values your model silently treats as zero</li> <li>Duplicate rows inflating your training set</li> <li>PII leaking into features</li> <li>Outliers skewing distributions</li> <li>Categories that don't match between train and test</li> </ul> <p>You find these problems after 4 hours of training. Or after deploying to production. Or you find them now, in 30 seconds, before you start.</p>"},{"location":"platforms/kaggle/#quick-start","title":"Quick Start","text":""},{"location":"platforms/kaggle/#kaggle-notebook","title":"Kaggle Notebook","text":"<pre><code># Cell 1 \u2014 Install\n!pip install duckguard -q\n</code></pre> <pre><code># Cell 2 \u2014 Load and Profile\nimport pandas as pd\nfrom duckguard import connect\n\ndf = pd.read_csv(\"/kaggle/input/ecommerce-data/orders.csv\")\ndg = connect(df)\n\nprofile = dg.profile()\nprofile.show()\n</code></pre>"},{"location":"platforms/kaggle/#google-colab","title":"Google Colab","text":"<pre><code># Cell 1 \u2014 Install\n!pip install duckguard -q\n</code></pre> <pre><code># Cell 2 \u2014 Load and Profile\nimport pandas as pd\nfrom duckguard import connect\n\ndf = pd.read_csv(\"orders.csv\")\ndg = connect(df)\n\nprofile = dg.profile()\nprofile.show()\n</code></pre> <p>One-click Colab badge</p> <p>Add this to your notebook's README or description to let others run your quality checks instantly:</p> <pre><code>[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USER/YOUR_REPO/blob/main/quality_check.ipynb)\n</code></pre> <p>The badge links directly to your notebook in Colab \u2014 one click to reproduce your data quality analysis.</p>"},{"location":"platforms/kaggle/#the-full-workflow","title":"The Full Workflow","text":"<p>Load \u2192 Profile \u2192 Validate \u2192 Fix \u2192 Model</p>"},{"location":"platforms/kaggle/#1-load-your-dataset","title":"1. Load Your Dataset","text":"<pre><code>import pandas as pd\nfrom duckguard import connect\n\n# From Kaggle dataset\ndf = pd.read_csv(\"/kaggle/input/ecommerce-data/orders.csv\")\ndg = connect(df)\n</code></pre>"},{"location":"platforms/kaggle/#2-profile-understand-what-you-have","title":"2. Profile \u2014 Understand What You Have","text":"<pre><code>profile = dg.profile()\nprofile.show()\n</code></pre> <p>Output:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 DuckGuard Profile: orders.csv                   \u2502\n\u2502 Rows: 51,243  |  Columns: 8  |  Size: 4.2 MB   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Column       \u2502 Type     \u2502 Nulls \u2502 Unique\u2502 Issues\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 order_id     \u2502 int64    \u2502 0%    \u2502 100%  \u2502       \u2502\n\u2502 customer_id  \u2502 int64    \u2502 0%    \u2502 42%   \u2502       \u2502\n\u2502 order_date   \u2502 object   \u2502 0.2%  \u2502 38%   \u2502 \u26a0     \u2502\n\u2502 amount       \u2502 float64  \u2502 1.1%  \u2502 89%   \u2502 \u26a0     \u2502\n\u2502 quantity     \u2502 int64    \u2502 0%    \u2502 0.4%  \u2502       \u2502\n\u2502 status       \u2502 object   \u2502 0%    \u2502 0.01% \u2502       \u2502\n\u2502 email        \u2502 object   \u2502 3.4%  \u2502 41%   \u2502 \ud83d\udd12 PII\u2502\n\u2502 ship_country \u2502 object   \u2502 0.8%  \u2502 0.5%  \u2502       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\nQuality Score: 72/100\nIssues Found: 4\n  \u26a0 order_date: 102 null values, inconsistent date formats detected\n  \u26a0 amount: 563 null values, 3 extreme outliers (&gt;$50,000)\n  \ud83d\udd12 email: Contains PII (email addresses)\n  \u2139 quantity: Low cardinality (12 unique values)\n</code></pre>"},{"location":"platforms/kaggle/#3-validate-set-expectations","title":"3. Validate \u2014 Set Expectations","text":"<pre><code>result = dg.expect({\n    \"order_id\": {\"not_null\": True, \"unique\": True},\n    \"customer_id\": {\"not_null\": True},\n    \"amount\": {\"not_null\": True, \"min\": 0, \"max\": 10000},\n    \"quantity\": {\"not_null\": True, \"min\": 1},\n    \"status\": {\"in\": [\"pending\", \"processing\", \"shipped\", \"delivered\", \"cancelled\"]},\n})\n\nresult.show()\n</code></pre> <p>Output:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 DuckGuard Validation Results                    \u2502\n\u2502 5 checks  |  3 passed  |  2 failed             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Check                    \u2502 Status   \u2502 Details   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 order_id not null        \u2502 \u2713 PASS   \u2502           \u2502\n\u2502 order_id unique          \u2502 \u2713 PASS   \u2502           \u2502\n\u2502 customer_id not null     \u2502 \u2713 PASS   \u2502           \u2502\n\u2502 amount not null          \u2502 \u2717 FAIL   \u2502 563 nulls \u2502\n\u2502 amount max \u2264 10000       \u2502 \u2717 FAIL   \u2502 max=87431 \u2502\n\u2502 quantity not null        \u2502 \u2713 PASS   \u2502           \u2502\n\u2502 quantity min \u2265 1         \u2502 \u2713 PASS   \u2502           \u2502\n\u2502 status in set            \u2502 \u2713 PASS   \u2502           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"platforms/kaggle/#4-find-issues-drill-down","title":"4. Find Issues \u2014 Drill Down","text":"<pre><code># Get the rows that failed\nbad_amounts = dg.failures(\"amount\")\nprint(f\"Null amounts: {len(bad_amounts[bad_amounts['amount'].isna()])}\")\nprint(f\"Outliers: {len(bad_amounts[bad_amounts['amount'] &gt; 10000])}\")\n\n# See the actual outlier values\nprint(bad_amounts[bad_amounts[\"amount\"] &gt; 10000][[\"order_id\", \"amount\", \"order_date\"]])\n</code></pre>"},{"location":"platforms/kaggle/#5-fix-before-modeling","title":"5. Fix Before Modeling","text":"<pre><code># Drop nulls in amount\ndf = df.dropna(subset=[\"amount\"])\n\n# Cap outliers\ndf.loc[df[\"amount\"] &gt; 10000, \"amount\"] = 10000\n\n# Re-validate\ndg = connect(df)\nresult = dg.expect({\n    \"amount\": {\"not_null\": True, \"min\": 0, \"max\": 10000},\n})\nassert result.passed  # \u2713 Now passes\n</code></pre>"},{"location":"platforms/kaggle/#key-features-for-notebooks","title":"Key Features for Notebooks","text":""},{"location":"platforms/kaggle/#quality-score","title":"Quality Score","text":"<p>Every profile generates a 0-100 quality score:</p> <pre><code>profile = dg.profile()\nprint(f\"Quality Score: {profile.score}/100\")\n</code></pre> Score Meaning 90-100 Clean data. Minor issues at most. 70-89 Usable, but check nulls and outliers. 50-69 Significant issues. Clean before modeling. 0-49 Major problems. Investigate data source."},{"location":"platforms/kaggle/#pii-detection","title":"PII Detection","text":"<p>DuckGuard automatically flags columns that look like personal data:</p> <pre><code>profile = dg.profile()\npii = profile.pii_columns()\nprint(pii)\n# ['email', 'phone', 'ip_address']\n</code></pre> <p>PII in competition data</p> <p>If you find PII in a Kaggle dataset, consider:</p> <ul> <li>Don't include PII as features \u2014 it won't generalize</li> <li>Hash or drop PII columns before training</li> <li>Report to dataset owner if PII shouldn't be public</li> </ul>"},{"location":"platforms/kaggle/#anomaly-detection","title":"Anomaly Detection","text":"<p>Spot statistical anomalies without manual investigation:</p> <pre><code>anomalies = dg.detect_anomalies()\nanomalies.show()\n</code></pre> <pre><code>Anomalies Detected:\n  amount: 3 values &gt; 5\u03c3 from mean (likely data entry errors)\n  order_date: 17 dates in the future (data collection issue)\n  quantity: 1 negative value (should be \u2265 1)\n</code></pre>"},{"location":"platforms/kaggle/#competition-notebook-pattern","title":"Competition Notebook Pattern","text":"<p>Add to your competition notebook in 3 lines:</p> <pre><code># Add this at the top of any competition notebook\n!pip install duckguard -q\nfrom duckguard import connect\nconnect(train_df).profile().show()\n</code></pre>"},{"location":"platforms/kaggle/#full-competition-template","title":"Full Competition Template","text":"<pre><code># \u2500\u2500 Cell 1: Setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n!pip install duckguard -q\n\nimport pandas as pd\nfrom duckguard import connect\n\n# \u2500\u2500 Cell 2: Load \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain = pd.read_csv(\"/kaggle/input/competition/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/competition/test.csv\")\n\n# \u2500\u2500 Cell 3: Profile Training Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntrain_dg = connect(train)\ntrain_profile = train_dg.profile()\ntrain_profile.show()\n\n# \u2500\u2500 Cell 4: Profile Test Data \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ntest_dg = connect(test)\ntest_profile = test_dg.profile()\ntest_profile.show()\n\n# \u2500\u2500 Cell 5: Compare Train vs Test \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrom duckguard import compare\ndiff = compare(train, test)\ndiff.show()\n# Shows: distribution shifts, missing columns, type mismatches\n\n# \u2500\u2500 Cell 6: Validate &amp; Clean \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nresult = train_dg.expect({\n    \"target\": {\"not_null\": True},\n    \"feature_1\": {\"not_null\": True, \"min\": 0},\n    # ... add checks per column\n})\n\nif not result.passed:\n    print(\"Issues found \u2014 fix before training:\")\n    print(result.failures())\n\n# \u2500\u2500 Cell 7: Your model code goes here... \u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"platforms/kaggle/#working-with-different-file-formats","title":"Working with Different File Formats","text":"<p>DuckGuard handles whatever Kaggle throws at you:</p> CSVParquetJSONExcelMultiple Files <pre><code>df = pd.read_csv(\"/kaggle/input/data/file.csv\")\ndg = connect(df)\n</code></pre> <pre><code># Direct \u2014 no pandas needed\ndg = connect(\"/kaggle/input/data/file.parquet\")\n</code></pre> <pre><code>df = pd.read_json(\"/kaggle/input/data/file.json\")\ndg = connect(df)\n</code></pre> <pre><code>df = pd.read_excel(\"/kaggle/input/data/file.xlsx\")\ndg = connect(df)\n</code></pre> <pre><code># Profile all CSVs in a directory\ndg = connect(\"/kaggle/input/data/*.csv\")\ndg.profile().show()\n</code></pre>"},{"location":"platforms/kaggle/#example-e-commerce-dataset-analysis","title":"Example: E-Commerce Dataset Analysis","text":"<p>Full walkthrough with a realistic orders dataset:</p> <pre><code>!pip install duckguard -q\n\nimport pandas as pd\nfrom duckguard import connect\n\n# Load\ndf = pd.read_csv(\"/kaggle/input/ecommerce/orders.csv\")\nprint(f\"Shape: {df.shape}\")\n\n# Profile\ndg = connect(df)\nprofile = dg.profile()\nprofile.show()\n\n# Check for modeling readiness\nresult = dg.expect({\n    \"order_id\": {\"not_null\": True, \"unique\": True},\n    \"customer_id\": {\"not_null\": True},\n    \"product_id\": {\"not_null\": True},\n    \"amount\": {\"not_null\": True, \"min\": 0},\n    \"quantity\": {\"not_null\": True, \"min\": 1},\n    \"order_date\": {\"not_null\": True},\n})\n\nprint(f\"\\nQuality Score: {profile.score}/100\")\nprint(f\"Checks Passed: {result.stats['passed']}/{result.stats['total']}\")\n\n# Detailed column stats\nfor col in profile.columns:\n    c = profile.columns[col]\n    print(f\"\\n{col}:\")\n    print(f\"  Type: {c.type}, Nulls: {c.null_pct}%, Unique: {c.unique_pct}%\")\n    if c.is_numeric:\n        print(f\"  Range: [{c.min}, {c.max}], Mean: {c.mean:.2f}, Std: {c.std:.2f}\")\n\n# Fix issues\nif not result.passed:\n    # Drop rows with null amounts\n    df = df.dropna(subset=[\"amount\", \"quantity\"])\n    # Remove impossible values\n    df = df[df[\"amount\"] &gt;= 0]\n    df = df[df[\"quantity\"] &gt;= 1]\n\n    # Verify fix\n    dg = connect(df)\n    assert dg.expect({\n        \"amount\": {\"not_null\": True, \"min\": 0},\n        \"quantity\": {\"not_null\": True, \"min\": 1},\n    }).passed\n\n    print(f\"\\n\u2713 Cleaned: {len(df)} rows ready for modeling\")\n</code></pre>"},{"location":"platforms/kaggle/#tips","title":"Tips","text":"<p>Profile before you model</p> <p>Every minute spent on data quality saves an hour of debugging model performance. Profile first. Always.</p> <p>Compare train and test</p> <p>Distribution shift between train and test is the silent killer. Use <code>compare()</code> to catch it before your leaderboard score tanks.</p> <p>Save your quality checks</p> <p>Export expectations so teammates can reproduce your cleaning steps:</p> <pre><code>result = dg.expect({...})\nresult.save(\"quality_checks.json\")\n\n# Teammate loads and re-runs\nfrom duckguard import load_expectations\nresult = dg.expect(load_expectations(\"quality_checks.json\"))\n</code></pre> <p>Kaggle kernel resources</p> <p>DuckGuard uses DuckDB under the hood. It's fast and memory-efficient. Profiling a 1M-row DataFrame takes ~2 seconds and ~50MB of RAM on a standard Kaggle kernel.</p>"},{"location":"platforms/snowflake/","title":"DuckGuard for Snowflake","text":""},{"location":"platforms/snowflake/#validate-your-snowflake-data-in-3-lines-of-python","title":"Validate your Snowflake data in 3 lines of Python","text":"<pre><code>from duckguard import connect\n\ndg = connect(\"snowflake://account/db\", table=\"orders\")\ndg.validate()\n</code></pre> <p>That's it. No YAML. No context objects. No 200-line config files.</p>"},{"location":"platforms/snowflake/#the-problem","title":"The Problem","text":"<p>You're running dbt models in Snowflake. Data lands in tables. You need to know if it's correct.</p> <p>With Great Expectations, that looks like this:</p> <pre><code># Great Expectations \u2014 ~50 lines to validate one table\nimport great_expectations as gx\n\ncontext = gx.get_context()\n\ndatasource = context.sources.add_snowflake(\n    name=\"my_snowflake\",\n    connection_string=\"snowflake://user:pass@account/database/schema?warehouse=WH&amp;role=ROLE\",\n)\nasset = datasource.add_table_asset(name=\"orders\", table_name=\"orders\")\nbatch_request = asset.build_batch_request()\n\ncontext.add_or_update_expectation_suite(\"orders_suite\")\nvalidator = context.get_validator(\n    batch_request=batch_request,\n    expectation_suite_name=\"orders_suite\",\n)\n\nvalidator.expect_column_to_exist(\"order_id\")\nvalidator.expect_column_values_to_not_be_null(\"order_id\")\nvalidator.expect_column_values_to_be_between(\"amount\", min_value=0)\nvalidator.expect_column_values_to_be_in_set(\"status\", [\"pending\", \"shipped\", \"delivered\"])\n\nvalidator.save_expectation_suite(discard_failed_expectations=False)\n\ncheckpoint = context.add_or_update_checkpoint(\n    name=\"orders_checkpoint\",\n    validations=[{\n        \"batch_request\": batch_request,\n        \"expectation_suite_name\": \"orders_suite\",\n    }],\n)\nresult = checkpoint.run()\n</code></pre> <p>With DuckGuard:</p> <pre><code>from duckguard import connect\n\ndg = connect(\"snowflake://user:pass@account/database/schema\", table=\"orders\")\nresult = dg.expect({\n    \"order_id\": {\"not_null\": True, \"unique\": True},\n    \"amount\": {\"min\": 0},\n    \"status\": {\"in\": [\"pending\", \"shipped\", \"delivered\"]},\n})\n</code></pre> <p>Same checks. Fraction of the code.</p>"},{"location":"platforms/snowflake/#quick-start","title":"Quick Start","text":""},{"location":"platforms/snowflake/#install","title":"Install","text":"<pre><code>pip install duckguard[snowflake]\n</code></pre>"},{"location":"platforms/snowflake/#connect","title":"Connect","text":"Connection StringExplicit ParametersEnvironment Variables <pre><code>from duckguard import connect\n\ndg = connect(\"snowflake://user:pass@account/database/schema\", table=\"orders\")\n</code></pre> <pre><code>from duckguard import connect\n\ndg = connect(\n    \"snowflake://myaccount.us-east-1\",\n    database=\"analytics\",\n    schema=\"public\",\n    warehouse=\"COMPUTE_WH\",\n    role=\"DATA_ENGINEER\",\n    table=\"orders\",\n)\n</code></pre> <pre><code>import os\nfrom duckguard import connect\n\n# DuckGuard reads SNOWFLAKE_USER, SNOWFLAKE_PASSWORD, SNOWFLAKE_ACCOUNT\ndg = connect(\"snowflake://\", table=\"orders\")\n</code></pre>"},{"location":"platforms/snowflake/#connection-string-format","title":"Connection String Format","text":"<pre><code>snowflake://user:pass@account/database/schema?warehouse=WH&amp;role=ROLE\n</code></pre> Component Example Required <code>user</code> <code>data_engineer</code> Yes <code>pass</code> <code>s3cret</code> Yes <code>account</code> <code>xy12345.us-east-1</code> Yes <code>database</code> <code>analytics</code> Yes <code>schema</code> <code>public</code> No <code>warehouse</code> <code>COMPUTE_WH</code> (query param) No <code>role</code> <code>DATA_ENGINEER</code> (query param) No"},{"location":"platforms/snowflake/#query-pushdown","title":"Query Pushdown","text":"<p>DuckGuard doesn't pull your data out of Snowflake. It pushes computation into Snowflake.</p> <p>When you run:</p> <pre><code>dg.expect({\"amount\": {\"min\": 0, \"max\": 10000}})\n</code></pre> <p>DuckGuard generates and executes:</p> <pre><code>SELECT\n    MIN(amount) AS amount_min,\n    MAX(amount) AS amount_max,\n    COUNT(*) FILTER (WHERE amount IS NULL) AS amount_null_count,\n    COUNT(*) AS total_rows\nFROM orders\n</code></pre> <p>One query. One round trip. No data leaves Snowflake.</p> <p>What gets pushed down</p> <ul> <li>Null checks \u2192 <code>COUNT(*) FILTER (WHERE col IS NULL)</code></li> <li>Min/max/range \u2192 <code>MIN()</code>, <code>MAX()</code></li> <li>Uniqueness \u2192 <code>COUNT(DISTINCT col) = COUNT(col)</code></li> <li>Value sets \u2192 <code>COUNT(*) FILTER (WHERE col NOT IN (...))</code></li> <li>Pattern matching \u2192 <code>COUNT(*) FILTER (WHERE col NOT RLIKE ...)</code></li> <li>Row counts \u2192 <code>COUNT(*)</code></li> </ul> <p>DuckGuard batches all checks for a table into a single query when possible.</p> <p>Warehouse sizing</p> <p>Validation queries are lightweight aggregations. An <code>XS</code> warehouse handles most workloads. Don't spin up <code>XLARGE</code> for DuckGuard \u2014 you're wasting credits.</p>"},{"location":"platforms/snowflake/#key-workflows","title":"Key Workflows","text":""},{"location":"platforms/snowflake/#after-dbt-runs","title":"After dbt Runs","text":"<p>Add DuckGuard as a post-hook or a downstream test:</p> <pre><code># tests/test_dbt_output.py\nfrom duckguard import connect\n\ndef test_orders_output():\n    dg = connect(\"snowflake://account/analytics\", table=\"fct_orders\")\n    result = dg.expect({\n        \"order_id\": {\"not_null\": True, \"unique\": True},\n        \"customer_id\": {\"not_null\": True},\n        \"amount\": {\"min\": 0},\n        \"order_date\": {\"not_null\": True, \"max\": \"today\"},\n    })\n    assert result.passed\n</code></pre> <p>Run with pytest after <code>dbt run</code>:</p> <pre><code>dbt run --select fct_orders\npytest tests/test_dbt_output.py -v\n</code></pre>"},{"location":"platforms/snowflake/#ci-pipeline","title":"CI Pipeline","text":"<pre><code># .github/workflows/data-quality.yml\nname: Data Quality\non:\n  workflow_run:\n    workflows: [\"dbt Run\"]\n    types: [completed]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - run: pip install duckguard[snowflake]\n      - run: pytest tests/data_quality/ -v\n        env:\n          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}\n          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}\n          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n</code></pre>"},{"location":"platforms/snowflake/#airflow-dag-task","title":"Airflow DAG Task","text":"<pre><code>from airflow.decorators import task\n\n@task\ndef validate_orders():\n    from duckguard import connect\n\n    dg = connect(\"snowflake://account/analytics\", table=\"fct_orders\")\n    result = dg.expect({\n        \"order_id\": {\"not_null\": True, \"unique\": True},\n        \"amount\": {\"min\": 0},\n    })\n    if not result.passed:\n        raise ValueError(f\"Data quality failed: {result.summary()}\")\n</code></pre> <pre><code># In your DAG\nrun_dbt &gt;&gt; validate_orders() &gt;&gt; notify_downstream\n</code></pre>"},{"location":"platforms/snowflake/#snowflake-feature-integration","title":"Snowflake Feature Integration","text":""},{"location":"platforms/snowflake/#stages","title":"Stages","text":"<p>Validate files in a Snowflake stage before loading:</p> <pre><code>dg = connect(\"snowflake://account/db\", stage=\"@my_stage/data/\", file_format=\"csv\")\nresult = dg.profile()\nprint(result.summary())\n</code></pre>"},{"location":"platforms/snowflake/#streams","title":"Streams","text":"<p>Validate only new/changed rows using Snowflake streams:</p> <pre><code>dg = connect(\"snowflake://account/db\", stream=\"orders_stream\")\nresult = dg.expect({\n    \"order_id\": {\"not_null\": True},\n    \"amount\": {\"min\": 0},\n})\n# Only validates rows captured by the stream \u2014 not the full table\n</code></pre>"},{"location":"platforms/snowflake/#tasks","title":"Tasks","text":"<p>Schedule DuckGuard validation as a Snowflake task using the Python connector:</p> <pre><code># Create a stored procedure that runs DuckGuard\nCREATE OR REPLACE PROCEDURE validate_orders()\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.9'\nPACKAGES = ('duckguard', 'snowflake-connector-python')\nHANDLER = 'run'\nAS\n$$\ndef run(session):\n    from duckguard import connect\n    dg = connect(session=session, table=\"orders\")  # Use existing session\n    result = dg.expect({\"order_id\": {\"not_null\": True}})\n    return result.summary()\n$$;\n\n-- Run every hour\nCREATE TASK validate_orders_task\n  WAREHOUSE = XS_WH\n  SCHEDULE = 'USING CRON 0 * * * * UTC'\nAS\n  CALL validate_orders();\n</code></pre>"},{"location":"platforms/snowflake/#real-world-example-pipeline-output-validation","title":"Real-World Example: Pipeline Output Validation","text":"<p>You have a daily pipeline that loads order data from an API into Snowflake via dbt. Here's the full validation pattern:</p> <pre><code># validate_pipeline.py\nfrom duckguard import connect\nfrom datetime import date, timedelta\n\ndef validate_daily_orders():\n    dg = connect(\n        \"snowflake://account/analytics/public\",\n        warehouse=\"COMPUTE_WH\",\n        table=\"fct_orders\",\n    )\n\n    today = date.today()\n    yesterday = today - timedelta(days=1)\n\n    # Profile first \u2014 understand what you're working with\n    profile = dg.profile(where=f\"order_date = '{yesterday}'\")\n    print(profile.summary())\n\n    # Validate structure\n    result = dg.expect({\n        \"order_id\": {\"not_null\": True, \"unique\": True},\n        \"customer_id\": {\"not_null\": True},\n        \"order_date\": {\"not_null\": True, \"equals\": str(yesterday)},\n        \"amount\": {\"not_null\": True, \"min\": 0, \"max\": 100000},\n        \"currency\": {\"in\": [\"USD\", \"EUR\", \"GBP\", \"CAD\"]},\n        \"status\": {\"in\": [\"pending\", \"processing\", \"shipped\", \"delivered\", \"cancelled\"]},\n        \"email\": {\"not_null\": True, \"pattern\": r\"^[^@]+@[^@]+\\.[^@]+$\"},\n    }, where=f\"order_date = '{yesterday}'\")\n\n    # Check row count is reasonable (not empty, not duplicated)\n    row_check = dg.expect_table({\n        \"row_count\": {\"min\": 100, \"max\": 1000000},\n    }, where=f\"order_date = '{yesterday}'\")\n\n    if not result.passed or not row_check.passed:\n        # Send alert, block downstream\n        raise ValueError(\n            f\"Pipeline validation failed for {yesterday}:\\n\"\n            f\"{result.failures()}\\n{row_check.failures()}\"\n        )\n\n    print(f\"\u2713 {yesterday}: {result.stats['rows_checked']} rows, all checks passed\")\n\nif __name__ == \"__main__\":\n    validate_daily_orders()\n</code></pre>"},{"location":"platforms/snowflake/#migration-from-great-expectations","title":"Migration from Great Expectations","text":""},{"location":"platforms/snowflake/#concepts-mapping","title":"Concepts Mapping","text":"Great Expectations DuckGuard Notes Data Context <code>connect()</code> No YAML config needed Datasource Connection string One line Data Asset <code>table</code> parameter Just a name Expectation Suite <code>expect()</code> dict Inline or file Validator Return value of <code>expect()</code> Automatic Checkpoint <code>pytest</code> Standard testing Data Docs <code>dg.report()</code> Built-in HTML report"},{"location":"platforms/snowflake/#side-by-side","title":"Side-by-Side","text":"Great ExpectationsDuckGuard <pre><code>import great_expectations as gx\n\ncontext = gx.get_context()\nds = context.sources.add_snowflake(\"sf\", connection_string=\"...\")\nasset = ds.add_table_asset(\"orders\", table_name=\"orders\")\nbatch = asset.build_batch_request()\n\ncontext.add_or_update_expectation_suite(\"orders_suite\")\nv = context.get_validator(batch_request=batch, expectation_suite_name=\"orders_suite\")\n\nv.expect_column_values_to_not_be_null(\"order_id\")\nv.expect_column_values_to_be_unique(\"order_id\")\nv.expect_column_values_to_be_between(\"amount\", min_value=0)\nv.save_expectation_suite()\n\ncp = context.add_or_update_checkpoint(\"cp\", validations=[{\n    \"batch_request\": batch,\n    \"expectation_suite_name\": \"orders_suite\",\n}])\nresult = cp.run()\n</code></pre> <pre><code>from duckguard import connect\n\ndg = connect(\"snowflake://account/db\", table=\"orders\")\nresult = dg.expect({\n    \"order_id\": {\"not_null\": True, \"unique\": True},\n    \"amount\": {\"min\": 0},\n})\n</code></pre>"},{"location":"platforms/snowflake/#migration-steps","title":"Migration Steps","text":"<ol> <li>Install: <code>pip install duckguard[snowflake]</code></li> <li>Replace connection setup with a single <code>connect()</code> call</li> <li>Convert expectations to DuckGuard's dict format (see mapping above)</li> <li>Replace checkpoints with <code>pytest</code> tests</li> <li>Replace Data Docs with <code>dg.report()</code> or CI output</li> <li>Delete <code>great_expectations/</code> directory, YAML files, and checkpoint configs</li> </ol> <p>Migration time</p> <p>Most teams migrate a full GE suite in under an hour. The hardest part is deleting all that YAML.</p>"}]}