{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶Ü Data Quality for Your ML Pipeline with DuckGuard\n",
    "\n",
    "**Bad data is the #1 reason ML models fail in production.** Before you train, validate.\n",
    "\n",
    "This notebook shows how to profile, validate, and fix data quality issues in under 30 seconds ‚Äî using [DuckGuard](https://github.com/XDataHubAI/duckguard), a pytest-like data quality library powered by DuckDB.\n",
    "\n",
    "**Works with:** CSV, Parquet, S3, Snowflake, Databricks, BigQuery, and 15+ sources.\n",
    "\n",
    "[![GitHub](https://img.shields.io/github/stars/XDataHubAI/duckguard?style=social)](https://github.com/XDataHubAI/duckguard)\n",
    "[![PyPI](https://img.shields.io/pypi/v/duckguard.svg)](https://pypi.org/project/duckguard/)\n",
    "[![Docs](https://img.shields.io/badge/docs-GitHub%20Pages-blue)](https://xdatahubai.github.io/duckguard/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q duckguard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data\n",
    "\n",
    "We'll create a realistic e-commerce dataset with intentional quality issues ‚Äî the kind you'd find in production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os\n",
    "\n",
    "ORDERS_CSV = \"\"\"order_id,customer_id,product_name,quantity,unit_price,subtotal,tax,shipping,total_amount,status,country,email,phone,created_at,ship_date\n",
    "ORD001,CUST001,Widget Pro,2,29.99,59.98,5.40,4.99,70.37,shipped,US,alice@example.com,555-0101,2024-01-15,2024-01-17\n",
    "ORD002,CUST002,Gadget Plus,1,49.99,49.99,4.50,0.00,54.49,delivered,US,bob@example.com,555-0102,2024-01-15,2024-01-18\n",
    "ORD003,,Widget Pro,-3,29.99,-89.97,-8.10,4.99,-93.08,pending,UK,charlie@example.com,+44-20-7946-0958,2024-01-16,\n",
    "ORD004,CUST004,Super Gizmo,1,199.99,199.99,18.00,0.00,217.99,shipped,US,,555-0104,2024-01-16,2024-01-19\n",
    "ORD005,CUST005,Widget Pro,500,29.99,14995.00,1349.55,4.99,16349.54,pending,CA,eve@example.com,555-0105,2024-01-17,\n",
    "ORD006,CUST006,Gadget Plus,2,49.99,99.98,9.00,4.99,113.97,INVALID,US,frank@example.com,555-0106,2024-01-17,2024-01-20\n",
    "ORD007,CUST007,Basic Widget,1,9.99,9.99,0.90,4.99,15.88,delivered,US,grace@example,555-0107,2024-01-18,2024-01-20\n",
    "ORD008,CUST008,Premium Bundle,3,99.99,299.97,27.00,0.00,326.97,shipped,DE,hans@example.de,+49-30-12345678,2024-01-18,2024-01-22\n",
    "ORD009,CUST009,Widget Pro,1,29.99,29.99,2.70,4.99,37.68,delivered,US,ivan@example.com,,2024-01-19,2024-01-21\n",
    "ORD010,CUST010,Super Gizmo,2,199.99,399.98,36.00,0.00,435.98,pending,JP,jun@example.jp,+81-3-1234-5678,2024-01-19,\n",
    "\"\"\"\n",
    "\n",
    "with open(\"orders.csv\", \"w\") as f:\n",
    "    f.write(ORDERS_CSV.strip())\n",
    "\n",
    "print(\"‚úÖ Created orders.csv with 10 rows (and intentional quality issues)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Connect & Profile\n",
    "\n",
    "DuckGuard auto-detects file types, column types, and semantic types (email, phone, PII, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckguard import connect, AutoProfiler, SemanticAnalyzer\n",
    "\n",
    "orders = connect(\"orders.csv\")\n",
    "print(f\"Rows: {orders.row_count}\")\n",
    "print(f\"Columns: {orders.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full auto-profile with quality scoring\n",
    "profiler = AutoProfiler()\n",
    "profile = profiler.profile(orders)\n",
    "\n",
    "print(f\"\\nüìä Quality Grade: {profile.overall_quality_grade} ({profile.overall_quality_score:.1f}/100)\\n\")\n",
    "print(f\"{'Column':<20} {'Type':<12} {'Nulls %':<10} {'Unique %':<10} {'Grade'}\")\n",
    "print(\"-\" * 62)\n",
    "for col in profile.columns:\n",
    "    print(f\"{col.name:<20} {col.dtype:<12} {col.null_percent:<10.1f} {col.unique_percent:<10.1f} {col.quality_grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PII Detection\n",
    "\n",
    "Before sharing data or training models, check for personally identifiable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = SemanticAnalyzer().analyze(orders)\n",
    "\n",
    "print(\"üîí PII Detection Results:\\n\")\n",
    "for col in analysis.columns:\n",
    "    if col.is_pii:\n",
    "        print(f\"  ‚ö†Ô∏è  {col.name}: {col.semantic_type.value} (confidence: {col.confidence:.0%})\")\n",
    "\n",
    "if analysis.pii_columns:\n",
    "    print(f\"\\n  Found PII in {len(analysis.pii_columns)} columns: {analysis.pii_columns}\")\n",
    "    print(\"  ‚Üí Consider masking these before training!\")\n",
    "else:\n",
    "    print(\"  No PII detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate ‚Äî The Core\n",
    "\n",
    "pytest-like assertions. Each one returns a `ValidationResult` with details on what failed and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null checks\n",
    "result = orders.customer_id.is_not_null()\n",
    "print(f\"customer_id not null: {'‚úÖ' if result.passed else '‚ùå'}\")\n",
    "if not result.passed:\n",
    "    print(f\"  ‚Üí {result.summary()}\")\n",
    "\n",
    "# Uniqueness\n",
    "result = orders.order_id.is_unique()\n",
    "print(f\"order_id unique: {'‚úÖ' if result.passed else '‚ùå'}\")\n",
    "\n",
    "# Range checks\n",
    "result = orders.quantity.between(1, 100)\n",
    "print(f\"quantity in [1, 100]: {'‚úÖ' if result.passed else '‚ùå'}\")\n",
    "if not result.passed:\n",
    "    print(f\"  ‚Üí {result.summary()}\")\n",
    "\n",
    "# Enum checks\n",
    "result = orders.status.isin([\"pending\", \"shipped\", \"delivered\", \"cancelled\"])\n",
    "print(f\"status valid: {'‚úÖ' if result.passed else '‚ùå'}\")\n",
    "if not result.passed:\n",
    "    print(f\"  ‚Üí {result.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Row-Level Error Debugging\n",
    "\n",
    "Don't just know *that* something failed ‚Äî see *exactly which rows* and *why*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = orders.quantity.between(1, 100)\n",
    "\n",
    "if not result.passed:\n",
    "    print(\"Failed rows:\")\n",
    "    for row in result.failed_rows:\n",
    "        print(f\"  Row {row.row_number}: quantity={row.value} ‚Äî {row.reason}\")\n",
    "    print(f\"\\nFailed values: {result.get_failed_values()}\")\n",
    "    print(f\"Failed row indices: {result.get_failed_row_indices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quality Scoring\n",
    "\n",
    "Get a composite quality score across 4 dimensions: completeness, uniqueness, validity, and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = orders.score()\n",
    "\n",
    "print(f\"Overall Grade: {score.grade}\")\n",
    "print(f\"Overall Score: {score.overall:.1f}/100\")\n",
    "print(f\"\")\n",
    "print(f\"  Completeness: {score.completeness:.1f}%  (non-null values)\")\n",
    "print(f\"  Uniqueness:   {score.uniqueness:.1f}%  (distinct values in key columns)\")\n",
    "print(f\"  Validity:     {score.validity:.1f}%  (values passing type/range checks)\")\n",
    "print(f\"  Consistency:  {score.consistency:.1f}%  (consistent formatting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Anomaly Detection\n",
    "\n",
    "7 built-in methods: z-score, IQR, modified z-score, percent change, baseline, KS-test, seasonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckguard import detect_anomalies\n",
    "\n",
    "report = detect_anomalies(orders, method=\"zscore\", columns=[\"quantity\", \"total_amount\"])\n",
    "\n",
    "print(f\"Anomalies found: {report.has_anomalies}\")\n",
    "print(f\"Count: {report.anomaly_count}\")\n",
    "for a in report.anomalies:\n",
    "    status = \"üö® ANOMALY\" if a.is_anomaly else \"‚úÖ Normal\"\n",
    "    print(f\"  {a.column}: score={a.score:.2f} ‚Üí {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Auto-Suggest Validation Rules\n",
    "\n",
    "DuckGuard can analyze your data and generate YAML rules automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckguard import generate_rules\n",
    "\n",
    "yaml_rules = generate_rules(orders, dataset_name=\"orders\")\n",
    "print(yaml_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Use with Any Source\n",
    "\n",
    "Everything above works identically on any data source ‚Äî not just CSV:\n",
    "\n",
    "```python\n",
    "# Parquet files (local or cloud)\n",
    "data = connect(\"s3://my-bucket/data.parquet\")\n",
    "\n",
    "# Snowflake\n",
    "data = connect(\"snowflake://account/db\", table=\"orders\")\n",
    "\n",
    "# Databricks\n",
    "data = connect(\"databricks://workspace.databricks.com\", table=\"orders\")\n",
    "\n",
    "# BigQuery\n",
    "data = connect(\"bigquery://project\", table=\"orders\")\n",
    "\n",
    "# Delta Lake\n",
    "data = connect(\"delta://path/to/delta_table\")\n",
    "\n",
    "# pandas DataFrame\n",
    "data = connect(your_dataframe)\n",
    "```\n",
    "\n",
    "Install connectors as needed: `pip install duckguard[snowflake]` or `pip install duckguard[all]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. In Your pytest Suite\n",
    "\n",
    "DuckGuard validations are just Python assertions ‚Äî they work natively in pytest:\n",
    "\n",
    "```python\n",
    "# tests/test_data_quality.py\n",
    "from duckguard import connect\n",
    "\n",
    "def test_orders():\n",
    "    orders = connect(\"s3://warehouse/orders.parquet\")\n",
    "    assert orders.row_count > 0\n",
    "    assert orders.order_id.is_not_null()\n",
    "    assert orders.order_id.is_unique()\n",
    "    assert orders.total_amount.between(0, 10000)\n",
    "```\n",
    "\n",
    "Run with `pytest` ‚Äî data quality as part of CI/CD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| What | How |\n",
    "|------|-----|\n",
    "| Install | `pip install duckguard` |\n",
    "| Connect | `connect(\"file.csv\")` / `connect(\"snowflake://...\")` |\n",
    "| Validate | `assert data.col.is_not_null()` |\n",
    "| Profile | `AutoProfiler().profile(data)` |\n",
    "| Score | `data.score()` ‚Üí A/B/C/D/F |\n",
    "| Anomalies | `detect_anomalies(data, method=\"zscore\")` |\n",
    "| PII | `SemanticAnalyzer().analyze(data)` |\n",
    "| Rules | `generate_rules(data)` ‚Üí YAML |\n",
    "\n",
    "**3 lines of code. 10x faster than Great Expectations. Works with any data source.**\n",
    "\n",
    "üìö [Full Docs](https://xdatahubai.github.io/duckguard/) ¬∑ ‚≠ê [GitHub](https://github.com/XDataHubAI/duckguard) ¬∑ üì¶ [PyPI](https://pypi.org/project/duckguard/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
