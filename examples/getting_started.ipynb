{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# DuckGuard 2.2 - Getting Started Guide\n\n**DuckGuard** is a Python-native data quality tool built on DuckDB for speed.\n\n## What's New in v2.2\n- **Freshness Monitoring**: Detect stale data via file mtime or column timestamps\n- **ML-Based Anomaly Detection**: Auto-learn baselines, KS-test for distribution drift\n- **Schema Evolution Tracking**: Track schema changes and detect breaking changes\n- **Email Notifications**: SMTP-based alerts with HTML formatting\n- **Reference/FK Checks**: Validate foreign key relationships across datasets\n- **Cross-Dataset Validation**: Compare columns and row counts between datasets\n- **Reconciliation**: Comprehensive dataset comparison for migration validation\n- **Distribution Drift Detection**: KS-test based drift detection for ML pipelines\n- **Group By Checks**: Segmented validation for partition-level quality checks\n\n## What's in v2.1\n- **Slack/Teams Notifications**: Get alerts when data quality checks fail\n- **Row-Level Error Capture**: See exactly which rows failed validation\n- **dbt Integration**: Export rules as dbt tests, import dbt schema.yml\n- **Enhanced Error Messages**: Helpful suggestions and context in errors\n- **HTML/PDF Reports**: Generate beautiful, shareable quality reports\n- **Historical Tracking**: Store validation results and analyze trends over time\n- **Airflow Operator**: Native integration for data pipelines\n- **GitHub Action**: CI/CD data quality gates\n\n## What's in v2.0\n- **YAML-based Rules**: Define rules in YAML with a simple, clean syntax\n- **Semantic Type Detection**: Auto-detect emails, phones, PII, and 30+ types\n- **Data Contracts**: Schema + quality SLAs with breaking change detection\n- **Anomaly Detection**: Statistical anomaly detection (Z-score, IQR, percent change)\n- **Enhanced CLI**: Beautiful Rich output with new commands\n\nThis notebook walks you through:\n1. Connecting to data sources\n2. Exploring your data\n3. Calculating quality scores\n4. YAML-based rules\n5. Semantic type detection\n6. Data contracts\n7. Anomaly detection\n8. **NEW**: Freshness Monitoring\n9. **NEW**: ML-Based Anomaly Detection\n10. **NEW**: Schema Evolution Tracking\n11. **NEW**: Reference/FK Checks & Cross-Dataset Validation\n12. **NEW**: Reconciliation\n13. **NEW**: Distribution Drift Detection\n14. **NEW**: Group By Checks\n15. **NEW**: Email Notifications\n16. Python assertions\n17. Row-level error debugging\n18. Slack/Teams notifications\n19. dbt integration\n20. HTML/PDF Reports\n21. Historical Tracking\n22. Airflow Integration\n23. Using with pytest\n24. CLI commands"
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "metadata": {},
   "source": "# Install DuckGuard from PyPI\n%pip install duckguard --quiet\n\nprint(\"DuckGuard installed successfully!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Import DuckGuard - all the new features!\nfrom duckguard import (\n    # Core\n    connect, score, profile,\n    # YAML Rules\n    load_rules, load_rules_from_string, execute_rules, generate_rules, RuleSet,\n    # Semantic Types\n    SemanticType, SemanticAnalyzer, detect_type, detect_types_for_dataset,\n    # Data Contracts\n    DataContract, load_contract, validate_contract, generate_contract, diff_contracts,\n    # Anomaly Detection\n    AnomalyDetector, AnomalyResult, detect_anomalies,\n    # NEW in v2.1: Row-level errors\n    FailedRow,\n    # Version\n    __version__\n)\n# Additional contract utilities\nfrom duckguard.contracts import contract_to_yaml\n\nprint(f\"DuckGuard v{__version__} imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## 2. Connecting to Data Sources\n\nDuckGuard auto-detects the data source type from the path or connection string."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a CSV file\n",
    "orders = connect(\"sample_data/orders.csv\")\n",
    "\n",
    "print(f\"Dataset: {orders.name}\")\n",
    "print(f\"Rows: {orders.row_count}\")\n",
    "print(f\"Columns: {orders.column_count}\")\n",
    "print(f\"Column names: {orders.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "orders.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Other Connection Examples\n",
    "\n",
    "```python\n",
    "# Parquet files\n",
    "data = connect(\"data/events.parquet\")\n",
    "\n",
    "# JSON files\n",
    "data = connect(\"data/users.json\")\n",
    "\n",
    "# Cloud storage\n",
    "data = connect(\"s3://bucket/data.parquet\")\n",
    "data = connect(\"gs://bucket/data.csv\")\n",
    "\n",
    "# Databases\n",
    "data = connect(\"postgres://user:pass@host/db\", table=\"orders\")\n",
    "data = connect(\"snowflake://account/db\", table=\"orders\", schema=\"public\")\n",
    "data = connect(\"bigquery://project/dataset\", table=\"orders\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## 3. Exploring Columns\n\nAccess columns using attribute or bracket notation to get statistics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a column\n",
    "customer_col = orders.customer_id\n",
    "\n",
    "# View column statistics\n",
    "print(f\"Column: {customer_col.name}\")\n",
    "print(f\"Total values: {customer_col.total_count}\")\n",
    "print(f\"Null count: {customer_col.null_count}\")\n",
    "print(f\"Null %: {customer_col.null_percent:.2f}%\")\n",
    "print(f\"Unique count: {customer_col.unique_count}\")\n",
    "print(f\"Unique %: {customer_col.unique_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric column statistics\n",
    "amount_col = orders.total_amount\n",
    "\n",
    "print(f\"Column: {amount_col.name}\")\n",
    "print(f\"Min: {amount_col.min}\")\n",
    "print(f\"Max: {amount_col.max}\")\n",
    "print(f\"Mean: {amount_col.mean:.2f}\")\n",
    "print(f\"Median: {amount_col.median}\")\n",
    "print(f\"Stddev: {amount_col.stddev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View value distribution\n",
    "orders.status.get_value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Quality Scores\n",
    "\n",
    "Calculate data quality scores across standard dimensions:\n",
    "- **Completeness**: Are all required values present?\n",
    "- **Uniqueness**: Are values appropriately unique?\n",
    "- **Validity**: Do values conform to expected formats/ranges?\n",
    "- **Consistency**: Are values consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate quality score\n",
    "result = orders.score()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOverall Score: {result.overall:.1f} / 100\")\n",
    "print(f\"Grade: {result.grade}\")\n",
    "print(f\"\\nDimension Scores:\")\n",
    "print(f\"  Completeness: {result.completeness:.1f}\")\n",
    "print(f\"  Uniqueness:   {result.uniqueness:.1f}\")\n",
    "print(f\"  Validity:     {result.validity:.1f}\")\n",
    "print(f\"  Consistency:  {result.consistency:.1f}\")\n",
    "print(f\"\\nChecks: {result.passed_checks}/{result.total_checks} passed ({result.pass_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## 5. YAML-Based Rules (NEW in v2.0)\n\nDefine data quality rules in YAML with a simple, intuitive syntax. This is easier than Soda's SodaCL!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Define rules directly in Python using YAML string\n# Note: Our sample data has intentional nulls and anomalies, so we use thresholds\nyaml_rules = \"\"\"\ndataset: orders\ndescription: Data quality rules for orders\n\nrules:\n  # Table-level rules\n  - row_count > 0\n  - row_count < 1000000\n  \n  # Column-level rules with simple syntax\n  - order_id is not null\n  - order_id is unique\n  - customer_id null_percent < 10\n  - total_amount >= 0\n  - total_amount < 10000\n  - status in ['pending', 'shipped', 'delivered', 'cancelled']\n  - quantity >= 1\n\"\"\"\n\n# Load and execute rules\nrules = load_rules_from_string(yaml_rules)\nprint(f\"Loaded {len(rules.checks)} rules\")\nprint(f\"Dataset: {rules.dataset}\")\nprint(f\"\\nRules:\")\nfor check in rules.checks:\n    print(f\"  - {check.expression}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Execute rules against the dataset\nresult = execute_rules(rules, dataset=orders)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RULE EXECUTION RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"Total: {result.total_checks}\")\nprint(f\"Passed: {result.passed_count}\")\nprint(f\"Failed: {result.failed_count}\")\nprint(f\"Success Rate: {result.quality_score:.1f}%\")\nprint(f\"\\nDetails:\")\nfor check_result in result.results:\n    status = \"PASS\" if check_result.passed else \"FAIL\"\n    print(f\"  [{status}] {check_result.check.expression}\")\n    if not check_result.passed:\n        print(f\"         -> {check_result.message}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-generate YAML rules from data analysis\n",
    "generated_yaml = generate_rules(orders, dataset_name=\"orders\")\n",
    "print(\"Generated YAML Rules:\")\n",
    "print(generated_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": "### Save Rules to a File\n\n```python\n# Save generated rules\nwith open(\"duckguard.yaml\", \"w\") as f:\n    f.write(generated_yaml)\n\n# Later, load and execute\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, orders)\n```"
  },
  {
   "cell_type": "code",
   "id": "s1vvpxmry3",
   "source": "# Load rules from a YAML file (we have a sample file in sample_data/)\nfile_rules = load_rules(\"sample_data/duckguard.yaml\")\nprint(f\"Loaded {len(file_rules.checks)} rules from file\")\nprint(f\"Dataset: {file_rules.dataset}\")\nprint(f\"Description: {file_rules.description}\")\n\n# Execute the file-based rules (note: dataset must be passed as keyword argument)\nfile_result = execute_rules(file_rules, dataset=orders)\nprint(f\"\\nResults: {file_result.passed_count}/{file_result.total_checks} passed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "uy32pduzaa",
   "source": "# Working with RuleSet programmatically\n# RuleSet allows you to build rules in code instead of YAML\n\n# Create an empty RuleSet\ncustom_rules = RuleSet(name=\"custom_orders\", version=\"1.0\", description=\"Custom rules\")\n\n# Add simple checks using expressions (same syntax as YAML)\ncustom_rules.add_simple_check(\"row_count > 0\")\ncustom_rules.add_simple_check(\"order_id is not null\")\ncustom_rules.add_simple_check(\"quantity >= 1\")\ncustom_rules.add_simple_check(\"status in ['pending', 'shipped', 'delivered', 'cancelled']\")\n\nprint(f\"RuleSet: {custom_rules.name}\")\nprint(f\"Version: {custom_rules.version}\")\nprint(f\"Description: {custom_rules.description}\")\nprint(f\"Total checks: {len(custom_rules.checks)}\")\nprint(f\"\\nRules added:\")\nfor check in custom_rules.checks:\n    print(f\"  - {check.expression}\")\n\n# Execute our programmatic rules (note: dataset must be passed as keyword argument)\ncustom_result = execute_rules(custom_rules, dataset=orders)\nprint(f\"\\nResults: {custom_result.passed_count}/{custom_result.total_checks} passed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## 6. Semantic Type Detection (NEW in v2.0)\n\nDuckGuard automatically detects semantic types like emails, phone numbers, UUIDs, credit cards, and PII."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect semantic types for a single column\n",
    "email_type = detect_type(orders, \"email\")\n",
    "print(f\"Column 'email' detected as: {email_type.value if email_type else 'unknown'}\")\n",
    "\n",
    "order_id_type = detect_type(orders, \"order_id\")\n",
    "print(f\"Column 'order_id' detected as: {order_id_type.value if order_id_type else 'unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect types for entire dataset\n",
    "type_results = detect_types_for_dataset(orders)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SEMANTIC TYPE DETECTION\")\n",
    "print(f\"{'='*60}\")\n",
    "for col_name, sem_type in type_results.items():\n",
    "    type_name = sem_type.value if sem_type else \"generic\"\n",
    "    print(f\"  {col_name:20} -> {type_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Use the SemanticAnalyzer for detailed analysis\nanalyzer = SemanticAnalyzer()\nanalysis = analyzer.analyze(orders)\n\nprint(f\"\\nAnalysis Summary:\")\nprint(f\"  Columns analyzed: {len(analysis.columns)}\")\nprint(f\"  PII columns detected: {len(analysis.pii_columns)}\")\nif analysis.pii_columns:\n    print(f\"  PII warning: Columns {analysis.pii_columns} may contain PII!\")\n\nprint(f\"\\nDetected Types:\")\nfor col_analysis in analysis.columns:\n    confidence = f\"({col_analysis.confidence:.0%})\" if col_analysis.confidence else \"\"\n    detected = col_analysis.semantic_type.value if col_analysis.semantic_type else \"unknown\"\n    print(f\"  {col_analysis.name}: {detected} {confidence}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "### Supported Semantic Types\n\nDuckGuard detects 30+ semantic types including:\n\n| Category | Types |\n|----------|-------|\n| **Identifiers** | UUID, Email, Phone, URL, IP Address |\n| **Financial** | Credit Card, Currency, IBAN |\n| **Personal (PII)** | SSN, Name, Address, Date of Birth |\n| **Geographic** | Country, State, Zip Code, Latitude, Longitude |\n| **Technical** | JSON, Timestamp, Version, File Path |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 7. Data Contracts (NEW in v2.0)\n",
    "\n",
    "Define schema expectations and quality SLAs with automatic breaking change detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Auto-generate a contract from your data\ncontract = generate_contract(orders, name=\"orders_contract\", owner=\"data-team\")\n\nprint(f\"Contract: {contract.name}\")\nprint(f\"Version: {contract.version}\")\nprint(f\"Owner: {contract.metadata.owner}\")\nprint(f\"\\nSchema ({len(contract.schema)} columns):\")\nfor field in contract.schema:\n    req_status = \"required\" if field.required else \"optional\"\n    print(f\"  {field.name}: {field.type.value if hasattr(field.type, 'value') else field.type} ({req_status})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# View quality SLAs in the contract\nif contract.quality:\n    print(\"Quality SLAs:\")\n    if contract.quality.completeness is not None:\n        print(f\"  Completeness: >= {contract.quality.completeness}%\")\n    if contract.quality.row_count_min is not None:\n        print(f\"  Min row count: {contract.quality.row_count_min}\")\n    if contract.quality.row_count_max is not None:\n        print(f\"  Max row count: {contract.quality.row_count_max}\")\n    if contract.quality.freshness:\n        print(f\"  Freshness: {contract.quality.freshness}\")\n    \n    if contract.quality.uniqueness:\n        print(\"\\n  Uniqueness requirements:\")\n        for col, pct in contract.quality.uniqueness.items():\n            print(f\"    {col}: {pct}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data against a contract\n",
    "validation = validate_contract(contract, orders)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONTRACT VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Valid: {validation.is_valid}\")\n",
    "print(f\"Schema valid: {validation.schema_valid}\")\n",
    "print(f\"Quality valid: {validation.quality_valid}\")\n",
    "\n",
    "if validation.errors:\n",
    "    print(f\"\\nErrors:\")\n",
    "    for error in validation.errors:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "if validation.warnings:\n",
    "    print(f\"\\nWarnings:\")\n",
    "    for warning in validation.warnings:\n",
    "        print(f\"  - {warning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": "# Export contract to YAML (contract_to_yaml was imported at the top)\ncontract_yaml = contract_to_yaml(contract)\nprint(\"Contract as YAML:\")\nprint(contract_yaml)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Breaking Change Detection\n",
    "\n",
    "Compare contracts to detect breaking changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "# Simulate a contract change: make a required column optional (breaking change!)\nfrom duckguard.contracts import DataContract, SchemaField, FieldType\n\n# Original contract (order_id is required)\nold_contract = generate_contract(orders, dataset_name=\"orders_v1\", as_yaml=False)\n\n# New contract (modify to make order_id optional - a breaking change!)\nnew_contract = generate_contract(orders, dataset_name=\"orders_v2\", as_yaml=False)\n# Find and modify order_id field\nfor field in new_contract.schema:\n    if field.name == \"order_id\":\n        field.required = False  # This is a breaking change!\n\n# Detect breaking changes\ndiff_result = diff_contracts(old_contract, new_contract)\n\nprint(f\"\\nContract Diff:\")\nprint(f\"  Has breaking changes: {diff_result.has_breaking_changes}\")\nprint(f\"  Has changes: {diff_result.has_changes}\")\n\nif diff_result.breaking_changes:\n    print(f\"\\nBreaking Changes:\")\n    for change in diff_result.breaking_changes:\n        print(f\"  - {change}\")\n\nif diff_result.non_breaking_changes:\n    print(f\"\\nNon-Breaking Changes:\")\n    for change in diff_result.non_breaking_changes:\n        print(f\"  - {change}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 8. Anomaly Detection (NEW in v2.0)\n",
    "\n",
    "Detect statistical anomalies in your data using Z-score, IQR, or percent change methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick anomaly detection on numeric columns\n",
    "report = detect_anomalies(orders, method=\"zscore\", threshold=3.0)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANOMALY DETECTION REPORT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Source: {report.source}\")\n",
    "print(f\"Anomalies found: {report.anomaly_count}\")\n",
    "print(f\"\\n{report.summary()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed anomaly detection with custom settings\n",
    "detector = AnomalyDetector(method=\"iqr\", threshold=1.5)\n",
    "report = detector.detect(\n",
    "    orders,\n",
    "    columns=[\"quantity\", \"unit_price\", \"total_amount\"],\n",
    "    include_null_check=True\n",
    ")\n",
    "\n",
    "print(f\"Checked {report.statistics.get('columns_checked', 0)} columns\")\n",
    "print(f\"Method: {report.statistics.get('method')}\")\n",
    "print(f\"Threshold: {report.statistics.get('threshold')}\")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "for anomaly in report.anomalies:\n",
    "    status = \"ANOMALY\" if anomaly.is_anomaly else \"OK\"\n",
    "    print(f\"  [{status}] {anomaly.column}: {anomaly.message}\")\n",
    "    if anomaly.is_anomaly and anomaly.samples:\n",
    "        print(f\"          Samples: {anomaly.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies with historical baseline\n",
    "# Useful for monitoring metrics over time\n",
    "\n",
    "# Simulate historical baseline values\n",
    "historical_totals = [50.0, 55.0, 48.0, 52.0, 51.0, 49.0, 53.0, 50.0]\n",
    "\n",
    "detector = AnomalyDetector(method=\"percent_change\", threshold=0.2)  # 20% change threshold\n",
    "result = detector.detect_column(\n",
    "    orders, \n",
    "    \"total_amount\",\n",
    "    baseline_values=historical_totals\n",
    ")\n",
    "\n",
    "print(f\"Column: {result.column}\")\n",
    "print(f\"Is Anomaly: {result.is_anomaly}\")\n",
    "print(f\"Score: {result.score:.2f}\")\n",
    "print(f\"Threshold: {result.threshold}\")\n",
    "print(f\"Message: {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "### Available Anomaly Detection Methods\n",
    "\n",
    "| Method | Description | Best For |\n",
    "|--------|-------------|----------|\n",
    "| `zscore` | Standard deviations from mean | Normal distributions |\n",
    "| `iqr` | Interquartile range | Robust to outliers |\n",
    "| `percent_change` | % change from baseline | Monitoring metrics |\n",
    "| `modified_zscore` | Uses median & MAD | Non-normal distributions |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p5xevfu5wfr",
   "source": "## 8. Freshness Monitoring (NEW in v2.2)\n\nDetect stale data before it causes problems. DuckGuard checks freshness via file modification time or timestamp columns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ki0rijf09ji",
   "source": "# Freshness Monitoring - check data staleness\nfrom duckguard.freshness import FreshnessMonitor, FreshnessResult\nfrom datetime import timedelta\n\n# Quick freshness check via dataset property\nprint(\"Freshness Check via Property:\")\nprint(\"-\" * 60)\nfreshness = orders.freshness\nprint(f\"Source: {freshness.source}\")\nprint(f\"Last modified: {freshness.last_modified}\")\nprint(f\"Age: {freshness.age_human}\")\nprint(f\"Is fresh (24h threshold): {freshness.is_fresh}\")\nprint(f\"Method: {freshness.method.value}\")\n\n# Custom threshold check\nprint(\"\\nCustom Threshold Check:\")\nprint(\"-\" * 60)\nis_fresh_6h = orders.is_fresh(timedelta(hours=6))\nprint(f\"Fresh within 6 hours: {is_fresh_6h}\")\n\nis_fresh_1d = orders.is_fresh(timedelta(days=1))\nprint(f\"Fresh within 1 day: {is_fresh_1d}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bbl1h2orieu",
   "source": "# FreshnessMonitor for advanced freshness checks\nmonitor = FreshnessMonitor(threshold=timedelta(hours=12))\n\n# Check via file modification time\nresult = monitor.check_file_mtime(\"sample_data/orders.csv\")\nprint(\"File Modification Time Check:\")\nprint(\"-\" * 60)\nprint(f\"  Last modified: {result.last_modified}\")\nprint(f\"  Age: {result.age_human}\")\nprint(f\"  Is fresh: {result.is_fresh}\")\nprint(f\"  Threshold: {result.threshold_seconds / 3600:.1f} hours\")\n\n# Check via timestamp column (if you have one)\n# result = monitor.check_column_timestamp(orders, \"created_at\")\n# print(f\"Column timestamp fresh: {result.is_fresh}\")\n\n# Full result as dictionary (useful for logging/storage)\nprint(\"\\nResult as dict:\")\nprint(result.to_dict())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "xd7tb5qpci",
   "source": "## 9. ML-Based Anomaly Detection (NEW in v2.2)\n\nDuckGuard now supports machine learning-based anomaly detection methods:\n- **Baseline Method**: Learn from historical data and detect deviations\n- **KS-Test Method**: Kolmogorov-Smirnov test for distribution drift\n- **Seasonal Method**: Account for time-based patterns\n\nThese methods auto-learn patterns without requiring manual thresholds!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "43njidib4hy",
   "source": "# ML-Based Anomaly Detection\nfrom duckguard.anomaly import BaselineMethod, KSTestMethod\n\n# Baseline Method - learn and compare\nprint(\"Baseline Method:\")\nprint(\"-\" * 60)\nbaseline = BaselineMethod(sensitivity=2.0)\n\n# Fit on numeric column data\nbaseline.fit(orders.total_amount)\nprint(f\"Learned baseline for 'total_amount'\")\nprint(f\"  Mean: {baseline.baseline_mean:.2f}\")\nprint(f\"  Stddev: {baseline.baseline_std:.2f}\")\n\n# Score values against baseline (0 = normal, higher = more anomalous)\nscores = baseline.score(orders.total_amount)\nprint(f\"  Scored {len(scores)} values\")\nprint(f\"  Max anomaly score: {max(scores):.2f}\")\nprint(f\"  Anomalies found: {sum(1 for s in scores if s > 1.0)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "g6tnm8hi218",
   "source": "# KS-Test Method - detect distribution drift\nprint(\"KS-Test Method (Distribution Drift):\")\nprint(\"-\" * 60)\nks_method = KSTestMethod(p_value_threshold=0.05)\n\n# Compare current distribution to a reference\ncomparison = ks_method.compare_distributions(orders.total_amount)\nprint(f\"Column: total_amount\")\nprint(f\"  P-value: {comparison.p_value:.4f}\")\nprint(f\"  Statistic: {comparison.statistic:.4f}\")\nprint(f\"  Is drift detected: {comparison.is_drift}\")\nprint(f\"  Message: {comparison.message}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "l9xffm11uw",
   "source": "## 10. Schema Evolution Tracking (NEW in v2.2)\n\nTrack schema changes over time and detect breaking changes before they cause issues.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ojrutnherz",
   "source": "# Schema Evolution Tracking\nfrom duckguard.schema_history import SchemaTracker, SchemaChangeAnalyzer\nfrom duckguard.history import HistoryStorage\n\n# Use temp storage for demo\nimport tempfile\nimport os\nschema_db = os.path.join(tempfile.gettempdir(), \"demo_schema.db\")\nschema_storage = HistoryStorage(db_path=schema_db)\n\n# Create a schema tracker\ntracker = SchemaTracker(storage=schema_storage)\n\n# Capture a snapshot of the current schema\nprint(\"Schema Snapshot:\")\nprint(\"-\" * 60)\nsnapshot = tracker.capture(orders)\nprint(f\"Source: {snapshot.source}\")\nprint(f\"Snapshot ID: {snapshot.snapshot_id[:8]}...\")\nprint(f\"Columns: {snapshot.column_count}\")\nprint(f\"Rows: {snapshot.row_count}\")\nprint(f\"\\nColumn Schema:\")\nfor col in snapshot.columns[:5]:  # Show first 5 columns\n    print(f\"  {col.name}: {col.dtype} (nullable={col.nullable})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "5vf6e1kdswi",
   "source": "# Detect schema changes\nanalyzer = SchemaChangeAnalyzer(storage=schema_storage)\n\n# Detect changes against previous snapshot\nprint(\"Schema Change Detection:\")\nprint(\"-\" * 60)\nreport = analyzer.detect_changes(orders)\n\nprint(f\"Previous snapshot: {report.previous_snapshot.snapshot_id[:8] if report.previous_snapshot else 'None'}...\")\nprint(f\"Current snapshot: {report.current_snapshot.snapshot_id[:8]}...\")\nprint(f\"Has changes: {report.has_changes}\")\nprint(f\"Has breaking changes: {report.has_breaking_changes}\")\n\nif report.changes:\n    print(f\"\\nChanges detected:\")\n    for change in report.changes:\n        print(f\"  {change}\")\nelse:\n    print(\"\\nNo schema changes detected (same schema as previous snapshot)\")\n\n# View schema history\nprint(\"\\nSchema History:\")\nhistory = tracker.get_history(orders.source, limit=5)\nfor snap in history:\n    print(f\"  {snap.captured_at}: {snap.column_count} columns, {snap.row_count} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8iudpin12i5",
   "source": "## 11. Reference/FK Checks & Cross-Dataset Validation (NEW in v2.2)\n\nValidate foreign key relationships and compare data across multiple datasets. This is essential for data lake integrity and ensuring referential integrity without a traditional database.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "28opftx3t4n",
   "source": "# Create sample data for cross-dataset validation demo\nimport tempfile\nimport os\n\n# Create a customers reference table\ncustomers_content = \"\"\"id,name,email\nCUST-001,Alice,alice@example.com\nCUST-002,Bob,bob@example.com\nCUST-003,Charlie,charlie@example.com\nCUST-004,Diana,diana@example.com\nCUST-005,Eve,eve@example.com\n\"\"\"\n\n# Create orders with some invalid customer references (orphans)\norders_orphans_content = \"\"\"order_id,customer_id,amount,status\nORD-001,CUST-001,100.00,shipped\nORD-002,CUST-002,200.00,pending\nORD-003,CUST-999,150.00,shipped\nORD-004,CUST-001,50.00,delivered\nORD-005,CUST-888,300.00,pending\nORD-006,CUST-003,75.00,shipped\n\"\"\"\n\n# Create a status lookup table\nstatus_lookup_content = \"\"\"code,description\nshipped,Order has been shipped\npending,Order is pending\ndelivered,Order has been delivered\ncancelled,Order was cancelled\n\"\"\"\n\n# Write temp files\ntemp_dir = tempfile.gettempdir()\ncustomers_file = os.path.join(temp_dir, \"demo_customers.csv\")\norders_orphans_file = os.path.join(temp_dir, \"demo_orders_orphans.csv\")\nstatus_lookup_file = os.path.join(temp_dir, \"demo_status_lookup.csv\")\n\nwith open(customers_file, 'w') as f:\n    f.write(customers_content)\nwith open(orders_orphans_file, 'w') as f:\n    f.write(orders_orphans_content)\nwith open(status_lookup_file, 'w') as f:\n    f.write(status_lookup_content)\n\nprint(\"Created demo files for cross-dataset validation\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "exrmbmm5xv6",
   "source": "# Reference/FK Checks - Validate foreign key relationships\nfrom duckguard import connect\n\n# Connect to our demo datasets\ncustomers = connect(customers_file)\norders_with_orphans = connect(orders_orphans_file)\nstatus_lookup = connect(status_lookup_file)\n\nprint(\"Datasets loaded:\")\nprint(f\"  Customers: {customers.row_count} rows\")\nprint(f\"  Orders: {orders_with_orphans.row_count} rows\")\nprint(f\"  Status Lookup: {status_lookup.row_count} rows\")\n\n# Check if all customer_id values exist in customers table\nprint(\"\\n\" + \"=\" * 60)\nprint(\"REFERENCE/FK VALIDATION\")\nprint(\"=\" * 60)\n\nresult = orders_with_orphans[\"customer_id\"].exists_in(customers[\"id\"])\n\nprint(f\"\\nCheck: orders.customer_id exists_in customers.id\")\nprint(f\"Passed: {result.passed}\")\nprint(f\"Orphan count: {result.actual_value}\")\n\nif not result.passed:\n    print(f\"\\nOrphan records found:\")\n    for row in result.failed_rows:\n        print(f\"  Row {row.row_number}: customer_id = '{row.value}'\")\n    \n    print(f\"\\nDetails: {result.details}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j8lxfju89fh",
   "source": "# references() - FK check with null handling options\nprint(\"references() with null handling:\")\nprint(\"-\" * 60)\n\n# allow_nulls=True (default) - treats nulls as valid (optional FK)\nresult_allow_nulls = orders_with_orphans[\"customer_id\"].references(\n    customers[\"id\"],\n    allow_nulls=True\n)\nprint(f\"With allow_nulls=True: {result_allow_nulls.actual_value} failures\")\n\n# allow_nulls=False - treats nulls as failures (required FK)\nresult_no_nulls = orders_with_orphans[\"customer_id\"].references(\n    customers[\"id\"],\n    allow_nulls=False\n)\nprint(f\"With allow_nulls=False: {result_no_nulls.actual_value} failures\")\n\n# Get list of orphan values for debugging\nprint(\"\\nfind_orphans() - Get orphan values:\")\nprint(\"-\" * 60)\norphans = orders_with_orphans[\"customer_id\"].find_orphans(customers[\"id\"])\nprint(f\"Orphan customer IDs: {orphans}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ph7rwvdjmtl",
   "source": "# Cross-Dataset Validation - Compare columns and row counts\nprint(\"Cross-Dataset Validation:\")\nprint(\"=\" * 60)\n\n# matches_values() - Check if column values match a lookup table\nprint(\"\\nmatches_values() - Validate against lookup table:\")\nprint(\"-\" * 60)\nresult = orders_with_orphans[\"status\"].matches_values(status_lookup[\"code\"])\nprint(f\"Check: orders.status matches_values status_lookup.code\")\nprint(f\"Passed: {result.passed}\")\nprint(f\"Details:\")\nprint(f\"  Missing in other: {result.details.get('missing_in_other', 0)} values\")\nprint(f\"  Extra in other: {result.details.get('extra_in_other', 0)} values\")\n\n# The orders have: shipped, pending, delivered\n# The lookup has: shipped, pending, delivered, cancelled\n# So \"cancelled\" is extra in the lookup (not used in orders)\n\n# row_count_matches() - Compare row counts between datasets\nprint(\"\\nrow_count_matches() - Compare dataset sizes:\")\nprint(\"-\" * 60)\n\n# Create a backup orders file with same data\nbackup_orders_content = \"\"\"order_id,customer_id,amount,status\nORD-001,CUST-001,100.00,shipped\nORD-002,CUST-002,200.00,pending\nORD-003,CUST-003,150.00,shipped\n\"\"\"\nbackup_file = os.path.join(temp_dir, \"demo_backup_orders.csv\")\nwith open(backup_file, 'w') as f:\n    f.write(backup_orders_content)\n\nbackup_orders = connect(backup_file)\n\n# Exact match (will fail - different counts)\nresult = orders_with_orphans.row_count_matches(backup_orders)\nprint(f\"Exact match: {result.passed}\")\nprint(f\"  Source: {result.details['source_count']} rows\")\nprint(f\"  Backup: {result.details['other_count']} rows\")\nprint(f\"  Difference: {result.actual_value}\")\n\n# With tolerance (allows small differences)\nresult_tolerance = orders_with_orphans.row_count_matches(backup_orders, tolerance=5)\nprint(f\"\\nWith tolerance=5: {result_tolerance.passed}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "237lim1e0cij",
   "source": "### Cross-Dataset Validation Summary\n\n| Method | Description | Use Case |\n|--------|-------------|----------|\n| `col.exists_in(other_col)` | Check all values exist in reference column | FK validation |\n| `col.references(other_col, allow_nulls)` | FK check with null handling | Optional/Required FK |\n| `col.find_orphans(other_col)` | Get list of orphan values | Debugging |\n| `col.matches_values(other_col)` | Check value sets match | Lookup validation |\n| `dataset.row_count_matches(other, tolerance)` | Compare row counts | Backup validation |\n| `dataset.row_count_equals(other)` | Exact row count match | Exact comparison |\n\n### Features\n\n- **Efficient SQL**: Uses anti-join patterns for performance on large datasets\n- **Row-Level Details**: See exactly which rows have orphan values\n- **Null Handling**: Control how nulls are treated in FK checks\n- **Tolerance**: Allow small differences in row count comparisons\n- **Shared Engine**: Multiple datasets share the same DuckDB connection",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "krtg31oeago",
   "source": "## 12. Reconciliation (NEW in v2.2)\n\nReconciliation is essential for validating data migrations, ETL pipelines, and ensuring data synchronization between systems. It performs comprehensive row-by-row comparison using key columns.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0edyh2z2lntv",
   "source": "# Create source and target datasets for reconciliation demo\nsource_content = \"\"\"order_id,customer_id,amount,status\nORD-001,CUST-001,100.00,shipped\nORD-002,CUST-002,200.00,pending\nORD-003,CUST-001,150.00,shipped\nORD-004,CUST-003,50.00,delivered\nORD-005,CUST-002,300.00,pending\n\"\"\"\n\n# Target has some differences: ORD-002 amount changed, ORD-004/005 missing, ORD-006 added\ntarget_content = \"\"\"order_id,customer_id,amount,status\nORD-001,CUST-001,100.00,shipped\nORD-002,CUST-002,205.00,pending\nORD-003,CUST-001,150.00,shipped\nORD-006,CUST-003,75.00,delivered\n\"\"\"\n\n# Write temp files\nsource_recon_file = os.path.join(temp_dir, \"demo_source_orders.csv\")\ntarget_recon_file = os.path.join(temp_dir, \"demo_target_orders.csv\")\n\nwith open(source_recon_file, 'w') as f:\n    f.write(source_content)\nwith open(target_recon_file, 'w') as f:\n    f.write(target_content)\n\nprint(\"Created reconciliation demo files\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3qftcvrcdss",
   "source": "# Reconciliation - Compare two datasets comprehensively\nsource = connect(source_recon_file)\ntarget = connect(target_recon_file)\n\nprint(\"Source dataset:\", source.row_count, \"rows\")\nprint(\"Target dataset:\", target.row_count, \"rows\")\n\n# Reconcile using order_id as key\nprint(\"\\n\" + \"=\" * 60)\nprint(\"RECONCILIATION RESULTS\")\nprint(\"=\" * 60)\n\nresult = source.reconcile(\n    target,\n    key_columns=[\"order_id\"],\n    compare_columns=[\"customer_id\", \"amount\", \"status\"]\n)\n\nprint(f\"\\nPassed: {result.passed}\")\nprint(f\"Match percentage: {result.match_percentage:.1f}%\")\nprint(f\"Missing in target: {result.missing_in_target} rows\")\nprint(f\"Extra in target: {result.extra_in_target} rows\")\nprint(f\"Value mismatches: {result.value_mismatches}\")\n\n# Full summary\nprint(\"\\n\" + result.summary())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1s9dnmwfe2l",
   "source": "## 13. Distribution Drift Detection (NEW in v2.2)\n\nDetect when your data distribution has changed significantly. Essential for ML model monitoring, feature drift detection, and ensuring data pipeline consistency. Uses the Kolmogorov-Smirnov (KS) test for statistical rigor.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5m4z8sx20kk",
   "source": "# Create baseline and drifted datasets\nbaseline_content = \"\"\"id,amount,score\n1,100.0,0.5\n2,150.0,0.6\n3,120.0,0.55\n4,180.0,0.7\n5,130.0,0.58\n6,140.0,0.62\n7,160.0,0.65\n8,110.0,0.52\n9,170.0,0.68\n10,125.0,0.56\n\"\"\"\n\n# Drifted data has significantly different distribution\ndrifted_content = \"\"\"id,amount,score\n1,1000.0,0.9\n2,1500.0,0.95\n3,1200.0,0.88\n4,1800.0,0.99\n5,1300.0,0.92\n6,1400.0,0.94\n7,1600.0,0.96\n8,1100.0,0.87\n9,1700.0,0.98\n10,1250.0,0.91\n\"\"\"\n\nbaseline_file = os.path.join(temp_dir, \"demo_baseline.csv\")\ndrifted_file = os.path.join(temp_dir, \"demo_drifted.csv\")\n\nwith open(baseline_file, 'w') as f:\n    f.write(baseline_content)\nwith open(drifted_file, 'w') as f:\n    f.write(drifted_content)\n\nprint(\"Created drift detection demo files\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vnqarorx9qh",
   "source": "# Distribution Drift Detection\nbaseline = connect(baseline_file)\ndrifted = connect(drifted_file)\n\nprint(\"=\" * 60)\nprint(\"DISTRIBUTION DRIFT DETECTION\")\nprint(\"=\" * 60)\n\n# Detect drift in amount column\nprint(\"\\nChecking 'amount' column for drift:\")\nprint(\"-\" * 60)\nresult = baseline[\"amount\"].detect_drift(drifted[\"amount\"])\n\nprint(f\"Drift detected: {result.is_drifted}\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"KS statistic: {result.statistic:.4f}\")\nprint(f\"Threshold: {result.threshold}\")\nprint(f\"Method: {result.method}\")\nprint(f\"\\nMessage: {result.message}\")\n\n# Check another column\nprint(\"\\nChecking 'score' column for drift:\")\nprint(\"-\" * 60)\nresult_score = baseline[\"score\"].detect_drift(drifted[\"score\"])\nprint(f\"Drift detected: {result_score.is_drifted}\")\nprint(f\"P-value: {result_score.p_value:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3g0gyixjwgd",
   "source": "## 14. Group By Checks (NEW in v2.2)\n\nRun validation checks on groups/segments of your data. Essential for partition-level validation, regional quality checks, and ensuring data quality across different segments (e.g., by date, region, product category).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eee0alc2ck",
   "source": "# Create grouped data for demo\ngrouped_content = \"\"\"order_id,customer_id,amount,status,region,date\nORD-001,CUST-001,100.00,shipped,US,2024-01-01\nORD-002,CUST-002,200.00,pending,US,2024-01-01\nORD-003,CUST-001,150.00,shipped,EU,2024-01-02\nORD-004,CUST-003,50.00,delivered,EU,2024-01-02\nORD-005,CUST-002,300.00,pending,US,2024-01-03\nORD-006,CUST-001,75.00,shipped,EU,2024-01-03\nORD-007,CUST-004,125.00,shipped,APAC,2024-01-01\nORD-008,CUST-004,175.00,pending,APAC,2024-01-02\n\"\"\"\n\ngrouped_file = os.path.join(temp_dir, \"demo_grouped_orders.csv\")\nwith open(grouped_file, 'w') as f:\n    f.write(grouped_content)\n\ngrouped_orders = connect(grouped_file)\nprint(f\"Created grouped orders: {grouped_orders.row_count} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "djclocpc74i",
   "source": "# Group By - Get statistics per group\nprint(\"=\" * 60)\nprint(\"GROUP BY CHECKS\")\nprint(\"=\" * 60)\n\n# Group by region\nprint(\"\\nGroup by region:\")\nprint(\"-\" * 60)\ngrouped = grouped_orders.group_by(\"region\")\nprint(f\"Groups found: {grouped.groups}\")\nprint(f\"Total groups: {grouped.group_count}\")\n\n# Get statistics per group\nprint(\"\\nStatistics per group:\")\nstats = grouped.stats()\nfor g in stats:\n    print(f\"  {g['region']}: {g['row_count']} rows\")\n\n# Validate row count per group\nprint(\"\\nValidation: row_count > 1 per region:\")\nprint(\"-\" * 60)\nresult = grouped_orders.group_by(\"region\").row_count_greater_than(1)\nprint(f\"Passed: {result.passed}\")\nprint(f\"Passed groups: {result.passed_groups}/{result.total_groups}\")\n\n# More restrictive validation\nprint(\"\\nValidation: row_count > 5 per region:\")\nprint(\"-\" * 60)\nresult = grouped_orders.group_by(\"region\").row_count_greater_than(5)\nprint(f\"Passed: {result.passed}\")\nprint(f\"Passed groups: {result.passed_groups}/{result.total_groups}\")\n\nif not result.passed:\n    print(\"\\nFailed groups:\")\n    for g in result.get_failed_groups():\n        print(f\"  {g.group_key}: {g.row_count} rows\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d8aojgfsk3g",
   "source": "### New Features Summary (v2.2)\n\n| Feature | Method | Description |\n|---------|--------|-------------|\n| **Reconciliation** | `dataset.reconcile(target, key_columns)` | Compare datasets row-by-row using keys |\n| **Distribution Drift** | `col.detect_drift(reference_col)` | KS-test based distribution comparison |\n| **Group By Checks** | `dataset.group_by(\"col\").row_count_greater_than(n)` | Validate per-group metrics |\n\n### Result Types\n\n| Result Type | Key Attributes |\n|-------------|---------------|\n| `ReconciliationResult` | `.passed`, `.missing_in_target`, `.extra_in_target`, `.value_mismatches`, `.summary()` |\n| `DriftResult` | `.is_drifted`, `.p_value`, `.statistic`, `.threshold`, `.summary()` |\n| `GroupByResult` | `.passed`, `.total_groups`, `.passed_groups`, `.get_failed_groups()`, `.summary()` |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ox21p92dv8l",
   "source": "## 11. Email Notifications (NEW in v2.2)\n\nSend email alerts when data quality checks fail via SMTP.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7ky0hvpasqi",
   "source": "# Email Notifications\nfrom duckguard.notifications import EmailNotifier, EmailConfig\n\n# Configure email notifier\n# In practice, use environment variable DUCKGUARD_EMAIL_CONFIG with JSON config\nemail = EmailNotifier(\n    smtp_host=\"smtp.gmail.com\",\n    smtp_port=587,\n    smtp_user=\"alerts@company.com\",\n    smtp_password=\"your_app_password\",  # Use app passwords, not regular passwords!\n    from_address=\"duckguard@company.com\",\n    to_addresses=[\"team@company.com\", \"oncall@company.com\"],\n    use_tls=True,\n)\n\nprint(\"EmailNotifier configured:\")\nprint(\"-\" * 60)\nprint(f\"SMTP Host: {email.config.smtp_host}\")\nprint(f\"SMTP Port: {email.config.smtp_port}\")\nprint(f\"From: {email.config.from_address}\")\nprint(f\"To: {email.config.to_addresses}\")\nprint(f\"TLS: {email.config.use_tls}\")\n\n# To send an alert (uncomment when you have real SMTP settings):\n# result = execute_rules(rules, dataset=orders)\n# if not result.passed:\n#     email.send_failure_alert(result)\n#     print(\"Failure alert sent!\")\n# \n# # Or send results regardless of pass/fail:\n# email.send_results(result)\n\nprint(\"\\nNote: Email sending requires valid SMTP credentials.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": "## 9. Python Assertions (Traditional Approach)\n\nYou can still use simple Python assertions - DuckGuard integrates with pytest!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks using properties\n",
    "assert orders.row_count > 0, \"Dataset should not be empty\"\n",
    "assert orders.customer_id.null_percent < 5, \"Customer ID should have < 5% nulls\"\n",
    "assert orders.total_amount.min >= 0, \"Amounts should be non-negative\"\n",
    "\n",
    "print(\"All basic assertions passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation methods with detailed results\n",
    "result = orders.order_id.is_not_null(threshold=1.0)\n",
    "print(f\"is_not_null: {result}\")\n",
    "print(f\"  Message: {result.message}\")\n",
    "\n",
    "result = orders.total_amount.between(0, 100000)\n",
    "print(f\"\\nbetween: {result}\")\n",
    "print(f\"  Message: {result.message}\")\n",
    "\n",
    "result = orders.status.isin(['pending', 'shipped', 'delivered', 'cancelled'])\n",
    "print(f\"\\nisin: {result}\")\n",
    "print(f\"  Message: {result.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "ut31o921v4s",
   "source": "# More validation methods\nprint(\"Additional validation methods:\")\nprint(\"-\" * 60)\n\n# is_unique - check if column values are unique\nresult = orders.order_id.is_unique(threshold=100.0)\nprint(f\"is_unique: {result}\")\nprint(f\"  Message: {result.message}\")\n\n# matches - regex pattern matching\nresult = orders.email.matches(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\nprint(f\"\\nmatches (email pattern): {result}\")\nprint(f\"  Message: {result.message}\")\n\n# has_no_duplicates - check for duplicate values\nresult = orders.order_id.has_no_duplicates()\nprint(f\"\\nhas_no_duplicates: {result}\")\nprint(f\"  Message: {result.message}\")\n\n# greater_than - value comparison\nresult = orders.quantity.greater_than(0)\nprint(f\"\\ngreater_than(0): {result}\")\nprint(f\"  Message: {result.message}\")\n\n# less_than - value comparison\nresult = orders.unit_price.less_than(1000)\nprint(f\"\\nless_than(1000): {result}\")\nprint(f\"  Message: {result.message}\")\n\n# value_lengths_between - string length validation\nresult = orders.order_id.value_lengths_between(7, 7)  # ORD-XXX format = 7 chars\nprint(f\"\\nvalue_lengths_between(7, 7): {result}\")\nprint(f\"  Message: {result.message}\")\n\n# get_distinct_values - view unique values\ndistinct_products = orders.product_name.get_distinct_values(limit=5)\nprint(f\"\\nget_distinct_values (products): {distinct_products}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a9cbolvh839",
   "source": "## 10. Row-Level Error Debugging (NEW in v2.1)\n\nWhen validation fails, DuckGuard now captures exactly which rows failed, making debugging much easier.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ifx6vhgnc8f",
   "source": "# When a validation check fails, you can see exactly which rows failed\n# Let's use a restrictive range to force some failures\nresult = orders.quantity.between(1, 5)\n\nprint(f\"Check passed: {result.passed}\")\nprint(f\"Total failures: {result.total_failures}\")\n\n# Get the summary with sample failing rows\nif not result.passed:\n    print(f\"\\n{result.summary()}\")\n    \n    # Get just the failed values\n    failed_values = result.get_failed_values()\n    print(f\"\\nFailed values: {failed_values[:5]}...\")\n    \n    # Get just the row indices\n    failed_indices = result.get_failed_row_indices()\n    print(f\"Failed row indices: {failed_indices[:5]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "i42alk3wt8",
   "source": "# Work with individual FailedRow objects for detailed debugging\nfrom duckguard import FailedRow\n\nif result.failed_rows:\n    print(\"Detailed failed row information:\")\n    print(\"-\" * 60)\n    for row in result.failed_rows[:3]:  # Show first 3\n        print(f\"  Row {row.row_index}:\")\n        print(f\"    Column: {row.column}\")\n        print(f\"    Value: {row.value}\")\n        print(f\"    Expected: {row.expected}\")\n        if row.reason:\n            print(f\"    Reason: {row.reason}\")\n        print()\n\n# You can disable row capture for performance on large datasets\nresult_no_capture = orders.quantity.between(1, 5, capture_failures=False)\nprint(f\"With capture_failures=False: {len(result_no_capture.failed_rows)} rows captured\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5q83vh4y7hf",
   "source": "## 11. Slack/Teams Notifications (NEW in v2.1)\n\nGet notified when your data quality checks fail. DuckGuard supports Slack and Microsoft Teams webhooks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t93p2xyqrd9",
   "source": "# Import notification components\nfrom duckguard.notifications import SlackNotifier, TeamsNotifier, NotificationConfig\n\n# Configure a Slack notifier (use your actual webhook URL)\n# You can also set DUCKGUARD_SLACK_WEBHOOK environment variable\nslack = SlackNotifier(\n    webhook_url=\"https://hooks.slack.com/services/YOUR/WEBHOOK/URL\",\n    channel=\"#data-quality\",  # Optional override\n    username=\"DuckGuard Bot\"\n)\n\n# Configure a Teams notifier (use your actual webhook URL)\n# You can also set DUCKGUARD_TEAMS_WEBHOOK environment variable\nteams = TeamsNotifier(\n    webhook_url=\"https://outlook.office.com/webhook/YOUR/WEBHOOK/URL\"\n)\n\nprint(\"Notifiers configured!\")\nprint(f\"Slack channel: {slack.config.channel}\")\nprint(f\"Teams configured: {teams.webhook_url is not None}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cggr0rg3fu4",
   "source": "### Sending Notifications on Failures\n\n```python\n# Execute rules and send notification on failure\nfrom duckguard import load_rules, execute_rules\nfrom duckguard.notifications import SlackNotifier\n\nrules = load_rules(\"duckguard.yaml\")\nresult = execute_rules(rules, dataset=orders)\n\n# Only sends if there are failures (configurable)\nslack = SlackNotifier(webhook_url=\"https://hooks.slack.com/...\")\n\nif not result.passed:\n    # Send formatted failure alert\n    slack.send_failure_alert(result)\n    \n# Or send results regardless of pass/fail\nslack.send_results(result, notify_on_success=True)\n```\n\n### Notification Features\n\n| Feature | Description |\n|---------|-------------|\n| **Slack Blocks** | Rich formatted messages with sections and fields |\n| **Teams Cards** | Adaptive cards with color-coded status |\n| **Failure Details** | Shows which checks failed and why |\n| **Pass Rate** | Overall quality score included |\n| **Environment Variables** | `DUCKGUARD_SLACK_WEBHOOK` and `DUCKGUARD_TEAMS_WEBHOOK` |\n| **Channel Override** | Send to specific channels per notification |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "k66xhcpmxze",
   "source": "## 12. dbt Integration (NEW in v2.1)\n\nExport DuckGuard validation rules as dbt tests, or import existing dbt tests as DuckGuard rules.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "z8sv7tiswwl",
   "source": "# Import dbt integration functions\nfrom duckguard.integrations import dbt\n\n# Load DuckGuard rules\nrules = load_rules(\"sample_data/duckguard.yaml\")\n\n# Convert rules to dbt test format (schema.yml structure)\ndbt_tests = dbt.rules_to_dbt_tests(rules)\n\nprint(\"Converted to dbt schema.yml format:\")\nprint(\"-\" * 60)\nimport yaml\nprint(yaml.dump(dbt_tests, default_flow_style=False, sort_keys=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7m97vimvewh",
   "source": "### dbt Export Options\n\n```python\nfrom duckguard import load_rules\nfrom duckguard.integrations import dbt\n\nrules = load_rules(\"duckguard.yaml\")\n\n# Export to dbt schema.yml file (merges with existing if present)\ndbt.export_to_schema(rules, \"models/schema.yml\")\n\n# Generate dbt singular tests for complex checks\ndbt.generate_singular_tests(rules, \"tests/\")\n# Creates files like: tests/test_orders_email_null_percent.sql\n\n# Import dbt tests back as DuckGuard rules\nimported_rules = dbt.import_from_dbt(\"models/schema.yml\")\n```\n\n### Mapping from DuckGuard to dbt\n\n| DuckGuard Check | dbt Test |\n|-----------------|----------|\n| `not_null` | `not_null` |\n| `unique` | `unique` |\n| `isin`, `allowed_values` | `accepted_values` |\n| `between`, `range` | `dbt_utils.expression_is_true` |\n| `min`, `max` | `dbt_utils.expression_is_true` |\n| `positive`, `non_negative` | `dbt_utils.expression_is_true` |\n| `pattern`, `matches` | `dbt_utils.expression_is_true` with REGEXP |\n| `null_percent` | Singular test (SQL file) |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8vz06lqgv2f",
   "source": "## 13. HTML/PDF Reports (NEW in v2.1)\n\nGenerate beautiful, shareable data quality reports. Perfect for stakeholders and compliance documentation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "radz0chxym",
   "source": "# Generate HTML/PDF reports from validation results\nfrom duckguard.reports import generate_html_report, generate_pdf_report, ReportConfig\n\n# First, run some validation\nrules = load_rules_from_string(yaml_rules)\nresult = execute_rules(rules, dataset=orders)\n\n# Generate an HTML report\ngenerate_html_report(\n    result, \n    \"quality_report.html\",\n    title=\"Orders Data Quality Report\",\n    include_passed=True  # Include passed checks in the report\n)\n\nprint(\"HTML report generated: quality_report.html\")\nprint(f\"Quality Score: {result.quality_score:.1f}%\")\nprint(f\"Checks: {result.passed_count}/{result.total_checks} passed\")\n\n# For PDF reports (requires weasyprint: pip install duckguard[reports])\n# generate_pdf_report(result, \"quality_report.pdf\", title=\"Orders Quality Report\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fszzvkdqcqd",
   "source": "### Report Features\n\n| Feature | Description |\n|---------|-------------|\n| **Standalone HTML** | No external dependencies, works offline |\n| **Beautiful Styling** | Professional look with color-coded status |\n| **Quality Score** | Overall score with A-F grade |\n| **Check Details** | All passed and failed checks with messages |\n| **PDF Export** | Print-ready PDF format (requires weasyprint) |\n| **Customizable** | Custom titles, include/exclude passed checks |\n\n### CLI Report Generation\n\n```bash\n# Generate HTML report\nduckguard report data.csv --output report.html\n\n# Generate PDF report\nduckguard report data.csv --format pdf --output report.pdf\n\n# With custom title and rules\nduckguard report data.csv --config rules.yaml --title \"Daily Quality Report\"\n\n# Store results in history while generating report\nduckguard report data.csv --store\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "oj4uuc45a28",
   "source": "## 14. Historical Tracking (NEW in v2.1)\n\nStore validation results over time and analyze quality trends. Perfect for monitoring data pipelines.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4daelldeq7a",
   "source": "# Store and query validation history\nfrom duckguard.history import HistoryStorage, TrendAnalyzer\n\n# Create a storage instance (defaults to ~/.duckguard/history.db)\n# Use a temp file for this demo\nimport tempfile\nimport os\ntemp_db = os.path.join(tempfile.gettempdir(), \"demo_history.db\")\nstorage = HistoryStorage(db_path=temp_db)\n\n# Store a validation result\nrun_id = storage.store(result)\nprint(f\"Stored validation run: {run_id[:8]}...\")\n\n# Store another run to simulate history\nrun_id_2 = storage.store(result)\n\n# Query historical runs\nruns = storage.get_runs(result.source, limit=5)\nprint(f\"\\nRecent runs for {result.source}:\")\nfor run in runs:\n    status = \"PASS\" if run.passed else \"FAIL\"\n    print(f\"  [{status}] {run.started_at:%Y-%m-%d %H:%M} - Score: {run.quality_score:.1f}%\")\n\n# Get list of tracked sources\nsources = storage.get_sources()\nprint(f\"\\nTracked sources: {sources}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jx3qq78c2b",
   "source": "# Analyze quality trends over time\nanalyzer = TrendAnalyzer(storage)\ntrend = analyzer.analyze(result.source, days=30)\n\nprint(f\"\\n{'='*60}\")\nprint(\"QUALITY TREND ANALYSIS\")\nprint(f\"{'='*60}\")\nprint(f\"Trend: {trend.score_trend.upper()}\")\nprint(f\"Current Score: {trend.current_score:.1f}%\")\nprint(f\"Average Score: {trend.average_score:.1f}%\")\nprint(f\"Pass Rate: {trend.pass_rate:.1f}%\")\nprint(f\"Total Runs: {trend.total_runs}\")\nprint(f\"\\n{trend.summary()}\")\n\n# Check for quality regression\nif analyzer.has_regression(result.source):\n    print(\"\\nWARNING: Quality regression detected!\")\nelse:\n    print(\"\\nNo quality regression detected.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gayoinw55xb",
   "source": "### CLI History Commands\n\n```bash\n# View recent validation history\nduckguard history\n\n# View history for a specific source\nduckguard history data.csv\n\n# View history for the last 7 days\nduckguard history data.csv --last 7d\n\n# Show trend analysis\nduckguard history data.csv --trend\n\n# Output as JSON\nduckguard history --format json\n```\n\n### History Features\n\n| Feature | Description |\n|---------|-------------|\n| **SQLite Storage** | Lightweight, file-based storage |\n| **Trend Analysis** | Improving, declining, or stable trends |\n| **Anomaly Detection** | Detect unusual quality score drops |\n| **Pass Rate Tracking** | Track validation success over time |\n| **Source Filtering** | Query by data source |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "zxe5c07z8s",
   "source": "## 15. Airflow Integration (NEW in v2.1)\n\nUse DuckGuard in Apache Airflow data pipelines with native operators.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "8ffoux08xn5",
   "source": "### Using DuckGuard in Airflow DAGs\n\n```python\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom duckguard.integrations.airflow import DuckGuardOperator, DuckGuardSensor\nfrom datetime import datetime\n\nwith DAG(\n    \"data_quality_pipeline\",\n    start_date=datetime(2024, 1, 1),\n    schedule_interval=\"@daily\",\n) as dag:\n    \n    # Validate data after loading\n    validate_orders = DuckGuardOperator(\n        task_id=\"validate_orders\",\n        source=\"s3://bucket/orders/{{ ds }}.parquet\",\n        config=\"duckguard.yaml\",\n        fail_on_error=True,      # Fail task if validation fails\n        store_history=True,       # Store results in history\n        notify_on_failure=True,   # Send Slack/Teams notification\n    )\n    \n    # Wait for quality threshold to be met\n    wait_for_quality = DuckGuardSensor(\n        task_id=\"wait_for_quality\",\n        source=\"s3://bucket/orders/{{ ds }}.parquet\",\n        min_quality_score=80.0,   # Wait until score >= 80%\n        timeout=3600,             # Timeout after 1 hour\n        poke_interval=300,        # Check every 5 minutes\n    )\n    \n    # Chain tasks\n    validate_orders >> wait_for_quality\n```\n\n### Operator Features\n\n| Feature | Description |\n|---------|-------------|\n| **Template Fields** | Use Airflow Jinja templating in source paths |\n| **XCom Integration** | Returns quality score and results to XCom |\n| **History Storage** | Automatically store results for trending |\n| **Notifications** | Send Slack/Teams alerts on failure |\n| **Fail on Error** | Configurable task failure behavior |\n| **Quality Sensor** | Wait for quality thresholds to be met |\n\n### Installation\n\n```bash\npip install duckguard[airflow]\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "97notw2n6j9",
   "source": "## 16. GitHub Action (NEW in v2.1)\n\nAdd data quality gates to your CI/CD pipeline with the DuckGuard GitHub Action.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "h8l9tlbm1kt",
   "source": "### GitHub Actions Workflow\n\n```yaml\n# .github/workflows/data-quality.yml\nname: Data Quality Check\n\non:\n  push:\n    paths:\n      - 'data/**'\n  pull_request:\n    paths:\n      - 'data/**'\n\njobs:\n  quality-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Run DuckGuard Quality Check\n        uses: XDataHubAI/duckguard/.github/actions/duckguard-check@main\n        with:\n          source: data/orders.csv\n          config: duckguard.yaml\n          fail-on-warning: false\n          python-version: '3.11'\n        \n      - name: Check Results\n        if: always()\n        run: |\n          echo \"Quality Score: ${{ steps.duckguard.outputs.quality-score }}\"\n          echo \"Grade: ${{ steps.duckguard.outputs.grade }}\"\n          echo \"Passed: ${{ steps.duckguard.outputs.passed }}\"\n```\n\n### Action Inputs\n\n| Input | Description | Required | Default |\n|-------|-------------|----------|---------|\n| `source` | Data source path or URL | Yes | - |\n| `config` | Path to duckguard.yaml | No | Auto-discover |\n| `fail-on-warning` | Fail on warnings | No | `false` |\n| `fail-on-error` | Fail on errors | No | `true` |\n| `python-version` | Python version | No | `3.11` |\n\n### Action Outputs\n\n| Output | Description |\n|--------|-------------|\n| `passed` | Whether all checks passed |\n| `quality-score` | Overall quality score (0-100) |\n| `grade` | Letter grade (A, B, C, D, F) |\n| `checks-total` | Total number of checks |\n| `checks-passed` | Number of passed checks |\n| `checks-failed` | Number of failed checks |\n\n### Features\n\n- Automatic GitHub Step Summary with formatted results\n- Exit codes for CI/CD integration\n- Caching of Python dependencies\n- Works with any data source DuckGuard supports",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "a0wlm8estel",
   "source": "## 13. Enhanced Error Messages (NEW in v2.1)\n\nDuckGuard v2.1 provides helpful error messages with suggestions, documentation links, and context.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ofm33q6etka",
   "source": "# Enhanced error classes with helpful suggestions\nfrom duckguard.errors import (\n    ColumnNotFoundError,\n    ValidationError,\n    UnsupportedConnectorError,\n)\n\n# Example: Column not found error with suggestions\ntry:\n    # Simulate accessing a non-existent column\n    raise ColumnNotFoundError(\n        column=\"order\",\n        available_columns=[\"order_id\", \"customer_id\", \"total_amount\", \"status\"]\n    )\nexcept ColumnNotFoundError as e:\n    print(\"ColumnNotFoundError:\")\n    print(\"-\" * 60)\n    print(str(e))\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3wgjep2rzvm",
   "source": "# Validation error with failed rows and context\ntry:\n    raise ValidationError(\n        check_name=\"between\",\n        column=\"quantity\",\n        actual_value=5,\n        expected_value=\"[1, 100]\",\n        failed_rows=[150, 200, 300, 400, 500]\n    )\nexcept ValidationError as e:\n    print(\"ValidationError:\")\n    print(\"-\" * 60)\n    print(str(e))\n    print()\n\n# Unsupported connector error with format suggestions\ntry:\n    raise UnsupportedConnectorError(source=\"data.xyz\")\nexcept UnsupportedConnectorError as e:\n    print(\"UnsupportedConnectorError:\")\n    print(\"-\" * 60)\n    print(str(e))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": "## 14. Auto-Profiling\n\nLet DuckGuard analyze your data and suggest validation rules."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": "from duckguard.profiler import AutoProfiler\n\n# Profile the dataset\nprofiler = AutoProfiler(dataset_var_name=\"orders\")\nprofile_result = profiler.profile(orders)\n\nprint(f\"Profiled: {profile_result.source}\")\nprint(f\"Rows: {profile_result.row_count}\")\nprint(f\"Columns: {profile_result.column_count}\")\nprint(f\"\\nSuggested Rules ({len(profile_result.suggested_rules)}):\")\nprint(\"-\" * 60)\nfor rule in profile_result.suggested_rules[:10]:  # Show first 10\n    print(rule)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": "## 15. Using with pytest\n\nDuckGuard integrates seamlessly with pytest. Create a test file:\n\n```python\n# test_data_quality.py\nimport pytest\nfrom duckguard import connect, load_rules, execute_rules, validate_contract, load_contract\n\n@pytest.fixture\ndef orders():\n    return connect(\"data/orders.csv\")\n\n# Test with YAML rules\ndef test_yaml_rules(orders):\n    rules = load_rules(\"duckguard.yaml\")\n    result = execute_rules(rules, orders)\n    assert result.failed == 0, f\"Failed checks: {result.failed}\"\n\n# Test with data contract\ndef test_contract(orders):\n    contract = load_contract(\"contract.yaml\")\n    result = validate_contract(contract, orders)\n    assert result.is_valid, f\"Contract violations: {result.errors}\"\n\n# Traditional assertion tests\ndef test_orders_not_empty(orders):\n    assert orders.row_count > 0\n\ndef test_order_ids_valid(orders):\n    assert orders.order_id.null_percent == 0\n    assert orders.order_id.has_no_duplicates()\n\ndef test_quality_score(orders):\n    score = orders.score()\n    assert score.overall >= 80, f\"Quality score too low: {score.overall}\"\n\n# NEW: Test with row-level error details\ndef test_quantity_range(orders):\n    result = orders.quantity.between(1, 100)\n    if not result.passed:\n        # Get detailed failure info for debugging\n        print(result.summary())\n    assert result.passed, f\"Found {result.total_failures} values out of range\"\n```\n\nRun with: `pytest test_data_quality.py -v`"
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": "## 19. CLI Commands\n\nDuckGuard provides powerful CLI commands with beautiful Rich output:\n\n```bash\n# Quick check with auto-generated rules\nduckguard check data/orders.csv\n\n# Check with YAML rules file\nduckguard check data/orders.csv --config duckguard.yaml\n\n# Discover data and generate rules\nduckguard discover data/orders.csv\nduckguard discover data/orders.csv --output duckguard.yaml\n\n# Generate data contract\nduckguard contract generate data/orders.csv\nduckguard contract generate data/orders.csv --output contract.yaml --owner \"data-team\"\n\n# Validate against contract\nduckguard contract validate data/orders.csv --contract contract.yaml\n\n# Compare contracts for breaking changes\nduckguard contract diff old_contract.yaml new_contract.yaml\n\n# Detect anomalies\nduckguard anomaly data/orders.csv\nduckguard anomaly data/orders.csv --method iqr --threshold 1.5\n\n# ML-based anomaly detection (NEW in v2.2)\nduckguard anomaly data/orders.csv --learn-baseline    # Learn baseline\nduckguard anomaly data/orders.csv --method baseline   # Compare to baseline\nduckguard anomaly data/orders.csv --method ks_test    # Distribution drift\n\n# Generate reports (NEW in v2.1)\nduckguard report data/orders.csv                           # HTML report\nduckguard report data/orders.csv --format pdf              # PDF report\nduckguard report data/orders.csv --title \"Daily Report\"    # Custom title\n\n# View validation history (NEW in v2.1)\nduckguard history                                          # All recent runs\nduckguard history data/orders.csv --last 7d               # Last 7 days\nduckguard history data/orders.csv --trend                  # Trend analysis\n\n# Freshness monitoring (NEW in v2.2)\nduckguard freshness data/orders.csv                        # Check via file mtime\nduckguard freshness data/orders.csv --column updated_at    # Check via column\nduckguard freshness data/orders.csv --max-age 6h           # Custom threshold\n\n# Schema evolution tracking (NEW in v2.2)\nduckguard schema data/orders.csv --action show             # Show current schema\nduckguard schema data/orders.csv --action capture          # Capture snapshot\nduckguard schema data/orders.csv --action history          # View schema history\nduckguard schema data/orders.csv --action changes          # Detect changes\n\n# Show version and info\nduckguard info\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": "## 20. Quick Reference\n\n### YAML Rule Syntax\n\n```yaml\ndataset: my_data\nrules:\n  # Table-level\n  - row_count > 0\n  - row_count < 1000000\n  \n  # Column nulls\n  - column_name is not null\n  - column_name null_percent < 5\n  \n  # Uniqueness\n  - column_name is unique\n  - column_name unique_percent > 95\n  \n  # Ranges\n  - column_name >= 0\n  - column_name between 0 and 100\n  \n  # Sets\n  - column_name in ['a', 'b', 'c']\n  \n  # Patterns\n  - column_name matches '^[A-Z]{3}$'\n```\n\n### Reference/FK Checks & Cross-Dataset Validation (v2.2)\n\n```python\nfrom duckguard import connect\n\norders = connect(\"orders.parquet\")\ncustomers = connect(\"customers.parquet\")\nstatus_lookup = connect(\"status_codes.csv\")\n\n# Check FK relationship - all values exist in reference\nresult = orders[\"customer_id\"].exists_in(customers[\"id\"])\n\n# FK check with null handling options\nresult = orders[\"customer_id\"].references(customers[\"id\"], allow_nulls=True)\n\n# Get list of orphan values\norphans = orders[\"customer_id\"].find_orphans(customers[\"id\"])\n\n# Compare value sets between columns\nresult = orders[\"status\"].matches_values(status_lookup[\"code\"])\n\n# Compare row counts between datasets\nresult = orders.row_count_matches(backup_orders)\nresult = orders.row_count_matches(backup_orders, tolerance=10)\n```\n\n### Reconciliation (v2.2)\n\n```python\n# Compare two datasets row-by-row using key columns\nresult = source.reconcile(\n    target,\n    key_columns=[\"order_id\"],\n    compare_columns=[\"amount\", \"status\"],\n    tolerance=0.01,  # Numeric tolerance\n    sample_mismatches=10  # Number of sample mismatches to capture\n)\n\nprint(f\"Match: {result.match_percentage}%\")\nprint(f\"Missing in target: {result.missing_in_target}\")\nprint(f\"Extra in target: {result.extra_in_target}\")\nprint(f\"Value mismatches: {result.value_mismatches}\")\nprint(result.summary())\n```\n\n### Distribution Drift Detection (v2.2)\n\n```python\n# Detect distribution drift using KS-test\nbaseline = connect(\"baseline.csv\")\ncurrent = connect(\"current.csv\")\n\nresult = baseline[\"amount\"].detect_drift(current[\"amount\"])\n\nprint(f\"Drift detected: {result.is_drifted}\")\nprint(f\"P-value: {result.p_value:.4f}\")\nprint(f\"KS statistic: {result.statistic:.4f}\")\nprint(f\"Threshold: {result.threshold}\")\nprint(result.summary())\n```\n\n### Group By Checks (v2.2)\n\n```python\n# Run validation checks on data segments\norders = connect(\"orders.csv\")\n\n# Group by region and validate\ngrouped = orders.group_by(\"region\")\nprint(f\"Groups: {grouped.groups}\")\nprint(f\"Stats: {grouped.stats()}\")\n\n# Validate row counts per group\nresult = orders.group_by(\"region\").row_count_greater_than(10)\nprint(f\"Passed: {result.passed}\")\nprint(f\"Passed groups: {result.passed_groups}/{result.total_groups}\")\n\n# Get failed groups for debugging\nfor g in result.get_failed_groups():\n    print(f\"  {g.group_key}: {g.row_count} rows\")\n```\n\n### Freshness Monitoring (v2.2)\n\n```python\nfrom duckguard.freshness import FreshnessMonitor\nfrom datetime import timedelta\n\n# Quick check via property\nprint(orders.freshness.age_human)  # \"2 hours ago\"\nprint(orders.freshness.is_fresh)   # True\n\n# Custom threshold\nif not orders.is_fresh(timedelta(hours=6)):\n    print(\"Data is stale!\")\n\n# Column-based freshness\nmonitor = FreshnessMonitor(threshold=timedelta(hours=1))\nresult = monitor.check_column_timestamp(orders, \"updated_at\")\n```\n\n### ML-Based Anomaly Detection (v2.2)\n\n```python\nfrom duckguard.anomaly import BaselineMethod, KSTestMethod\n\n# Learn baseline and detect anomalies\nbaseline = BaselineMethod(sensitivity=2.0)\nbaseline.fit(orders.amount)\nscores = baseline.score(orders.amount)\n\n# Distribution drift detection\nks = KSTestMethod(p_value_threshold=0.05)\nresult = ks.compare_distributions(orders.amount)\nprint(f\"Drift detected: {result.is_drift}\")\n```\n\n### Schema Evolution (v2.2)\n\n```python\nfrom duckguard.schema_history import SchemaTracker, SchemaChangeAnalyzer\n\ntracker = SchemaTracker()\nsnapshot = tracker.capture(orders)\n\nanalyzer = SchemaChangeAnalyzer()\nreport = analyzer.detect_changes(orders)\nif report.has_breaking_changes:\n    print(\"Breaking changes detected!\")\n```\n\n### Email Notifications (v2.2)\n\n```python\nfrom duckguard.notifications import EmailNotifier\n\nemail = EmailNotifier(\n    smtp_host=\"smtp.gmail.com\",\n    smtp_user=\"alerts@company.com\",\n    smtp_password=\"app_password\",\n    to_addresses=[\"team@company.com\"],\n)\n# Or set DUCKGUARD_EMAIL_CONFIG env var\n\nif not result.passed:\n    email.send_failure_alert(result)\n```\n\n### Row-Level Error Debugging (v2.1)\n\n```python\nresult = orders.quantity.between(1, 100)\nif not result.passed:\n    print(result.summary())           # Human-readable summary\n    print(result.get_failed_values()) # [150, 200, ...]\n    print(result.get_failed_row_indices())  # [5, 12, ...]\n    for row in result.failed_rows:\n        print(f\"Row {row.row_index}: {row.value}\")\n```\n\n### Notifications (v2.1)\n\n```python\nfrom duckguard.notifications import SlackNotifier, TeamsNotifier\n\nslack = SlackNotifier(webhook_url=\"...\")  # or DUCKGUARD_SLACK_WEBHOOK\nteams = TeamsNotifier(webhook_url=\"...\")  # or DUCKGUARD_TEAMS_WEBHOOK\n\nresult = execute_rules(rules, dataset=orders)\nif not result.passed:\n    slack.send_failure_alert(result)\n    teams.send_failure_alert(result)\n```\n\n### dbt Integration (v2.1)\n\n```python\nfrom duckguard.integrations import dbt\n\ndbt.export_to_schema(rules, \"models/schema.yml\")\ndbt.generate_singular_tests(rules, \"tests/\")\nrules = dbt.import_from_dbt(\"models/schema.yml\")\n```\n\n### HTML/PDF Reports (v2.1)\n\n```python\nfrom duckguard.reports import generate_html_report, generate_pdf_report\n\ngenerate_html_report(result, \"report.html\", title=\"Quality Report\")\ngenerate_pdf_report(result, \"report.pdf\")  # requires weasyprint\n```\n\n### Historical Tracking (v2.1)\n\n```python\nfrom duckguard.history import HistoryStorage, TrendAnalyzer\n\nstorage = HistoryStorage()\nrun_id = storage.store(result)\n\nanalyzer = TrendAnalyzer(storage)\ntrend = analyzer.analyze(\"data.csv\", days=30)\nprint(trend.summary())\n```\n\n### Airflow Integration (v2.1)\n\n```python\nfrom duckguard.integrations.airflow import DuckGuardOperator\n\nvalidate = DuckGuardOperator(\n    task_id=\"validate\",\n    source=\"s3://bucket/data.parquet\",\n    config=\"duckguard.yaml\",\n    fail_on_error=True,\n)\n```\n\n### Semantic Types Detected\n\n- `email`, `phone`, `url`, `ip_address`\n- `uuid`, `credit_card`, `iban`\n- `ssn`, `date_of_birth` (PII)\n- `country`, `state`, `zip_code`\n- `latitude`, `longitude`\n- `timestamp`, `currency`, `percentage`\n\n### Contract Validation\n\n- Schema: column names, types, nullability\n- Quality: completeness, null %, custom rules\n- Breaking changes: removed columns, type changes, nullability\n\n### Anomaly Detection Methods\n\n| Method | Threshold | Use Case |\n|--------|-----------|----------|\n| `zscore` | 3.0 (std devs) | Normal data |\n| `iqr` | 1.5 (IQR multiplier) | Outlier-robust |\n| `percent_change` | 0.2 (20%) | Time-series monitoring |\n| `modified_zscore` | 3.5 | Non-normal distributions |\n| `baseline` | 2.0 (sensitivity) | Learn from history |\n| `ks_test` | 0.05 (p-value) | Distribution drift |\n\n### Cross-Dataset Validation Methods\n\n| Method | Description | Use Case |\n|--------|-------------|----------|\n| `col.exists_in(other_col)` | Check values exist in reference | FK validation |\n| `col.references(other_col)` | FK check with null handling | Optional/Required FK |\n| `col.find_orphans(other_col)` | Get orphan values | Debugging |\n| `col.matches_values(other_col)` | Compare value sets | Lookup validation |\n| `dataset.row_count_matches(other)` | Compare row counts | Backup validation |\n| `dataset.reconcile(target, key_columns)` | Row-by-row comparison | Migration validation |\n| `col.detect_drift(other_col)` | Distribution drift (KS-test) | ML monitoring |\n| `dataset.group_by(col).row_count_greater_than(n)` | Per-group validation | Segmented checks |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": "## 21. Next Steps\n\n- **Documentation**: https://duckguard.dev\n- **GitHub**: https://github.com/XDataHubAI/duckguard\n- **Issues**: https://github.com/XDataHubAI/duckguard/issues\n\n### What to explore next:\n1. Generate a `duckguard.yaml` file for your data with `duckguard discover`\n2. Create a data contract with `duckguard contract generate`\n3. Set up anomaly monitoring with `duckguard anomaly`\n4. Add rules to your CI/CD pipeline with pytest\n5. Detect PII with semantic type detection\n6. Set up Slack/Teams/Email alerts for data quality failures\n7. Export your rules to dbt with `dbt.export_to_schema()`\n8. Use row-level error capture for debugging failed validations\n9. Generate HTML/PDF reports with `duckguard report`\n10. Track quality trends with `duckguard history --trend`\n11. Add DuckGuard to your Airflow DAGs\n12. Set up GitHub Actions for CI/CD quality gates\n13. **NEW**: Monitor data freshness with `duckguard freshness`\n14. **NEW**: Learn baselines for ML-based anomaly detection\n15. **NEW**: Track schema changes with `duckguard schema`\n16. **NEW**: Set up email notifications for alerts\n17. **NEW**: Validate FK relationships with `exists_in()` and `references()`\n18. **NEW**: Compare datasets with `row_count_matches()` and `matches_values()`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}