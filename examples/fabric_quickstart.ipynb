{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶Ü DuckGuard for Microsoft Fabric\n",
    "\n",
    "Validate your Fabric Lakehouse and Warehouse data in 3 lines of Python.\n",
    "\n",
    "**Works with:** OneLake (Parquet/Delta), SQL endpoints, Fabric notebooks.\n",
    "\n",
    "[![GitHub](https://img.shields.io/github/stars/XDataHubAI/duckguard?style=social)](https://github.com/XDataHubAI/duckguard)\n",
    "[![PyPI](https://img.shields.io/pypi/v/duckguard.svg)](https://pypi.org/project/duckguard/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Fabric notebook:\n",
    "%pip install duckguard[fabric] -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Your Lakehouse\n",
    "\n",
    "### Option A: OneLake (direct file access)\n",
    "\n",
    "Access Parquet and Delta tables in your Lakehouse via OneLake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckguard import connect\n",
    "\n",
    "# OneLake path ‚Äî Lakehouse tables\n",
    "orders = connect(\n",
    "    \"fabric://my-workspace/my-lakehouse/Tables/orders\",\n",
    "    token=\"<your-azure-ad-token>\"\n",
    ")\n",
    "\n",
    "# Or use the full OneLake URL\n",
    "# orders = connect(\n",
    "#     \"onelake://my-workspace/my-lakehouse.Lakehouse/Tables/orders\",\n",
    "#     token=\"<your-azure-ad-token>\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: SQL Endpoint\n",
    "\n",
    "Query via T-SQL ‚Äî works with both Lakehouse and Warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL endpoint\n",
    "orders = connect(\n",
    "    \"fabric+sql://your-workspace-guid.datawarehouse.fabric.microsoft.com\",\n",
    "    table=\"orders\",\n",
    "    database=\"my_lakehouse\",\n",
    "    token=\"<your-azure-ad-token>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: Inside a Fabric Notebook\n",
    "\n",
    "If you're running in a Fabric notebook, you can load data via Spark and pass it as a DataFrame ‚Äî no token needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Fabric notebook ‚Äî load via Spark, validate via DuckGuard\n",
    "# df = spark.sql(\"SELECT * FROM my_lakehouse.orders\").toPandas()\n",
    "# orders = connect(df)\n",
    "\n",
    "# For this demo, we'll create sample data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"order_id\": [f\"ORD{i:04d}\" for i in range(1, 11)],\n",
    "    \"customer_id\": [f\"CUST{i:03d}\" if i != 3 else None for i in range(1, 11)],\n",
    "    \"product\": [\"Widget\", \"Gadget\", \"Widget\", \"Gizmo\", \"Widget\",\n",
    "                \"Gadget\", \"Widget\", \"Bundle\", \"Widget\", \"Gizmo\"],\n",
    "    \"quantity\": [2, 1, -3, 1, 500, 2, 1, 3, 1, 2],\n",
    "    \"total_amount\": [70.37, 54.49, -93.08, 217.99, 16349.54,\n",
    "                      113.97, 15.88, 326.97, 37.68, 435.98],\n",
    "    \"status\": [\"shipped\", \"delivered\", \"pending\", \"shipped\", \"pending\",\n",
    "               \"INVALID\", \"delivered\", \"shipped\", \"delivered\", \"pending\"],\n",
    "    \"email\": [\"alice@example.com\", \"bob@example.com\", \"charlie@example.com\",\n",
    "              None, \"eve@example.com\", \"frank@example.com\", \"grace@example\",\n",
    "              \"hans@example.de\", \"ivan@example.com\", \"jun@example.jp\"],\n",
    "})\n",
    "\n",
    "orders = connect(df)\n",
    "print(f\"Rows: {orders.row_count}, Columns: {len(orders.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate ‚Äî Same API Everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These assertions work the same on Fabric, Snowflake, S3, or CSV\n",
    "checks = [\n",
    "    (\"order_id not null\", orders.order_id.is_not_null()),\n",
    "    (\"order_id unique\", orders.order_id.is_unique()),\n",
    "    (\"customer_id not null\", orders.customer_id.is_not_null()),\n",
    "    (\"quantity in [1, 100]\", orders.quantity.between(1, 100)),\n",
    "    (\"total_amount positive\", orders.total_amount.greater_than(0)),\n",
    "    (\"status valid\", orders.status.isin([\"pending\", \"shipped\", \"delivered\", \"cancelled\"])),\n",
    "]\n",
    "\n",
    "for name, result in checks:\n",
    "    icon = \"‚úÖ\" if result.passed else \"‚ùå\"\n",
    "    print(f\"{icon} {name}\")\n",
    "    if not result.passed:\n",
    "        print(f\"   ‚Üí {result.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = orders.score()\n",
    "\n",
    "print(f\"Quality Grade: {score.grade} ({score.overall:.1f}/100)\")\n",
    "print(f\"  Completeness: {score.completeness:.1f}%\")\n",
    "print(f\"  Uniqueness:   {score.uniqueness:.1f}%\")\n",
    "print(f\"  Validity:     {score.validity:.1f}%\")\n",
    "print(f\"  Consistency:  {score.consistency:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Profile & PII Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckguard import AutoProfiler, SemanticAnalyzer\n",
    "\n",
    "# Full profile\n",
    "profile = AutoProfiler().profile(orders)\n",
    "print(f\"{'Column':<20} {'Nulls %':<10} {'Unique %':<10} {'Grade'}\")\n",
    "print(\"-\" * 50)\n",
    "for col in profile.columns:\n",
    "    print(f\"{col.name:<20} {col.null_percent:<10.1f} {col.unique_percent:<10.1f} {col.quality_grade}\")\n",
    "\n",
    "# PII scan\n",
    "analysis = SemanticAnalyzer().analyze(orders)\n",
    "if analysis.pii_columns:\n",
    "    print(f\"\\n‚ö†Ô∏è  PII found in: {analysis.pii_columns}\")\n",
    "    print(\"   ‚Üí Consider masking before sharing this data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Auto-Generate Validation Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckguard import generate_rules\n",
    "\n",
    "yaml_rules = generate_rules(orders, dataset_name=\"fabric_orders\")\n",
    "print(yaml_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrate with Fabric Pipelines\n",
    "\n",
    "### In a Fabric Notebook Activity (Data Pipeline)\n",
    "\n",
    "```python\n",
    "from duckguard import connect, load_rules, execute_rules\n",
    "\n",
    "# Load data from Lakehouse\n",
    "df = spark.sql(\"SELECT * FROM my_lakehouse.orders\").toPandas()\n",
    "data = connect(df)\n",
    "\n",
    "# Validate against rules\n",
    "rules = load_rules(\"/lakehouse/default/Files/duckguard.yaml\")\n",
    "result = execute_rules(rules, data)\n",
    "\n",
    "if not result.passed:\n",
    "    raise Exception(\n",
    "        f\"Data quality check failed: {result.failed_count} failures\\n\"\n",
    "        f\"{result.summary()}\"\n",
    "    )\n",
    "```\n",
    "\n",
    "### As a pytest Check in CI/CD\n",
    "\n",
    "```python\n",
    "# tests/test_fabric_quality.py\n",
    "from duckguard import connect\n",
    "\n",
    "def test_orders_quality():\n",
    "    orders = connect(\n",
    "        \"fabric+sql://workspace.datawarehouse.fabric.microsoft.com\",\n",
    "        table=\"orders\", database=\"lakehouse\", token=os.environ[\"FABRIC_TOKEN\"]\n",
    "    )\n",
    "    assert orders.row_count > 0\n",
    "    assert orders.order_id.is_not_null()\n",
    "    assert orders.order_id.is_unique()\n",
    "    assert orders.total_amount.between(0, 50000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Getting Your Azure AD Token\n",
    "\n",
    "```python\n",
    "# Option 1: Azure Identity (recommended)\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token(\"https://analysis.windows.net/powerbi/api/.default\").token\n",
    "\n",
    "# Option 2: In a Fabric notebook (automatic)\n",
    "# Token is available via mssparkutils\n",
    "token = mssparkutils.credentials.getToken(\"pbi\")\n",
    "\n",
    "# Then pass to DuckGuard\n",
    "data = connect(\"fabric://workspace/lakehouse/Tables/orders\", token=token)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- üìö [Full Docs](https://xdatahubai.github.io/duckguard/)\n",
    "- üîå [All Connectors](https://xdatahubai.github.io/duckguard/connectors/overview/) ‚Äî S3, Snowflake, Databricks, BigQuery, and more\n",
    "- ü§ñ [AI Features](https://xdatahubai.github.io/duckguard/guide/ai-features/) ‚Äî LLM-powered explain, suggest, and fix\n",
    "- ‚≠ê [Star on GitHub](https://github.com/XDataHubAI/duckguard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
